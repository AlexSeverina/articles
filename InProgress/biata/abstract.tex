\documentclass[12pt]{article}  % standard LaTeX, 12 point type
\usepackage{amsfonts,latexsym}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[utf8x]{inputenc} % Кодировка
\usepackage[english]{babel} % Многоязычность
\usepackage{multirow}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}{Определение}[section]
\newtheorem{example}{Example}[section]

% unnumbered environments:

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{notation}{Notation}
\newtheorem*{note}{Note}

\setlength{\parskip}{5pt plus 2pt minus 1pt}
%\setlength{\parindent}{0pt}

\usepackage{color}
\usepackage{listings}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{ucs}
\usepackage{hyperref}
\usepackage{textcomp}

\newcommand{\tab}[1][0.3cm]{\ensuremath{\hspace*{#1}}}
% A generalized view on parsing and translation
% http://dl.acm.org/citation.cfm?id=2206331
\title{Improved Architecture of Artificial Neural Network for Secondary Structure Analysis\footnote{The research was supported by the Russian Science Foundation grant 18-11-00100 and a grant from JetBrains Research.}}
% Context-free path querying ...
\author{Semyon Grigorev, Polina Lunina
\\ 
       \small{Saint Petersburg State University}\\
       \small{7/9 Universitetskaya nab., St. Petersburg, 199034, Russia}\\
       \small{JetBrains Research}\\
       \small{Primorskiy prospekt 68-70, Building 1, St. Petersburg, 197374, Russia} \\
       \small{semyon.grigorev@jetbrains.com, lunina\_polina@mail.ru}
       }
\date{}

%\tmargin{-25pt}

\begin{document}
\maketitle
Algorithms that can efficiently perform sequences classification and subsequences detection have recently become a focus in bioinformatics and many of them utilize the idea about considering these sequences secondary structure~\cite{GrammarsRNA, PCFG, meta, LWPCFG}. One of the classical ways of describing secondary structure is formal grammars.

An approach for biological sequences processing by combination of formal grammars and neural networks is proposed in the work~\cite{grigorevcomposition}.
While classical way is to model secondary structure of the full sequence by using grammar, the proposed approach utilizes it only for primitive secondary structure features description. These features can be extracted by parsing algorithm and processed by using artificial neural network. It is shown that this approach is applicable for real-world data processing and some questions are formulated for future research. In this work we provide answers to some of them.  

The first question is whether it is possible to use convolutional neural networks for parsing result processing. The result of matrix-based parsing algorithm for an input string and fixed nonterminal is an upper triangular boolean matrix. Presently, we came up with two possible ways of these matrices representation. The first one is to drop out the bottom left triangle and vectorize the rest of matrix row by row. It requires the equal length of the input sequences, therefore we propose to either cut sequences or add some special symbol till the definite length. The second way is to represent this matrix as an image: the false bits of matrix as white pixels and the true bits as black ones. This approach makes it possible to process sequences with different length since the images may be easily transformed to the same size. To handle these images we use network with a small number of convolutional layers, linearization and then the same structure as for vectorized data (dense and dropout layers with batch normalization).

The second question is whether it is possible to move parsing to network training step. This question is important because parsing is the most time-consuming operation of the proposed solution. We solve this problem by using two-staged learning. At the first step, we prepare a neural network (vector- or image-based) for our task which takes parsed data as an input. After that we extend trained network with a number of input layers that should convert original nucleotide sequence into parsing result. This way we create a network which can handle sequences, not parsing result. So, parsing is required only for training the first network.

We use the proposed improvements to create neural networks for tRNA sequences analysis problems: classification of tRNA into 2 classes: eukaryotes and prokaryotes (EP) and 4 classes: archaea, bacteria, plants and fungi (ABFP). We train networks on sequences from databases~\cite{trnadb1, trnadb2}. Results for both image- and vector-based classifiers are presented in the table~\ref{res}, where base model means network which handles parsing result and extended model handles sequences and is based on the corresponding base model.

\begin{table}[h]
\begin{tabular}{|l||l|l||l|l|}
\hline
Classifier                                                               & \multicolumn{2}{l||}{EP}               & \multicolumn{2}{l|}{ABFP}           \\ \hline \hline
Approach                                                                 & Vector-based       & Image-based      & Vector-based      & Image-based     \\ \hline
\begin{tabular}[c]{@{}l@{}}Base model\\ accuracy\end{tabular}            & 94.1\%             & 96.2\%           & 86.7\%            & 93.3\%          \\ \hline
\begin{tabular}[c]{@{}l@{}}Extended model \\ accuracy\end{tabular}       & 97.5\%             & 97.8\%           & 96.2\%            & 95.7\%          \\ \hline
\begin{tabular}[c]{@{}l@{}}Total samples\\ (train:valid:test)\end{tabular} & \multicolumn{2}{l||}{20000:5000:10000} & \multicolumn{2}{l|}{8000:1000:3000} \\ \hline
\end{tabular}
\caption{Test results}
\label{res}
\end{table}

 

\begin{thebibliography}{7}

\bibitem{GrammarsRNA}
Rivas, Elena, and Sean R. Eddy. \emph{``The language of RNA: a formal grammar that includes pseudoknots.''} Bioinformatics 16.4 (2000): 334-340.

\bibitem{PCFG}
Knudsen, Bjarne, and Jotun Hein. \emph{``RNA secondary structure prediction using stochastic context-free grammars and evolutionary history.''} Bioinformatics (Oxford, England) 15.6 (1999): 446-454.

\bibitem{meta}
Yuan, Cheng, et al. \emph{``Reconstructing 16S rRNA genes in metagenomic data.''} Bioinformatics 31.12 (2015): i35-i43.

\bibitem{LWPCFG}
Dowell, Robin D., and Sean R. Eddy. \emph{``Evaluation of several lightweight stochastic context-free grammars for RNA secondary structure prediction.''} BMC bioinformatics 5.1 (2004): 71.

\bibitem{grigorevcomposition}
Grigorev, Semyon, Lunina, Polina. \emph{``The Composition of Dense Neural Networks and Formal Grammars for Secondary Structure Analysis.''} In Proceedings of the 12th International Joint Conference on Biomedical Engineering Systems and Technologies - Volume 3: BIOINFORMATICS (2019): 234-241.

\bibitem{trnadb1}
Genomic tRNA Database.~--~URL: \url{http://gtrnadb2009.ucsc.edu/} (online; accessed: 17.05.2019).

\bibitem{trnadb2}
tRNADB-CE.~--~URL: \url{http://trna.ie.niigata-u.ac.jp/cgi-bin/trnadb/index.cgi} (online; accessed: 17.05.2019).

\end{thebibliography}


\end{document}
