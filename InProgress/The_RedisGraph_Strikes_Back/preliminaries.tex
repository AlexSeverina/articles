\section{Preliminaries}

In this section we introduce common definitions in graph theory and formal language theory which will be used in this paper. 
Also, we provide brief description of Azimov's algorithm which is used as a base of our solution.

\subsection{Graphs}

<<<<<<< HEAD
In this work we use edge-labelled digraph as a data model and define it as follows.
=======
In this work we use labelled digraph as a data model and define it as follows.
>>>>>>> d6909abbd2037a2af8674306336ee7245e3a0e19
\begin{definition} \emph{Labeled directed graph} is a triple $D = (V, E, \lambda)$, where
\begin{itemize}
    \item $V$ is a set of vertices. For simplicity, we assume that the vertices are natural numbers.
    \item $E \subseteq V \times V$ is a set of edges
    \item $\lambda : (E\cup V) \xrightarrow{} 2^\Sigma$ is a function that maps edges and vertices to their labels from the label set $\Sigma$.
\end{itemize}
\end{definition}

An example of the graph is presented in figure~\ref{fig:example_input_graph}. Here the set of labels $\Sigma = \{A, B, C, D, X\}$.

\begin{figure}[h]
    \centering        
    \begin{tikzpicture}[shorten >=1pt,auto]
       \node[state] (q_0)                        {$1: X$};
       \node[state] (q_1) [right=of q_0]         {$2$};
       \node[state] (q_2) [below left=of q_1]    {$3: X$};
       \node[state] (q_3) [below left=of q_2]    {$4$};
       \node[state] (q_4) [below right=of q_2]   {$5$};
       \node[state] (q_5) [below right=of q_1]   {$6$};
       \path[->]
        (q_0) edge  node {$A$} (q_1)
        (q_1) edge  node {$B$} (q_5)
        (q_1) edge  node {$B$} (q_2)
        (q_3) edge[above]  node {$C$} (q_2)
        (q_4) edge  node {$C$} (q_3)
        (q_2) edge[above]  node {$C$} (q_4)
        (q_5) edge[bend left, below]  node {$D$} (q_4)
        (q_4) edge[bend left, above]  node {$D$} (q_5);
    \end{tikzpicture}
    \caption{The example of input graph $\mathcal{G}$}
    \label{fig:example_input_graph}
\end{figure}

We use adjacency matrix decomposed to a set of a boolean matrix as a representation of the graph.
\begin{definition}
An adjacency matrix $M$ of the labelled graph $\mathcal{G}=(V, E, \lambda)$ is a square $|V|\times|V|$ matrix, such that $M[i,j] = \lambda((i, j))$.
\end{definition}

Adjacency matrix $M$ of the graph $\mathcal{G}$ is

$$
    M =
    \begin{pmatrix}
    .     & \{A\} &   .   &   .   &   .   &   .   \\
    .     &   .   & \{B\} &   .   &       & \{B\} \\
    .     &   .   &   .   &   .   & \{C\} &   .   \\
    .     &   .   & \{C\} &   .   &   .   &   .   \\
    .     &   .   &   .   & \{C\} &   .   & \{D\} \\
    .     & .     &   .   &   .   & \{D\} &   .
    \end{pmatrix}.
$$

\begin{definition}

Boolean decomposition of adjacency matrix $M$ of graph $\mathcal{G}=(V, E, \lambda)$ is set of Boolean matrix $$\mathcal{M} = \{M^l \mid l \in \Sigma, M^l[i,j]=1 \iff l \in M[i,j]\}.$$

\end{definition}

Matrix $M$ can be represented as a set of two Boolean matrices $M^a$ and $M^b$ where
\begin{align}
& M^{A} =    
\begin{pmatrix}
    .     &   1   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .
\end{pmatrix}, 
M^{B} =
\begin{pmatrix}
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   1   &   .   &   .   &   1   \\
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .
\end{pmatrix} \\
& M^{C} =
\begin{pmatrix}
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   1   &   .   \\
    .     &   .   &   1   &   .   &   .   &   .   \\
    .     &   .   &   .   &   1   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .
\end{pmatrix},
M^{D} =
\begin{pmatrix}
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   .   \\
    .     &   .   &   .   &   .   &   .   &   1   \\
    .     &   .   &   .   &   .   &   1   &   .
\end{pmatrix}.
\label{eq:boolean_decomposition_of_graph}
\end{align}

\begin{definition}

An vertex label matrix $N$ of the labelled graph $\mathcal(G)=(V, E, \lambda)$ is a square $|V|\times|V|$ matrix, such that $N[i,i] = \lambda (i)$ and $N[i,j] = \emptyset$ for $i \neq j$.

\end{definition}

\begin{definition}

Boolean decomposition of vertex label matrix $N$ of graph $\mathcal{G}=(V, E, \lambda)$ is set of Boolean matrix $$\mathcal{N} = \{N^l \mid l \in \Sigma, N^l[i,j]=1 \iff l \in N[i,j]\}.$$

\end{definition}

\subsection{Languages}
We formulate constraints in terms of context-free languages, for this reason there are following definitions.
\begin{definition}\emph{Context-free grammar} is a 4-tuple $G=(N, \Sigma, P, S)$, where 
\begin{itemize}
    \item $N$ is a finite set of nonterminals
    \item $\Sigma$ is a finite set of terminals
    \item $P$ is a finite set of productions of the following forms: $A \to \alpha, ~A \in N,~ \alpha \in (N \cup \Sigma)^*$
    \item $S$ is a starting nonterminal
\end{itemize}
\end{definition}

\begin{definition} \emph{Context-free language} is a language generated by a context-free grammar $G$:
\begin{align*}
     L(G) = \{w \in \Sigma^* \mid S \xLongrightarrow[G]{*} w \} 
\end{align*}
Where $S \xLongrightarrow[G]{*} w$  denotes that a string $w$ can be generated from a starting non-terminal $S$ using some sequence of production rules from $P$.
\end{definition}
\begin{definition} Context-free grammar $G = (N, \Sigma, P, S)$ is said to be in \emph{Chomsky normal form} if all productions in $P$ are in one of the following forms:
    \begin{itemize}
        \item $A \rightarrow BC,~A \in N,~B,~C \in N \setminus S$
        \item  $A \rightarrow a,~A \in N,~a \in \Sigma$
        \item $S \rightarrow \varepsilon,~\varepsilon$ is an empty string
    \end{itemize}
\end{definition}
 Since matrix-based CFPQ algorithms processes grammars only in Chomsky normal form, it should be noted that every context-free grammar can be transformed into an equivalent one in this form. 
\begin{definition} Context-free grammar $G = (N, \Sigma, P, S)$ is said to be in \emph{weak Chomsky normal form} if all productions in $P$ are in one of the following forms:
    \begin{itemize}
        \item $A \rightarrow BC,~A,~B,~C \in N$
        \item  $A \rightarrow a,~A \in N,~a \in \Sigma$
        \item $A \rightarrow \varepsilon,~A \in N$
    \end{itemize}
\end{definition}
In other words, weak Chomsky normal form differs from Chomsky normal form in the following:
\begin{itemize}
    \item $\varepsilon$ can be derived from any non-terminal
    \item $S$ can be at a right part of productions
\end{itemize}
    
    
For example, let's consider the following context-free grammar, which generates the language $L(G) = \{A^nB^n, n \in \mathbb{N}\}$:
$G=(N, \Sigma, P, S), ~N=\{S\},~\Sigma=\{a,b\}$ and productions: 
\begin{align*}
S \rightarrow aSb\\
S \rightarrow ab
\end{align*}
After transformation to weak Chomsky normal form the resulting grammar:
\begin{gather*}
S \rightarrow AB  \quad S \rightarrow AS_1 \quad S_1 \rightarrow SB \\
A \rightarrow a \quad B \rightarrow b \quad
\end{gather*}

These productions itself are the grammar that has the same result as original grammar.

We use a context-free grammar in the weak Chomsky normal form without a starting non-terminal, which will be specified in the path queries for the graph. Also we omit the rules of the form $A \rightarrow \varepsilon$ for the reason that they correspond to trivial paths, which are more convenient to consider separately.

\begin{definition}\emph{Context-free relation} is a relation $R_A \subseteq V \times V$ for edge-labeled graph $D = (V, E)$, context-free grammar $G = (N,~\Sigma,~P)$ and fixed non-terminal $A$:
\begin{align*}
     R_A = \{(n, m) \mid \exists n \pi m~(l(\pi) \in L(G_A))\}
\end{align*}
Where $l(\pi)$ is a word obtained by concatenating the labels along the path $\pi$.
\end{definition}

Finally, in these notations context-free path querying problem is the problem of finding context-free relations in which the language is specified by a context-free grammar.
 
\subsection{Matrix-Based Algorithm}
Let $G = (N, \Sigma, P)$ be the input grammar, $D = (V, E)$ be the input edge-labeled graph and language $L$ over alphabet $\Sigma$. For the context-free path query evaluation, we need to provide context-free relations \mbox{$R_A \subseteq V \times V$} for every \mbox{$A \in N$}.
The matrix-based algorithm for CFPQ can be expressed in terms of operations over Boolean matrices (see listing~\ref{alg:algo0}) which is an advantage for implementation.
{\footnotesize
\begin{algorithm}
\begin{algorithmic}[1]
\caption{Context-free path querying algorithm}
\label{alg:algo0}
\Function{evalCFPQ}{$D=(V,E), G=(N,\Sigma,P)$}
    \State{$n \gets$ |V|}
    \State{$T \gets \{T^{A_i} \mid A_i \in N, T^{A_i}$ is a matrix $n \times n$, $T^{A_i}_{k,l} \gets$ \texttt{false}\} }
    \ForAll{$(i,x,j) \in E$, $A_k \mid A_k \to x \in P$}
        %\Comment{Matrices initialization}
        %\For{$A_k \mid A_k \to x \in P$}
          {$T^{A_k}_{i,j} \gets \texttt{true}$}
        %\EndFor
    \EndFor
    \ForAll{$A_k \mid A_k \to \varepsilon \in P$}
        \ForAll{$i \in \{0,\ldots ,n-1\}$}
            {$T^{A_k}_{i,i} \gets \texttt{true}$}
        \EndFor
    \EndFor

    \While{any matrix in $T$ is changing}
        %\Comment{Transitive c	losure calculation}
        \For{$A_i \to A_j A_k \in P$}
          { $T^{A_i} \gets T^{A_i} + (T^{A_j} \times T^{A_k})$ } 
        \EndFor
    \EndWhile
\State \Return $T$
\EndFunction
\end{algorithmic}
\end{algorithm}
}

This CFPQ algorithm allows efficiently apply GPGPU techniques, but it solves all-pairs problem and takes unreasonable amount of memory in scenarios in which we want to find paths from a relatively small set of vertices, since it calculates a lot of redundant information.  