\section{Bar-Hillel Theorem Mechanization in Coq}
\label{sec:main}

In this section we describe in detail all the fundamental parts of the proof. 
Also in this section, we briefly describe motivation to use the chosen definitions. 
In addition, we discuss the advantages and disadvantages of using of third-party proofs. 

Overall goal of this section is to provide step-by-step algorithm of constructing the context-free (CF) grammar of the intersection of two languages.
Final formulation of the obtained theorem can be found in the last subsection. 
   
\subsection{ Smolka's Results Generalization}
\label{sec:solka-generalized}

In this section, we describe the exact steps taken to use the results of Gert Smolka and Jana Hofmann~\cite{smolkaHofmann2016} on context-free languages mechanization in Coq in the proof of this article's theorem.

A substantial part of this proof relies on the work of~\cite{smolkaHofmann2016}\footnote{Gert Smolka, Jana Hofmann, Verified Algorithms for Context-Free Grammars in Coq. Related sources in Coq: \url{https://www.ps.uni-saarland.de/~hofmann/bachelor/coq_src.zip}. Documentation: \url{https://www.ps.uni-saarland.de/~hofmann/bachelor/coq/toc.html}. Access date: 10.10.2018.} from which many definitions and theorems were taken. Namely, the definition of a grammar, definitions of a derivation in grammar, some auxiliary lemmas about the decidability of properties of grammar/derivation, we also use the theorem that states that there always exists the transformation from context-free grammar to grammar in Chomsky Normal Form (CNF).

However, the proof about existence of transformation to CNF had one major flaw that we needed to fix. One could define a terminal symbol as in inductive type over natural numbers~(Lst.\ref{lst:terms}).

\begin{listing}[h]
	\begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Inductive ter : Type := | T : nat -> ter.
	\end{pyglist}
	\caption{The original Smolka's definition of terminals}
	\label{lst:terms}
\end{listing}

That is how it was done in~\cite{smolkaHofmann2016}. However for purposes of our proof, we need to consider nonterminals over the alphabet of triples. Therefore, it was decided to add polymorphism over the target alphabet. Namely, let $Tt$ and $Vt$ be types with decidable relation of equality, then we can define the types of terminal and nonterminal over alphabets $Tt$ and $Vt$ respectively as presented in~\ref{lst:polymorphic-terminals}.

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Inductive ter : Type := | T : Tt -> ter.
  Inductive var : Type := | V : Vt -> var.
    \end{pyglist}
    \caption{The new polymorphic definitions of terminals and nonterminals}
    \label{lst:polymorphic-terminals}
\end{listing}

The proof of Smolka has a clear structure, therefore only part of the proof where the use of natural numbers was essential has become incorrect. One of the grammar transformations (namely deletion of long rules) requires the creation of many new non-terminals. In the original proof for this purpose, the maximum over non-terminals included in the grammar was used. However, it is impossible for an arbitrary type.

To tackle this problem we introduce an additional assumption on alphabet types for terminals and nonterminals. We require an existence of the bijection between natural numbers and alphabet of terminals as well as nonterminals.

Another difficulty is that the original work defines grammar as a list of rules (without a distinct starting nonterminal). Thus, in order to define the language that is defined by a  grammar, one needs to specify the grammar and a starting terminal. This leads to the fact that the theorem about the equivalence of a CF grammar and the corresponding CNF grammar isn't formulated in the most general way, namely it guarantees equivalence only for non-empty words. 

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Lemma language_normal_form 
      (G:grammar) (A: var) (u: word):
    u <> [] -> 
    (language G A u <-> 
       language (normalize G) A u).
    \end{pyglist}
    \caption{The equivalence of languages specified by context-free grammar and by transformed grammar in CNF}
    \label{lst:verbments1}
\end{listing}

Changes in the definition of grammar or language would lead to significant code corrections. However, the question of whether the empty word is derivable is decidable for both the CF grammar and the DFA. Therefore, it is possible to simply consider two cases (1) when the empty word is derivable in the grammar and (2) when the empty word is not derivable.

\subsection{Basic Definitions}

In this section, we introduce the basic definitions used in the article, such as alphabets, context-free grammar, and derivation.

We define a symbol is either a terminal or a nonterminal~(\ref{lst:symb}).

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Inductive symbol : Type :=
    | Ts : ter -> symbol
    | Vs : var -> symbol.
    \end{pyglist}
    \caption{Definition of symbol (union of terminals and nonterminals)}
    \label{lst:symb}
\end{listing}

Next we define a word and a phrase as lists of terminals and symbols respectively~(\ref{lst:phrase}).
One can think that word is an element of the language defined by grammar, and phrase is an intermediate result of derivation.
Also, phrase is a right side of derivation rule.

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Definition word := list ter.
  Definition phrase := list symbol.
    \end{pyglist}
    \caption{Definitions of word and phrase.}
    \label{lst:phrase}
\end{listing}

The notion of nonterminal doesn't make sense for DFA, but in order to construct derivation in grammar we need to use nonterminal in intermediate states. For phrases, we introduce a predicate that defines whenever a phrase consists of only terminals. And if so, the phrase can be safely converted to the corresponding word.

We inheriting the definition of CFG from~\cite{smolkaHofmann2016}. Rule is defined as a pair of a nonterminal and a phrase, and grammar is a list of rules~(\ref{lst:grm}).
Note, that this definition of grammar is not include start nonterminal, and it is sufficient difference from classical definition, because such defined grammar is not specified language.

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Inductive rule : Type :=
  | R : var -> phrase -> rule.
        
  Definition grammar := list rule.
    \end{pyglist}
    \caption{Context-free rule and grammar definition}
    \label{lst:grm}
\end{listing}

An important step towards the definition of a language specified by a grammar is the definition of derivability~(\ref{lst:der}). Having $der(G, A, p)$ --- means that phrase $p$ is derivable in grammar $G$ starting from nonterminal $A$.

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Inductive der (G : grammar) 
                (A : var) : phrase -> Prop :=
  | vDer : der G A [Vs A]
  | rDer l : (R A l) el G -> der G A l
  | replN B u w v : 
      der G A (u ++ [Vs B] ++ w) -> 
      der G B v -> der G A (u ++ v ++ w).
    \end{pyglist}
    \caption{Derivability definition. Informally it is a recognizer of the language specified by grammar $G$ and start nonterminal $A$}
    \label{lst:der}
\end{listing}

Proof of ~\cite{beigelproof}  requires grammar to be in CNF. We used statement that every grammar in convertible into CNF from~\cite{smolkaHofmann2016}.

In the end, we recall the definition of language. We say that a phrase (not a word) $ w $ belongs to the language generated by a grammar $G$ from a non-terminal $A$, if $ w $ is derivable from nonterminal $ A $ in grammar $ G $ and $ w $ consists only of terminals.

\begin{listing}[h]
	\begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Definition language 
            (G : grammar) (A : var) (w : phrase) :=
    der G A w /\ terminal w.
	\end{pyglist}
	\caption{Definition of language}
	\label{lst:lang}
\end{listing}



\subsection{General Scheme of the Proof}

General scheme of our proof is based on constructive proof presented in~\cite{beigelproof}.
This proof does not use push-down automata explicitly and operates by grammars, so it looks pretty simple to be mechanized.
In the following subsections the main steps of the proof are presented. Overall, we will adhere to the following plan. 

\begin{enumerate}
    \item First we consider trivial case, when DFA has no state
    \item Every CF language can be converted to CNF
    \item Every DFA can be presented as an union of DFAs with single final state
    \item Intersecting grammar in CNF with DFA with one final state
    \item Proving than union of CF languages is CF language
\end{enumerate}

\subsection{Trivial Cases}

First, we consider the case when the DFA does not have any state, that is, the number of states is equal to zero. 
In this case, we can immediately derive a contradiction.
By definition, for any DFA an initial state is known. 
It means that there is at least one state, which contradicts the fact that the number of states is equal to zero.

In addition, it is worth mention, that in the proof~\cite{beigelproof} cases when in grammar an empty word is derivable or a DFA sets an empty language are discarded as trivial.
It is assumed that the proof for this cases one can carry out himself.
In our proof, we do not consider these cases as separate ones.
Consideration of these cases is included in the corresponding theorems in general formulation.

\subsection{Regular Languages and Automata}

In this section we describe definitions of DFA and DFA with exactly one final state, we also present function that converts any DFA to a set of DFA with one final state and lemma that states this split preserves language in some sense.

We assume that regular language is described by DFA. As the definition of an DFA, we have chosen a general definition, which does not impose any restrictions on the type of input symbols and the number of states. Thus, in our case, the DFA is a 5-tuple, (1) a state type, (2) a type of input symbols, (3) a start state, (4) a transition function, and (5) a list of final states~(Lst.\ref{lst:dfa}).

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Context {State T: Type}.
  Record dfa: Type :=
    mkDfa {
      start: State;
      final: list State;
      next: State -> (@ter T) -> State;
    }.
    \end{pyglist}
    \caption{Definition of deterministic finite automaton}
    \label{lst:dfa}
\end{listing}

Next we define a function that evaluates the final state of the automaton if it starts from state $s$ and receives a word $w$. 

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Fixpoint final_state 
             (next_d: dfa_rule) 
             (s: State) 
             (w: word): State :=
    match w with
    | nil => s 
    | h :: t => final_state next_d 
                            (next_d s h)
                            t 
    end.
    \end{pyglist}
    \caption{TODO}
    \label{lst:verbments1}
\end{listing}

We say that the automaton accepts a word $w$ being in state $s$ if the function $[\textit{final\_state} \ s \ w]$ returns one of the final states.
Finally, we say that an automaton accepts a word $w$, if the DFA starts from the initial state and ends in one of the final states.

%CODE
%Definition accepts (d : dfa) (s: State) (w: word) : Prop :=
%In (final_state (next d) s w) (final d). 

%CODE
%Definition dfa_language (d : dfa) := (accepts d (start d)).

In order to define the DFA with exactly one final state, it is necessary to replace the list of final states by one final state in the definition of an ordinary DFA. 
Related definitions such as \textit{accepts} and \textit{dfa\_language} should be modified slightly.

%Alternative: In the proof we need a subset (subtype?) of all automata. Namely, automata with one finite state. We can define them as follows. We say that dfa is a single-final-state-automata, if and only if the predicate "is final state?" can be represented as "is equal to the state fin?"

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Record s_dfa : Type :=
    s_mkDfa {
      s_start: State;
      s_final: State;
      s_next: State -> (@ter T) -> State;
  }.      
    \end{pyglist}
    \caption{Definition of DFA with exactly one final states}
    \label{lst:dfa-one-ss}
\end{listing}
  
Similarly, we can define functions $\textit{s\_accepts}$ and $\textit{s\_dfa\_language}$ for DFA with one final state.
Since in this case there is only one final state in order to define function $\textit{s\_accepts}$ it is enough to check the state in which the automaton stopped with the finite state. Function $\textit{s\_dfa\_language}$ repeats function $\textit{dfa\_language}$ except that the function for a DFA with one final state should use $\textit{s\_accepts}$ instead of $\textit{accepts}$.

%CODE
%Definition s_accepts (d : s_dfa) (s: State) (w: word) : Prop :=
%(final_state (s_next d) s w) = (s_final d).

%CODE
%Definition s_dfa_language (d : s_dfa) := (s_accepts d (s_start d)).

Now we can to define a function that converts an ordinary DFA into a set of DFAs with exactly one final state~(Lst.\ref{lst:dfa-split}).
Let $d$ be a DFA. Then the list of its final states is known. 
For each such state, one can construct a copy of the original DFA, but with one selected final state.

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Fixpoint split_dfa_list 
      (st_d : State) 
      (next_d : dfa_rule) 
      (f_list : list State): list (s_dfa) :=
    match f_list with
    | nil => nil
    | h :: t => (s_mkDfa st_d h next_d) 
                :: split_dfa_list st_d next_d t
    end.    
 
 Definition split_dfa (d: dfa) := 
   split_dfa_list (start d) (next d) (final d).
    \end{pyglist}
    \caption{Split DFA into set of DFAs with exactly one final state}
    \label{lst:dfa-split}
\end{listing}


% CODE
% Definition single_final_state_dfa (d: dfa)(fin: dfa_state) := dfa_fin dfa = pred1 fin.


% d DFA accepts word iff after transitions is comes to one of ist final states

% CODE
%Fixpoint dfa\_final x w :=
%match w with
%| [::] => x
%| a::w => dfa\_final (dfa\_step A x a) w
%end.

%Definition dfa\_accept x w := dfa\_fin (dfa\_final st word).

%It is easy to see that if our automaton is an automata with one final state fin, then dfa\_accept x w  is equivalent to dfa\_final x w = fin

%Regular language is a set words accepted by DFA.

%Definition dfa\_language (d : dfa):= fun word =>accepts d start word.

Now we should prove theorem that the function of splitting preserves the language~(Lst.\ref{lst:split-preserves-lang}).

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Lemma correct_split:
    forall dfa w,
      dfa_language dfa w <->
      exists sdfa, 
         In sdfa (split_dfa dfa) /\ 
         s_dfa_language sdfa w.
    \end{pyglist}
    \caption{Splitting of DFA into DFAs with exactly one final state preserves language}
    \label{lst:split-preserves-lang}
\end{listing}

\begin{theorem}
  Let $\textit{dfa}$ be an arbitrary DFA and $w$ be a word. Then the fact that $\textit{dfa}$ accepts $w$ implies that there exists a single-state DFA $\textit{s\_dfa}$, such that $\textit{s\_dfa} \in \textit{split\_dfa(dfa)}$. And vice versa, for any $\textit{s\_dfa} \in \textit{split\_dfa(dfa)}$ the fact that $\textit{s\_dfa}$ accepts a word $w$ implies that $\textit{dfa}$ also accepts $w$.
\end{theorem}

\textbf{Proof.}
Let us divide the proof into two parts.
\begin{enumerate}
\item Suppose $\textit{dfa}$ accepts $w$. Then we prove that there exists a single-state DFA $\textit{s\_dfa}$, such that $\textit{s\_dfa} \in \textit{split\_dfa(dfa)}$. 
Let \textit{finals} be the set of final states of \textit{dfa}. We carry out the proof by induction on \textit{finals}. 
\\
\textit{\underline{Base step}}: \textit{finals = [::]}. Trivial by contradiction (DFA with no final state cannot accept a word).
\\
\textit{\underline{Induction step}}: \textit{finals = a::old\_finals} and the statement holds for \textit{old\_finals}. Since \textit{dfa} accepts $w$, it either ends up in $a$, or in one of the state from \textit{old\_finals}.
If \textit{dfa} is ends up in $a$, then we simply choose an automaton with the final state that is equal to $a$.
Such an automaton exists, since now the list of final states also contains $a$.
On the other hand, if \textit{dfa} is ends up in one of the state from \textit{old\_finals}, then we can apply induction hypothesis.

\item Similarly for the opposite direction. Assume that there exists an automaton with exactly one final state from \textit{split\_dfa(dfa)} that accepts $w$. Then we prove that \textit{dfa} also accepts $w$. 
Let \textit{finals} be the set of final states of \textit{dfa}. We carry out the proof by induction on \textit{finals}. 
\\
\textit{\underline{Base step}}: \textit{finals = [::]}. Trivial by contradiction.
\\
\textit{\underline{Induction step}}: \textit{finals = a::old\_finals} and the statement holds for \textit{old\_finals}.
We know that one of the DFAs form \textit{split\_dfa(dfa} accepts $w$, its final state either is equal to $a$, or lies in \textit{old\_finals}.
If the final state is equal to $a$, then \textit{dfa} also ends up in state $a$.
On the other hand, if final state lies in \textit{old\_finals}, then we can apply induction hypothesis.
\end{enumerate}


\subsection{Chomsky Induction}
\label{sec:chomsky-induction}

In this section, we introduce the notion of Chomsky induction.

Naturally many statements about properties of language's words can be proved by induction over derivation structure. Unfortunately, grammar can derive phrase us an intermediate step, but DFA supposed to work only with words, so we can not simply apply induction over derivation structure. To tackle this problem we create custom induction principle for grammars in CNF.

As one might have noticed the current definition of derivability does not imply the ability to ``reverse'' the derivation back. That is, from the fact if a phrase $w$ is derived from a non-terminal $A$ in a grammar $G$ does not follow anything about the rules of the grammar or properties of this derivation. Because of this, we introduce an additional assumption on derivations that is similar in some sense to the syntactic analysis of words.
Namely, we assume that if phrase $w$ is derived from nonterminal $A$ in grammar $G$, then either there is a rule $A \to w \in G$ or there is an RHS $rhs$ such that $A \to rhs \in G$ and $w$ is derivable from $rhs$.

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
Definition syntactic_analysis_is_possible :=
  forall (G : grammar) (A : var) (w : phrase),
  der G A w ->
   (R A w \in G) \/ 
   (exists rhs, R A rhs \in G /\ derf G rhs w).
  
  \end{pyglist}
    \caption{If word in language then we can recostruct its derivation}
    \label{lst:synt-analysis-is-possible}
\end{listing}

The main point is that if we have a grammar in CNF, we can always divide the word into two parts, each of which is derived only from one nonterminal. Note that if we naively take a step back, we can get nonterminal in the middle of the word. Such a situation will not make any sense for DFA.

With induction we always work with subtrees that describes some part of word. Here is a picture of subtree describing intuition behind the Chomsky induction. \\

(\textbf{TODO}: is it okay to use png?)
\textbf{TODO}: add picture\\ 

(\textbf{TODO}: should be lemma) More formally: 
\begin{theorem}
Let $G$ be a grammar in CNF. Consider an arbitrary nonterminal $N \in G$ and phrase which consists only on terminals $w$. 
If $w$ is derivable from $N$ and $|w| \ge 2$, then there exists two nonterminals $N_1, N_2$ and subphrases of $w$ --- $w_1, w_2$ such that: $N \to N_1 N_2 \in G$, $der(N_1, w_1)$, $der(N_2, w_2)$, $|w_1| \ge 1$, $|w_2| \ge 1$ and $w_1 ++ w_2 = w$.
\end{theorem}

\textbf{Proof.}
The proof heavily uses the fact that grammar $G$ is in Chomsky Normal Form.
We apply the hypothesis ``syntactic analysis is possible''. After application, we get the fact that word $w$ is either an RHS of a rule of grammar $G$, or there is a phrase $phr$, such that (1) word $w$ is derivable from phrase $phr$ and (2) there exists a non-terminal $N$ such that $N -> prh \in G$.

The first case we finish with the proof by contradiction since the grammar is in CNF and there might be only a single terminal in an RHS (by assumption we have $|w| \ge 2$).
On the other hand, if there is an intermediate phrase that was obtained by applying a rule, then the phrase has form $N_1 N_2$, since it also derived by rule in normal form.
Finally, now we need to prove that both of this nonterminals has a non-empty contribution to word $w$. This is also true since it is impossible to derive empty word in CNF grammar (see ...).

(\textbf{TODO}: should be lemma)
\begin{theorem}
	Let $G$ be a grammar in CNF. And $P$ be a predicate on nonterminals and phrases (i.e. $P: var \to phrase \to Prop$).
	Let's also assume that the following two hypotheses are satisfied:
	(1) for every terminal production (i.e. in the form $N \to a$) of grammar $G$, $P(r, [Ts \ r])$ holds and (2) for every $N, N_1, N_2 \in G$ and two phrases that consist only of terminals $w_1, w_2$, if $P(N_1, w_1)$, $P(N_2, w_2)$, $der(G, N_1, w_1)$ and $der(G, N_2, w_2)$ then $P(N, w_1 ++ w_2)$.
	Then for any nonterminal $N$ and any phrase consisting only of terminals $w$, the fact that $w$ is derivable from $N$ implies $P(N,w)$.
\end{theorem}

\textbf{Proof.} 
Let $n$ be an upper bound of the length of word $w$. We carry out the proof by induction on $n$.

\underline{\textit{Base case:}} $ n = 0 $. Proof by contradiction. $|w| \le 0$ implies that $w$ is empty. But an empty word cannot be derived in CNF grammar (see ...).


\underline{\textit{Induction step:}} $|w| \le n+1$. This fact is equivalent to the following:  $|w| = n+1$ or $|w| < n$. 
In case of $|w| < n$ we use the induction hypothesis.
Next we consider two new cases, either $|w| = 1 $, or $1 < |w| = n + 1$.
In the first case, it is clear that this is possible only if there is a production $N -> w$, which means you can apply assumption (1).
If the word is longer than 1, then we apply the previous lemma, after that we can conclude that $\exists w1 w2, w = w1 ++ w2$. After that,
one need to apply assumption (2). We subgoals that are guaranteed by the lemma .... And for shorter words, we apply the induction hypothesis.

%CODE
%Hypothesis inductive_step_1:
%  forall (r : _) (t : ter),
%  R r [Ts t] el G ->
%  P r [Ts t].


%CODE
%Hypothesis inductive_step_2:
%  forall (r r1 r2: _) (w1 w2 : phrase),
%  R r [Vs r1; Vs r2] el G ->
%  P r1 w1 ->
%  P r2 w2 ->
%  terminal w1 ->
%  terminal w2 ->
%  der G r1 w1 ->
%  der G r2 w2 ->
%  P r (w1 ++ w2).


%CODE
%Lemma chomsky_derivability_induction:
%  forall (r : _) (w : _),
%  @der T V G r w ->
%  terminal w ->
%  P r w.


TODO: add some text



\subsection{Intersection of CFG and Automaton}

Since we already have lemmas about the transformation of a grammar to CNF and the transformation a DFA to a DFA with exactly one state, further we assume that we have (1) DFA with exactly one final state --- \textit{dfa} and (2) grammar in CNF --- $G$. In this section, we describe the proof of the lemma that states that for any grammar in CNF and any automaton with exactly one state there is the intersection grammar.

\subsubsection{Construction of Intersection}

Next we present adaptation of the algorithm given in~\cite{beigelproof}. 

Let $G_{INT}$ be the grammar of intersection. In $G_{INT}$ nonterminals presented as triples $(from \times var \times to) $ where \textit{from} and $to$ are states of \textit{dfa}, and \textit{var} is a nonterminal of $G$.

Since $G$ is a grammar in CNF, it has only two type of productions: $(1)\ N \to a $ and $(2) \ N \to N_{1} N_{2}$, where $N, N_1, N_2$ are nonterminals and $a$ is a terminal.

For every production $N \to N_1 N_2$ in $G$ we generate a set of productions of the form $$(from, N, to) \to (from, N_1,  m) (m, N_2, to)$$ where: $from$, $m$, $to$ --- goes through all $\textit{dfa}$ states.

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Definition convert_nonterm_rule_2 
    (r r1 r2: _) 
    (state1 state2 : _) :=
    map (fun s3 => R (V (s1, r, s3)) 
                     [Vs (V (s1, r1, s2)); 
                      Vs (V (s2, r2, s3))])
      list_of_states.

  Definition convert_nonterm_rule_1  
               (r r1 r2: _) 
               (s1 : _) :=
    flat_map (convert_nonterm_rule_2 r r1 r2 s1) 
             list_of_states.

  Definition convert_nonterm_rule (r r1 r2: _) :=
    flat_map (convert_nonterm_rule_1 r r1 r2) 
             list_of_states.
    \end{pyglist}
    \caption{Grammar rules convertions for nonterminal rules}
    \label{lst:verbments1}
\end{listing}

For every production of the form $N \to a$ we add a set of productions $$(\textit{from}, N, (\textit{dfa\_step}(\textit{from}, a))) \to a$$ where $\textit{from}$ --- goes through all $\textit{dfa}$ states and $\textit{dfa\_step (from, a)}$ is the state in which the $\textit{dfa}$ appears after receiving terminal $a$ in state $\textit{from}$.

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Definition convert_terminal_rule 
              (next: _) 
              (r: _) 
              (t: _): list TripleRule :=
    map (fun s1 => R (V (s1, r, next s1 t)) 
	               [Ts t]) 
        list_of_states.
    \end{pyglist}
    \caption{Grammar rules convertion for terminal rule}
    \label{lst:verbments1}
\end{listing}

Next we join the functions above to get a generic function that works for both types of productions. 
Note that since the grammar is in CNF, the third alternative can never be the case.

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Definition convert_rule (next: _) (r: _ ) :=
    match r with
    | R r [Vs r1; Vs r2] => 
        convert_nonterm_rule r r1 r2
    | R r [Ts t] => 
        convert_terminal_rule next r t 
    | _  => []   (* Never called *)
    end.
        
  Definition convert_rules 
    (rules: list rule) (next: _): list rule :=
    flat_map (convert_rule next) rules.
    
  (* Maps grammar and s_dfa 
     to grammar over triples *)
  Definition convert_grammar grammar s_dfa :=
    convert_rules grammar (s_next s_dfa). 
    \end{pyglist}
    \caption{Grammar convertion by using rules convertions}
    \label{lst:verbments1}
\end{listing}

Note that at this point we do not have any manipulations with starting rules. Nevertheless, the hypothesis of the uniqueness of the final state of the DFA, will help us unambiguously introduce the starting nonterminal of the grammar of intersection.

\subsubsection{Correctness of Intersection}

In this subsection we present a high-level description of the proof about correctness of the intersection function.

In the interest of clarity of exposition, we skip some auxiliary lemmas, such as (TODO:fix) "we can get the initial grammar from the grammar of intersection by projecting the triples back to terminals/nonterminals ". Also note that the grammar after the conversion remains in CFN. Since the transformation of rules does not change the structure of the rules, but only replaces one(??!!) terminals and nonterminals with others.


(\textbf{TODO}: move to ...?)
The starting nonterminal for the intersection grammar is the following nonterminal: \textit{(start, S, final)}. 
Where: \textit{start} --- the start state of DFA, \textit{S} --- the start symbol of initial grammar, and \textit{final} --- the final state of DFA. 


Next we prove the two main lemmas. Namely, the derivability in the initial grammar and the \textit{s\_dfa} implies the derivability in the grammar of intersection. And the other way around, the derivability in the grammar of intersection implies the derivability in the initial grammar and the \textit{s\_dfa}.

Let $G$ be a grammar in CNF. In order to use Chomsky Induction we also assume that syntactic analysis is possible. 

%TODO ????
% Если в грамматике из нетерминала r можно вывести слово word и стартуя из состояния from при принятии слова word автомат оказывается в состоянии to, тогда слово word также выводится и в грамматике (convert_rules G next) из нетерминала (V (from, r, to)). 

% Theorem der_in_initial_grammar_and_dfa_implies_der_in_triple_grammar:
%   forall (next: dfa_rule) (r: var) (from to: DfaState) (word: _),
%     der G r (to_phrase word) ->
%     final_state next from word = to ->
%     der (convert_rules G next) (V (from, r, to)) (to_phrase word).
\begin{theorem}
    Let \textit{s\_dfa} be an arbitrary DFA, let r be a nonterminal of grammar $G$, let from and to be two states of the DFA. We also pick an arbitrary word --- $w$. If in grammar $G$ it is possible to derive w out of $r$ and starting from the state from when $w$ is received, the \textit{s\_dfa} ends up in state to, then word $w$ is also derivable in grammar \textit{(convert\_rules G next)} from the nonterminal \textit{(V (from, r, to))}.
\end{theorem}

\textbf{Proof.}
It would be logical to use induction on the derivation structure in grammar $G$. But as it was discussed earlier, this is not the case, otherwise we will get a phrase (list of terminals and nonterminals) instead of a word. Therefore we should use another way to use induction, and for grammar in Chomsky normal form it is possible as we show in the section~\ref{chomsky-induction}. Roughly speaking, we can split the word into two subwords, such that each of them can be derived from some nonterminal and these two nonterminals is a RHS of some rule from the given grammar.

Let's apply chomsky induction principle with the following predicate $P$:
\begin{align*}
  P :=  \lambda \ r \ & phr \Rightarrow \\
        &\forall (\textit{next : dfa\_rule}) (\textit{from to : DfaState}), \\
        &\textit{final\_state next from (to\_word phr) = to} \to \\
        &\textit{der (convert\_rules G next) (V (from, r, to)) phr}.
\end{align*}

Basically, predicate $P$ is the property that we are trying to prove in the theorem. Chomsky Induction has 3 assumptions.
(1) The phrase to which $P$ is applied should consist of only non-terminals. We consider only words in this theorem, therefore after conversion of the word to the phrase, no terminals can appear in it. So, we do not violate this assumption.
Moreover, there is a base of induction (2) in the form of a property for a terminal rule and (3) an induction step for a non-terminal rule.
Both statements can be proved by induction on the number of rules in the grammar $G$ in combination with a simple calculation of the functions \textit{convert\_terminal\_rule} and \textit{convert\_nonterm\_rule} for terminal and non-terminal rules, respectively.

On the other side, now we need to prove the theorems of the form  ``if it is derivable in the grammar of triples, then it is derivable in the automaton and in ordinary grammar''!!!

We start with the DFA.
%          Lemma der_in_triple_grammar_implies_dfa_accepts:
%            forall var word,
%              der (convert_rules G next) (V (from, var , to)) (to_phrase word) ->
%              final_state next from word = to.

\begin{theorem}
	Let \textit{from} and $to$ be states of the automaton, \textit{var} be an arbitrary non-terminal grammar of $G$. We prove that If a word $w$ is derived from the non-terminal \textit{(from, var, to)} in the grammar \textit{(convert\_rules G)}, then the automaton, starting from the state \textit{from} at the input $w$ stops in state $to$.
\end{theorem}

\textbf{Proof.} 
Like last time, we use the principle of Chomsky Induction.
We apply the induction with the following parameter $P$:

\begin{align*}
P :=  \lambda & \ \textit{tr\_non} \ phr \Rightarrow  \\
              & \textit{final\_state} \textit{\ next \ (first3 \ tr\_non) (to\_word \ phr) =} \\
              &                       \textit{third3 r}. 
\end{align*}

Note that in this case, one need to use projections from triple-non-terminals to ``single'' non-terminals. Induction is carried out in the grammar of triples, but the property operates with the automaton. However, this property can also be expressed in terms of triples-non-terminals.
As last time, $P$ is the statement we want to prove.
After applying the induction principle, it remains only to prove the fidelity of the assumptions.
First of all, since $ w $ is a word, converting it into a phrase does not add any non-terminal.
Next, one need to show that the grammar \textit{convert\_rules \ G} is in CNF. It is easy to see, since $G$ in CNF and transformation \textit{convert\_rules} maps non-terminal rules to (TODO: "triple"?) "triple" non-terminal rules and terminal rules to "triple" terminal rules.
Finally, one need to show that both assumption of the induction principle hold.  
In this case it would be: 
\begin{align*}
r \to t  \in \textit{convert\_rules} \ G \implies \\ 
next \ (\textit{fst3} \ (\textit{unVar} \ r)) \ t = \textit{thi3} (\textit{unVar} \ r) 
\end{align*}
and 
\begin{align*}
& r \to [r1; r2] \in \textit{convert\_rules} \ G  -> \\
& ... -> \\
& \textit{final\_state} \ \textit{next} (fst3 (unVar r)) (to_word (w1 ++ w2)) = thi3 (unVar r)
\end{align*}

In both cases, proof can be done by an ``inverse'' calculation of functions \textit{convert\_terminal\_rule} and \textit{convert\_nonterm\_rule}. 
That is, by inversing the assumption $$ r \to [r1; r2] \in \textit{convert\_rules} \ G$$ in the assumptions, we gradually come to the conclusion that the only possible option is that the input satisfies the property of the goal. Then it remains only to simplify assumptions and conclusion.


Further we prove the theorem for grammar.

%          Lemma der_in_triple_gr_implies_der_in_initial_gr:
%            forall (s_start s_final: DfaState) (grammar_start: _) (word: word),  
%              der (convert_rules G next) (V (s_start, grammar_start, s_final)) (to_phrase word) ->
%              der G grammar_start (to_phrase word).
\begin{theorem}
	Let \textit{from} and $to$ be the states of the automaton, let \textit{var} be an arbitrary non-terminal of grammar G. We prove that if a word $w$ is derivable from non-terminal \textit{(from, var, to)} in the grammar \textit{(convert\_rules G)}, then $w$ is also derivable in grammar $G$ from nonterminal \textit{var}.
\end{theorem}

\textbf{Proof.} 
We again prove the theorem using Chomsky induction with the following predicate P:
\begin{align*}
P :=  \lambda \ r \ phr \Rightarrow \textit{der G (snd3 (unVar r)) (phr)}.
\end{align*}

Again, note that induction is carried out in the grammar of triples, but the property is talking about the ``unit'' grammar, therefore we use projections from triples to non-triples.
Here we can use the same idea of ``inversing'' of functions \textit{convert\_terminal\_rule} and \textit{convert\_nonterm\_rule}.
Again, by inversing the induction hypothesis, we gradually come to the conclusion that the only possible option is that the input satisfies the property of the goal. 

%R (V (from, r, to)) [Ts t] el convert_rules G next -> R r [Ts t] el G. 
%И аналогично для нетерминальных правил, только начиная с правила вида ... можно получить ... 
%R (V (from, r, to)) [Vs (V (from1, r1, to1)); Vs (V (from2, r2, to2))] el convert_rules G next -> R r [Vs r1; Vs r2] el G.
 


Well, in the end one need to combine both theorems to get full equivalence. On this, the correctness of the intersection is proved.

\subsection{Union of Languages}

After the previous step, we have a list of grammars of CF languages, in this section, we provide a function by which we construct a grammar of the union of languages.

For this, we need nonterminals from every language to be from different non-intersecting sets. To achieve this we add labels to nonterminals. Thus, each grammar of the union would have its own unique ID number, all nonterminals within one grammar will have the same ID which coincides with the ID of a grammar. In addition, it is necessary to introduce a new starting nonterminal of the union.

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Inductive labeled_Vt : Type :=
  | start : labeled_Vt
  | lV : nat -> Vt -> labeled_Vt.
  
  Definition label_var (label: nat) 
                       (v: @var Vt): @var 
                       labeled_Vt :=
    V (lV label v).  
    \end{pyglist}
    \caption{TODO}
    \label{lst:verbments1}
\end{listing}

Construction of new grammar is quite simple. The function that constructs the union grammar takes a list of grammars, then, it (1) splits the list into head [$h$] and tail [$tl$], (2) labels [\textit{length \ tl}] to $h$, (3) adds a new rule from the start nonterminal of the union to the start nonterminal of the grammar [$h$], finally (4) the function is recursively called on the tail [$tl$] of the list.

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Definition label_grammar label grammar := ...

  Definition label_grammar_and_add_start_rule 
               label 
               grammar :=
    let '(st, gr) := grammar in 
    (R (V start) [Vs (V (lV label st))]) 
       :: label_grammar label gr.        

  Fixpoint grammar_union 
     (grammars : seq (@var Vt * (@grammar Tt Vt)))
       : @grammar 
     Tt 
     labeled_Vt :=
    match grammars with
    |  [] => []
    |  (g::t) => 
         label_grammar_and_add_start_rule 
           (length t) 
           g ++ (grammar_union t)
    end.
    \end{pyglist}
    \caption{TODO}
    \label{lst:verbments1}
\end{listing}

\subsubsection{Proof of Languages Equivalence}

In this section, we prove that function \textit{grammar\_union} constructs a correct grammar of union language indeed. Namely, we prove the following theorem.

\begin{theorem}\label{theorem-correct-union}
    Let \textit{grammars} be a sequence of pairs of starting nonterminals and grammars. Then for any word $w$, the fact that $w$ belongs to language of union is equivalent to the fact that there exists a grammar $(st,gr) \in \textit{grammars}$ such that $w$ belongs to language generated by $(st,gr)$.
\end{theorem}

\begin{listing}[h]
    \begin{pyglist}[language=coq, numbers=none, numbersep=5pt]
  Variable grammars: seq (var * grammar).

  Theorem correct_union:
    forall word, 
      language (grammar_union grammars) 
        (V (start Vt)) (to_phrase word) <->
      exists s_l, 
        language (snd s_l) (fst s_l) 
          (to_phrase word) /\ 
        In s_l grammars.
    \end{pyglist}
    \caption{Theorem on languages equivalence}
    \label{lst:lang-eq}
\end{listing}


\textbf{Proof of theorem~\ref{lst:lang-eq}.} Since the statement is formulated as an equivalence, we divide the proof into two parts.
\begin{enumerate}
\item If $w$ belongs to the union language, then $w$ belongs to one of the initial language. \\
From an auxiliary lemma, we know that either (1) the phrase is equal to the starting nonterminal or (2) there exists a grammar $G$ from the grammars-list such that TODO
Let us prove that this is the grammar we are interested in.
Since we consider the word, it cannot be a starting non-terminal. So this might be only the second case.
According to another lemma, if the output doesn't start from the starting non-terminal, then it cannot appear in this derivation. So all the rules that use the starting non-terminal can be safely (for this derivation) removed from the grammar.
There is a lemma that says, for a derivation that starts from a nonterminal labeled with $x$, can not contain any nonterminals with a label other than x. Therefore, for this derivation, one can ignore the rules with other(?) labels.
These two grammars are identical, but one of them is labeled and the other is not. It is clear that if there exists a bijection between nonterminals the set of derivable words doesn't change.

\item If $w$ belongs to one of the initial language, then $w$ belongs to the union language. \\
In this case, one explicitly specify the corresponding derivation in the union-grammar.
The labeling function is arranged in such a way that knowing the place of a certain grammar in the list of grammars, one can calculate the exact number that will be assigned to this grammar as a label.
After that proof can be finished in two steps. 
\begin{enumerate}
\item One need to apply the rule from the starting non-terminal of the union-grammar to the starting non-terminal of the initial grammar. 
\item One should use the fact that derivation in initial grammar and labeled grammar are equivalent.
\end{enumerate}
\end{enumerate}

\subsection{Taking All Parts Together}

%В конце нужно собрать все леммы вместе.


\begin{theorem}
    For any two decidable types Terminal and Nonterminal for type of terminals and nonterminals correspondingly. If there exists bijection from Nonterminal to $\mathbb{N}$ and syntactic analysis in the sense of definition~\ref{lst:synt-analysis-is-possible} is possible, then for any DFA \textit{dfa} that accepts Terminal and any grammar $G$, there exists the grammar of intersection $G_{INT}$.
\end{theorem}   

Proof. 
Let $NG$ be the grammar in CNF obtained after applying the algorithm of~\cite{smolkaHofmann2016}. Let \textit{sdfas} be the list of DFAs with exactly one final state obtained after splitting \textit{dfa}.
Since we now have $NG$ in CNF and the list of DFAs with one state, we can compute a list of their intersections.
I.e. we intersect each of the \textit{sdfa} of the list with grammar $NG$.
Next, we find the union of the languages. 

Next, we divide the proof into two branches.
We check whether the empty word is derivable in \textit{dfa} and in grammar $G$.
(1) If so, we add one more rule to the language of the union ($ S \to \varepsilon$). (2) If not, we add nothing.

Now for the cases (1) and (2) we will prove that $G_{INT}$ is the grammar of the intersection indeed.
That is, if a word is derivable in \textit{dfa} and $G$, then it must also be derivable in $G_{INT}$. And vice versa.

For branch (1) we carry out the proof in 2 stages.

\begin{enumerate}
\item[a] Consider the case when $w$ is an empty word.
By assumption, we already know that the empty word is derivable in the DFA and in the grammar. But we also know that we have added a rule from the starting terminal to the empty word to the grammar of intersection. So, if $w$ is an empty word it derivable in both cases.
\item[b] Let's now prove for the case when $w$ is a non-empty word.
We consistently modify the premises and conclusion using theorems about equivalences. First we prove that the fact that $w$ derivable in $G$ and \textit{dfa} implies the fact that $w$ is also derivable in $G_{INT}$. We can safely remove the rule $S \to \varepsilon$, since we apply unification only to grammars in CNF (any grammar of intersection is in CNF), the epsilon rule cannot be used anywhere except the initial step.
We know, since the word is accepted by \textit{dfa}, then there is a DFA with one final state \textit{sdfa}, which also accepts this word. So, we can safely replace \textit{dfa} with \textit{sdfa}. 
For grammar $G$ there is an equivalent grammar $NG$ in CNF. And since word $w$ is not empty, we maintain equivalence.
Let $INT$ be a grammar of the intersection of \textit{sdfa} and $NG$. We can use theorem TODO to prove that it is a grammar of intersection of \textit{sdfa} and $G$ indeed. 
But by the construction, such a grammar belongs to the union of languages $G_{INT}$. this finishes inclusion of $G$ and \textit{sdfa} to $G_{INT}$.
In the other direction: the fact that $w$ is derivable in $G_{INT}$ implies that $w$ is derivable in grammar $G$ and is accepted by DFA \textit{dfa}. 
\end{enumerate}

Grammar $G_{INT}$ consists of a union of the empty language and list languages of the intersection of some sDFA and grammar in CNF. We can safely remove an empty grammar since $w$ is not an empty word and any grammar in the list of languages of intersection is in CNF.
We know that since the word is accepted by $G_{INT}$ grammar, there is a grammar from the union of grammars in which this word is derivable.
But this grammar is a grammar of intersection of some sDFA and grammar in CNF. But now we can use TODO in order to prove equivalence.

The second case is when the empty word is not derivable either in $G$ or in \textit{dfa}.
For an empty word, one needs to prove that it is not derivable in the grammar of the intersection. 
And indeed. None of the grammar from the union has the rule $S \to \varepsilon$. 
Next, one have to repeat what is discussed above, but without the additional steps about the empty language.
