\section{Evaluation} \label{section_evaluation}
To show that the proposed algorithm allows us to effectively apply GPGPU computing techniques, we implement the Algorithm~\ref{alg:graphParse_conj} on a CPU and on a GPU. Also, we apply these implementations to some classical conjunctive grammars~\cite{okhotin2001conjunctive} and synthetic graphs.

Algorithm~\ref{alg:graphParse_conj} is implemented in F\# programming language~\cite{fsharp} and is available on GitHub\footnote{GitHub repository of the YaccConstructor project: \url{https://github.com/YaccConstructor/YaccConstructor}.}. We denote our implementations of the Algorithm~\ref{alg:graphParse_conj} as follows:
\begin{itemize}
    \item onCPU --- an implementation using CSR format for sparse matrix representation and a CPU for matrix operations calculation. For sparse matrix representation in CSR format, we use the Math.Net Numerics\footnote{The Math.Net Numerics WebSite: \url{https://numerics.mathdotnet.com/}.} package.
    \item onGPU --- an implementation using the CSR format for sparse matrix representation and a GPU for matrix operations calculation. For calculations of the matrix operations on a GPU, where matrices represented in a CSR format, we use a wrapper for the CUSPARSE library from the managedCuda\footnote{GitHub repository of the managedCuda library: \url{https://kunzmi.github.io/managedCuda/}.} library.
\end{itemize}

Comparison of the performance of this implementations allows us to determine the efficiency of the GPU acceleration of the Algorithm~\ref{alg:graphParse_conj}.

All tests were run on a PC with the following characteristics:
\begin{itemize}
    \item OS: Microsoft Windows, 64 bit
    \item CPU: 3.60GHz, 4 Cores
    \item GPU: NVIDIA GeForce GTX 1070 (1920 Cuda cores, 1556 MHz, 8192 MB)
\end{itemize}

We consider two queries which correspond to classical conjunctive grammars, and evaluate these queries on random graphs with labels from the set $\{a,b,c\}$.

\textbf{Query 1} is based on the grammar $G^1_S$, which generates the language $\{a^n b^n c^n | n > 0\}$, where:
\begin{itemize}
    \item The grammar $G^1 = (N^1, \Sigma^1, P^1)$.
    \item The set of non-terminals $N^1 = \{S, A, B, C, D\}$.
    \item The set of terminals $\Sigma^1 = \{a, b, c\}.$
    \item The set of production rules $P^1$:
    \[
    \begin{array}{rcclrrccl}
    0: & S & \rightarrow & AB ~\& ~DC  \hfill & \quad 3: & C & \rightarrow & CC ~|~ c\\ 
    1: & A & \rightarrow & AA ~|~ a \hfill &\quad     4: & D & \rightarrow & aDb ~|~ ab \\ 
    2: & B & \rightarrow & bBc ~|~ bc \\
    \end{array}
    \]
\end{itemize}
\begin{table*}
    \centering
    \caption{Evaluation results for queries Query 1 and Query 2 (time in ms)}
    \label{tbl1}
    
    \begin{tabular}{ | c | c || c | c | c || c | c | c |}
        \hline
        \multicolumn{2}{|c||}{Input} & \multicolumn{3}{|c||}{Query 1} & \multicolumn{3}{|c|}{Query 2} \\
        \hline
        $|V|$ & $|E|$ & \#results & onCPU & onGPU & \#results & onCPU & onGPU \\
        \hline 
        \hline
        100 & 25 & 0 & 2 & 7 & 9 & 14 & 67\\
        100 & 75 & 0 & 10 & 20 & 29 & 114 & 129\\
        100 & 200 & 79 & 101 & 213 & 47 & 254 & 483\\
        1000  & 250 & 1 & 265 & 25 & 82 & 2566 & 127\\
        1000 & 750 & 13 & 2781 & 102 & 279 & 21394 & 530\\
        1000 & 2000 & 731 & 12050 & 347 & 438 & 64725 & 1951\\
        10000 & 2500 & 4 & 26595 & 41 & 829 & 268843 & 257\\
        10000 & 7500 & 136 & 241087 & 213 & 2796 & 3380046 & 1675\\
        10000 & 20000 & 4388 & 1305177 & 1316 & 27668 & --- & 3017\\
        \hline
    \end{tabular}
    
\end{table*}

%\begin{table*}[h]
%    \centering
%    \caption{Evaluation results for conjunctive Query 2 (time in ms)}
%    \label{tbl2}
%    
%    \begin{tabular}{ | c | c | c | c | c |}
%        \hline
%        |V| & |E| & \#results & onCPU(in ms) & onGPU(in ms) \\
%        \hline 
%        \hline
%        100 & 25 & 9 & 14 & 67\\
%        100 & 75 & 29 & 114 & 129\\
%        100 & 100 & 47 & 254 & 483\\
%        1000  & 250 & 82 & 2566 & 127\\
%        1000 & 750 & 279 & 21394 & 530\\
%        1000 & 1000 & 438 & 64725 & 1951\\
%        10000 & 2500 & 829 & 268843 & 257\\
%        10000 & 7500 & 2796 & 3380046 & 1675\\
%        10000 & 10000 & 27668 & --- & 3017\\
%        \hline
%    \end{tabular}
%    
%\end{table*}


The grammar $G^1$ is transformed into an equivalent grammar in normal form, which is necessary for the Algorithm~\ref{alg:graphParse_conj}. Let $R'_S$ be an over-approximation of the conjunctive relation for a start non-terminal in the transformed grammar, which is computed by the Algorithm~\ref{alg:graphParse_conj}.

The result of query 1 evaluation is presented in Table~\ref{tbl1}, where $|V|$ is a number of nodes in the graph, $|E|$ is a number of edges, and \#results is a number of pairs $(n,m)$ in the approximation $R'_S$ of the conjunctive relation $R_S$. The implementation which uses a CPU demonstrates a better performance only on some small graphs. We can conclude that acceleration from the $GPU$ increases with the graph size growth.

\textbf{Query 2} is based on the grammar $G^2_S$, which generates the language $\{wcw | w \in \{a,b\}^*\}$, where:
\begin{itemize}
    \item The grammar $G^2 = (N^2, \Sigma^2, P^2)$.
    \item The set of non-terminals $N^2 = \{S, A, B, C, D, E\}$.
    \item The set of terminals $\Sigma^2 = \{a, b, c\}.$
    \item The set of production rules $P^2$:
    \[
    \begin{array}{rccl}
    0: & S & \rightarrow & C ~\& ~ D \\ 
    1: & C & \rightarrow & aCa~|~aCb~|~bCa~|~bCb~|~c\\ 
    2: & D & \rightarrow & aA ~\& ~aD~|~bB ~\& ~bD~|~cE \\ 
    3: & A & \rightarrow & aAa~|~aAb~|~bAa~|~bAb~|~cEa\\ 
    4: & B & \rightarrow & aBa~|~aBb~|~bBa~|~bBb~|~cEb \\
    5: & E & \rightarrow & aE~|~bE~|~\varepsilon \\ 
    \end{array}
    \]

\end{itemize}

The grammar $G^2$ is transformed into an equivalent grammar in normal form. The result of the query 2 evaluation is presented in Table~\ref{tbl1}. On almost all graphs $onGPU$ implementation demonstrates a better performance than $onCPU$ implementation and we also can conclude that acceleration from the GPU increases with the graph size growth.

As a result, we conclude that the Algorithm~\ref{alg:graphParse_conj} allows us to speed up computations by means of GPGPU. We can also use other optimizations for matrix operations to increase the performance of the proposed algorithm.
