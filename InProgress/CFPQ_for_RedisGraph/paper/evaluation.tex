\section{Evaluation and Discussion}

We evaluate all the described implementations on all the datasets and the queries presented.
We compare our implementations with~\cite{Mishin:2019:ECP:3327964.3328503} and~\cite{Kuijpers:2019:ESC:3335783.3335791}.
We measure full time of query execution, so all overhead on data preparing is incuded.
Thus we can estimate applicability of matrix-based algrithm for real-world solutions.

For evaluation, we use a PC with Ubuntu 18.04 installed.
It has Intel core i7-6700 CPU, 3.4GHz, DDR4 32Gb RAM, and Geforce GTX 1070 GPGPU with 8Gb RAM.

The results of the evaluation are summarized in the tables below.
We provide results only for part of the collected data set because of the page limit.
Time is measured in seconds unless specified otherwise.
Note that for all implementations except our own we provide results form related paper.
The cell is left blank if the time limit is exceeded, or if there is not enough memory to allocate the data. %, or information is not available.

{\setlength{\tabcolsep}{0.4em}
\begin{table*}
\caption{RDFs querying results (time in milliseconds)}
\label{tbl:tableRDF}
\rowcolors{3}{}{lightgray}
\begin{tabular}{| p{1.25cm} | c | c |c | c | c | c | c | c | c | c | c | c | c | c |}
    \hline
    \multicolumn{3}{|c|}{RDF}        & \multicolumn{6}{|c|}{Query $G_4$}                               & \multicolumn{6}{|c|}{Query $G_5$} \\
    %\cline{2-13}
    \hline
    Name                                & \#V & \#E  & Scipy & M4RI  & GPU4R & GPU\_N & GPU\_Py & CuSprs & Scipy & M4RI & GPU4R & GPU\_N & GPU\_Py & CuSprs \\
    \hline
    \hline
    \small{funding}                     & 778 & 1480 & 4     &  7    & 4     & 1      & 5       & 279    & 2     & \ltz & 3     & \ltz   & 4       & 274  \\
    \small{pizza}                       & 671 & 2604 & 6     &  8    & 3     & 1      & 6       & 292    & 2     & \ltz & 2     & \ltz   & 5       & 278  \\
    \small{wine}                        & 733 & 2450 & 7     &  6    & 4     & 1      & 7       & 294    & 1     & \ltz & 3     & \ltz   & 3       & 281  \\
    \small{core}                        & 1323 & 8684  & 2     &  4    & 2     & \ltz   & 5       & 273    & \ltz  & \ltz & 1     & \ltz   & 2       & 265  \\
    \small{pathways}                    & 6238 & 37196  & 3     &  5    & 2     & \ltz   & 6       & 268    & 1     & \ltz & 1     & \ltz   & 3       & 271  \\
    \small{go-hierarchy}                    & 45007 & 1960436  & 2     &  4    & 2     & \ltz   & 5       & 266    & 1     & \ltz & 1     & \ltz   & 3       & 266  \\
    \small{enzyme}                        & 48815 & 219390  & 2     &  4    & 2     & \ltz   & 5       & 273    & \ltz  & \ltz & 1     & \ltz   & 2       & 265  \\
    \small{eclass\_514en}                    & 239111 & 1047454  & 3     &  5    & 2     & \ltz   & 6       & 268    & 1     & \ltz & 1     & \ltz   & 3       & 271  \\
    \small{go}                    & 272770 & 1068622  & 2     &  4    & 2     & \ltz   & 5       & 266    & 1     & \ltz & 1     & \ltz   & 3       & 266  \\
    \hline
  \end{tabular}
\end{table*}
}


The results of the first dataset \textbf{[RDF]} are presented in table~\ref{tbl:tableRDF}.
We can see, that in this case the running time of all our implementations is smaller than of the reference implementation, and all implementations but \textbf{[CuSprs]} demonstrate similar performance.
It is obvious that performance improvement in comparison with the first implementation is huge and it is necessary to extend the dataset with new RDFs of the significantly bigger size.

Geospecies dataset currently can be processed only by using CPU version.
So, we can compare our matrix-based CPU implentation with the best result form~\cite{Kuijpers:2019:ESC:3335783.3335791}.
Result is provided in the table~\ref{tbl:geo}.

{\setlength{\tabcolsep}{0.4em}
\begin{table}[H]
\caption{Evaluation results geospecies data}
\label{tbl:geo}
%\rowcolors{1}{}{lightgray}
\begin{tabular}{| c | c | c | c | }
    \hline
     d & CPU    & Neo4j & d         \\
     Time  & Mem  & Time & Mem   \\
    \hline
    \hline
    16   & 0.032    & 0.008  & 0.002   \\
    \hline
\end{tabular}
\end{table}
}


{\setlength{\tabcolsep}{0.4em}
\begin{table}[H]
\caption{Evaluation results for the worst case}
\label{tbl:tableWorst}
\rowcolors{1}{}{lightgray}
\begin{tabular}{| l | c | c | c | c | c | c | }
    \hline
    \#V  & Scipy    & M4RI    & GPU4R  & GPU\_N  & GPU\_Py & CuSprs   \\
    \hline
    \hline
    16   & 0.032    & \ltz    & 0.008  & 0.002   & 0.027   & 0.309    \\
    32   & 0.118    & 0.001   & 0.034  & 0.008   & 0.136   & 0.441    \\
    64   & 0.476    & 0.041   & 0.133  & 0.032   & 0.524   & 0.988    \\
    128  & 2.194    & 0.226   & 0.562  & 0.129   & 2.751   & 3.470    \\
    256  & 15.299   & 1.994   & 3.088  & 0.544   & 11.883  & 15.317   \\
    512  & 121.287  & 23.204  & 13.685 & 2.499   & 43.563  & 102.269  \\
    1024 & 1593.284 & 528.521 & 88.064 & 19.357  & 217.326 & 1122.055 \\
    2048 & -        & -       & -      & 325.174 & -       & -        \\
    \hline
  \end{tabular}
\end{table}
}

Results of the theoretical worst case (\textbf{[Worst]} datatset) are presented in table~\ref{tbl:tableWorst}.
This case is really hard to process: even for a graph of 1024 vertices, the query evaluation time is greater than 10 seconds even for the most performant implementation.
We can see, that the running time grows too fast with the number of vertices.

%
%{\setlength{\tabcolsep}{0.25em}
%\begin{table}[H]
%\caption{Sparse graphs querying results}
%\label{tbl:tableSparse}
%\rowcolors{1}{}{lightgray}
%\begin{tabular}{| l | c | c | c | c | c | c | }
%    \hline
%    Graph              & Scipy   & M4RI     & GPU4R  & GPU\_N & GPU\_Py & CuSprs  \\
%    \hline
%    \hline
%    \small{G5k-0.001}  & 10.352  & 0.647    & 0.113  & 0.041  & 0.216   & 5.729   \\
%    \small{G10k-0.001} & 37.286  & 2.395    & 0.435  & 0.215  & 1.331   & 35.937  \\
%    \small{G10k-0.01}  & 97.607  & 1.455    & 0.273  & 0.138  & 0.763   & 47.525  \\
%    \small{G10k-0.1}   & 601.182 & 1.050    & 0.223  & 0.114  & 0.859   & 395.393 \\
%    \small{G20k-0.001} & 150.774 & 11.025   & 1.842  & 1.274  & 6.180   & -       \\
%    \small{G40k-0.001} & -       & 97.841   & 11.663 & 8.393  & 37.821  & -       \\
%    \small{G80k-0.001} & -       & 1142.959 & 88.366 & 65.886 & -       & -       \\
%    \hline
%  \end{tabular}
%\end{table}
%}
%
The next is the \textbf{[Sparse]} datatset presented in table~\ref{tbl:tableSparse}.
The evaluation shows that sparsity of graphs (value of parameter \texttt{p}) is important both for implementations which use sparse matrices and for implementations which use dense matrices.
Note that the behavior of the sparse matrices based implementation is as expected, but for dense matrices we can see, that more sparse graphs are processed faster.
Reasons for such behavior demand further investigation.
Note that we estimate only the query execution time, so it is hard to compare our results with the results presented in~\cite{fan2018scaling}.
Nevertheless, the running time of our \textbf{[GPU\_N]} implementation is significantly smaller than the one provided in~\cite{fan2018scaling}.

\begin{table*}
\caption{Free scale graphs querying results}
\label{tbl:tableFreeScale}
\rowcolors{1}{}{lightgray}
\begin{tabular}{| l | c | c | c | c | c | c | c | c |}
    \hline
    \multirow{2}{*}{Graph} & \multicolumn{2}{|c|}{CPU} & \multicolumn{2}{|c|}{m4ri} & \multicolumn{2}{|c|}{CUSP} &\multicolumn{2}{|c|}{neo4j}\\
    \cline{2-9}
                           & Time   & Mem              & Time   & Mem               & Time & Mem                 & Time    & Mem \\
    \hline
    \hline
    G(100,1)               & \ltz    & \ltz              & 0.002   & \ltz           & 0.003   & 0.278            & 0.023    & 0.076  \\
    G(100,3)               & \ltz    & \ltz              & 0.002   & 0.001          & 0.004   & 0.279            & 0.105    & 0.098  \\
    G(100,5)               & \ltz    & \ltz              & 0.003   & 0.001          & 0.004   & 0.329            & 1.636    & 0.094  \\
    G(100,10)              & \ltz    & \ltz              & 0.005   & 0.001          & 0.006   & 0.571            & 13.071   & 0.106  \\
    \hline
    G(500,1)               & \ltz    & \ltz              & 0.019   & 0.003          & 0.017   & 1.949            & 93.676   & 0.108   \\ G(500,3)               & 0.003   & \ltz              & 0.125   & 0.038          & 0.150   & 99.651           & 1205.421 & 0.851   \\
    G(500,5)               & 0.005   & \ltz              & 0.552   & 0.315          & 0.840   & 1029.042         & -        & 4.690   \\
    G(500,10)              & 1.239   & 7202              & 7.252   & 5.314          & 15.521  & -                & -        & 70.823  \\
    \hline
    G(2500,1)              & 40.309  & 0.063             & 0.019   & 0.003          & 0.017   & 1.949            & 93.676   & 0.108   \\
    G(2500,3)              & 651.343 & 0.366             & 0.125   & 0.038          & 0.150   & 99.651           & 1205.421 & 0.851   \\
    G(2500,5)              & -       & 1.932             & 0.552   & 0.315          & 0.840   & 1029.042         & -        & 4.690   \\
    G(2500,10)             & -       & 360.035           & 58.751  & 44.611         & 129.641 & -                & -        & 775.765 \\
    \hline
    G(10000,1)             & 0.009   & 1024              & 0.019   & 0.003          & 0.017   & 1.949            & 93.676   & 0.108  \\
    G(10000,3)             & 5.439   & 4353              & 0.125   & 0.038          & 0.150   & 99.651           & 1205.421 & 0.851  \\
    G(10000,5)             & 7.978   & 8193              & 0.552   & 0.315          & 0.840   & 1029.042         & -        & 4.690  \\
    G(10000,10)            & 13.180  & 47362             & 256.579 & 190.343        & 641.260 & -                & -        & -      \\

    \hline
  \end{tabular}
\end{table*}

The last dataset is \textbf{[Full]}, and results are shown in table~\ref{tbl:tableFull}.
As we expect, this case is very hard for sparse matrices based implementations: the running time grows too fast.
This dataset also demonstrates the impact of the grammar size.
Both queries specify the same constraints, but the grammar $G_3$ in CNF contains 2 times more rules then the grammar $G_2$, so, the running time for big graphs differs by more than twice.

Finally, we can conclude that GPGPU utilization for CFPQ can significantly improve performance, but more research on advanced optimization techniques should be done.
On the other hand, the high-level implementation (\textbf{[GPU\_Py]}) is comparable with other GPGPU-based implementations.
So, it may be a balance between implementation complexity and performance.
Highly optimized existing libraries can be of some use: the implementation based on m4ri is faster than the reference implementation and the other CPU-based implementation.
Moreover, it is comparable with some GPGPU-based implementations in some cases.
Sparse matrices utilization demands more thorough investigation.
The main question is if we can create an efficient implementation for sparse boolean matrices multiplication.
