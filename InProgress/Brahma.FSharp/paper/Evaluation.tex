\section{Evaluation}

Implemented.
Sources available here: \url{https://github.com/YaccConstructor/Brahma.FSharp}
Binary package available here: \url{https://www.nuget.org/packages/Brahma.FSharp/}
Examples: \url{https://github.com/YaccConstructor/Brahma.FSharp.Examples}

Matrix multiplication

Substring matching

Substring matching with agents~\cite{BrahmaStringMatching}~\cite{aleaGPUasync}

Results of performance test of GPGPU calculation using Brahma.FSharp and MailboxProcessor  composition are presented.
Problem to solve is substring matching for data carving. Rabin-Karp algorithm was implemented using Brahma.FSharp for substring matching.
F\# MailboxProcessor  used for composing of data reading, data processing on GPGPU, and data processing on CPU.
Library for fast and flexible configuration of MailboxProcessors was created.
Set of templates for search was fixed.
Tests were performed for HDD and SSD storages.
Low level sequential reading was implemented.
First 16.5 Mb was processed.

\begin{itemize}
\item OS: Microsoft Windows 8.1 Pro
\item System Type: x64-based PC
\item Processor: Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz, 3601 Mhz, 4 Core(s), 8 Logical Processor(s)
\item RAM: 16.0 GB
\item HDD for test: 
\begin{itemize}
  \item Model: ST3250410AS
  \item Size: 232.88 GB
  \item 7200 rpm
\end{itemize}
 
\item SSD for test
\begin{itemize}
  \item Model: INTEL SSDSC2BW240A4
  \item Size: 223.57 GB
  \item Max read speed: 540 Mb/sec
\end{itemize}

\item GPGPU:
\begin{itemize}
  \item NVIDIA GeForce GTX 560 Ti
  \item CUDA Cores:     384 
  \item Core clock:     822 MHz 
  \item Shader clock:       1645 MHz
  \item Memory data rate:   4008 MHz
  \item Memory interface:   256-bit 
  \item Memory bandwidth:   128.26 GB/s
  \item Total available graphics memory:    4095 MB
  \item Dedicated video memory: 2048 MB GDDR5
  \item Shared system memory:   2047 MB
\end{itemize}
\end{itemize}

Tables below present results of tests. 
``buffers for data'' --- a number of arrays to fill by disc reader for each MailboxProcessor which communicate with GPGPU.
``threads'' --- a number of MailboxProcessors which communicate with GPGPU.
In current configuration we have  only one GPGU, so all MailboxProcessors use it.
For multi-GPGPU systems  we can configure k MailboxProcessors for each GPGPU.
 
In each cell  --- total time and GPGPU loading graph.

\begin{table*}[ht]
\caption{WEWEW}
\label{tbl:eval1}
\begin{center}
  \begin{tabular}{ l | c | r }
    \hline
    1 & 2 & 3 \\ \hline
    4 & 5 & 6 \\ \hline
    7 & 8 & 9 \\
    \hline
  \end{tabular}
\end{center}
\end{table*}

Conclusion:
Data reading bufferization can sufficiently increase performance.
Especially for HDD, where speed of reading is low.  
For SSD processing with multi-GPGPU systems may be useful.
Data reading is not so critical as for HDD and more than one GPGPU can be fully loaded by using flexible MailboxProcessors configuration.
Configuration with two MailboxProcessors and two buffers for each of them can fully load one GPGPU.




