%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
%\documentclass[sigplan]{acmart}\settopmatter{}


%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\acmConference[PL'18]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2018}{New York, NY, USA}
\acmYear{2018}
\acmISBN{} % \acmISBN{978-x-xxxx-xxxx-x/YY/MM}
\acmDOI{} % \acmDOI{10.1145/nnnnnnn.nnnnnnn}
\startPage{1}

%% Copyright information
%% Supplied to authors (based on authors' rights management selection;
%% see authors.acm.org) by publisher for camera-ready submission;
%% use 'none' for review submission.
\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\copyrightyear{2018}           %% If different from \acmYear

%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%\citestyle{acmauthoryear}  %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

%%
\newcommand\question[1]{{\color{violet}#1}}
\newcommand\todo[1]{{\color{red}#1}}
%%

%%code things
\usepackage{listings}
\usepackage{xcolor}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\begin{document}

%% Title information
\title[Short Title]{Optimizing GPU Programs By Partial Evaluation}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%\titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
%\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
\author{First1 Last1}
\authornote{with author1 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position1}
  \department{Department1}              %% \department is recommended
  \institution{Institution1}            %% \institution is required
  \streetaddress{Street1 Address1}
  \city{City1}
  \state{State1}
  \postcode{Post-Code1}
  \country{Country1}                    %% \country is recommended
}
\email{first1.last1@inst1.edu}          %% \email is recommended

%% Author with two affiliations and emails.
\author{First2 Last2}
\authornote{with author2 note}          %% \authornote is optional;
                                        %% can be repeated if necessary
\orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
\affiliation{
  \position{Position2a}
  \department{Department2a}             %% \department is recommended
  \institution{Institution2a}           %% \institution is required
  \streetaddress{Street2a Address2a}
  \city{City2a}
  \state{State2a}
  \postcode{Post-Code2a}
  \country{Country2a}                   %% \country is recommended
}
\email{first2.last2@inst2a.com}         %% \email is recommended
\affiliation{
  \position{Position2b}
  \department{Department2b}             %% \department is recommended
  \institution{Institution2b}           %% \institution is required
  \streetaddress{Street3b Address2b}
  \city{City2b}
  \state{State2b}
  \postcode{Post-Code2b}
  \country{Country2b}                   %% \country is recommended
}
\email{first2.last2@inst2b.org}         %% \email is recommended


%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
While GPU utilization allows one to speed up computations to the orders of magnitude, it is often challenging to achieve maximum performance. Notably, memory optimizations are being the most significant problem: GPUs memory hierarchy implies certain limitations, thus making data memory allocation management nontrivial and memory to be utilized carefully.
In the paper we propose to automate data memory management leveraging partial evaluation, a program transformation technique that enables the data to be embedded into the code, optimized, and eventually end up directly in the registers.
As an empirical evaluation of our approach we applied the technique to a straightforward CUDA C na\"ive string pattern matching algorithm implementation.
Our experiments show that the transformed program is up to 8 times as efficient as the original one.
\end{abstract}


%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming languages</concept_desc>
<concept_significance>300</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[300]{Social and professional topics~History of programming languages}
%% End of generated code


%% Keywords
%% comma separated list
\keywords{GPU, CUDA, Partial Evaluation}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}

Performance of GPU-based solutions critically depend on data allocation and memory management, since most applications tend to be bandwidth bound problem.
Thus memory optimizations appear to be in a prevailing significance and addressed in a huge amount of research~\cite{10.1007/978-3-319-74313-4_27, Xie2018ICCADU, zhang2019efficient}.
The GPUs memory access latency varies between different memory types, from hundreds of cycles for global memory to just a few for shared and register memory.
Moreover, the latency could be aggravated by wrong access patterns or misaligned accesses and the possibility of proper access patterns could depend on the domain of the problem being solved.
For example, the global memory access pattern could be not clear, thus preventing GPU from efficient coalescing. 
It imposes a burden of memory management to a programmer or makes one rely on caching mechanisms. 

In order to achieve the fastest memory access constant, shared or registers memory should be utilized.
However, constant memory lacks flexibility in the sense that the size of data should be known beforehand and access pattern also should be kept in mind.
Shared memory should be used carefully due to considerations of synchronization and bank conflicts, while register allocation is managed by the compiler and explicit storing of data to them is difficult.
E.g. small arrays could be stored in registers, but only if the compiler is able to figure out that arrays indexing is static and if it does not, the array would end up in local memory.
Moreover, all these approaches require to create a special nontrivial code for manual data allocation management which makes algorithm implementation harder.
Finally, such optimizations require specific knowledge from developers.
One of the ways to solve these problems is to automate memory management.

At the same time, a common workflow of a GPU-based solution has a feature that allows one to introduce runtime optimizations which originally are static.
Suppose the next scenario.
We have created an interactive solution for huge data analysis.
The user can sequentially write queries to a dataset, and GPU kernel is used for query processing.
Suppose that query is relatively small (compared to data), data is huge and thus query execution time is significant.
Simple examples of such scenarios are multiple pattern matching, database querying, convolutional filters applying.
The kernel should be generic and have at least two parameters: query and data.
But at the moment, when the user specifies a query and the host code is ready to run GPU kernel, the query could be used as a static data for the kernel optimization.

There is a known program optimization technique that optimizes a given program with respect to statically known inputs, producing another, optimized, program which if given only the remaining dynamic inputs yields the same results as the initial one being executed given both inputs.
The technique is \textit{partial evaluation} (aka program specialization)~\cite{Jones:1993:PEA:153676,PartialEvalPaper}.
Basically, given a function $f$ of $n$ arguments with some of them being static, denoted with $k$, partial evaluator evaluates or \textit{specializes} those parts of the function depending only on static arguments, producing a residual function $f'$ of $(n-k)$ arguments.
Thus it produces a more optimal function in a sense that the specialized function being invoked makes fewer computations than the original one.

Application of specialization for one of the described scenarios, namely database querying, have been known to significantly improve query execution performance~\cite{10.1007/978-3-319-74313-4_27}.
Given that regarding GPU memory management partial evaluation is able to produce an optimization for memory access, could similar results be achieved for GPU-based applications? 
Particularly, considering the problem of multiple patterns matching with a known set of patterns, the result of memory access for a particular pattern could be embedded into the code during compilation, rather than being compiled to load instructions for different memory spaces. 
More precisely, partial evaluation results in data being accessed through instruction cache. 
In this work, we propose to apply \emph{partial evaluation} for GPU code optimization.

%Consider the snippet of code from \hyperref[lst1]{\textbf{Listing 1}}.
%Suppose the array named \textit{template} and its size \textit{template\_size} are statically known. The array \textit{ibuffer} is dynamic.
%Given that memory access indexing and the content of the array are static, the snippet could be transformed to \textit{.ptx} instructions during the compilation as presented in \hyperref[lst2]{\textbf{Listing 2}}.
%The thing is the content of \textit{template} array has been embedded into the comparison instructions rather than to be globally loaded multiple times as in \hyperref[lst2]{\textbf{Listing 3}}.  
%\lstset{style=mystyle}
% \begin{figure}[b]
% \begin{lstlisting}[language=C,caption=Partial evaluation example,label=lst1]
% /*
% *   Suppose we have static template_size and
% *    template array itself, assume ['a',...] 
% */

%  for i in unroll(0,template_size) {
    % //global/constant memory access
    % if ibuffer(t_id + i as i64) != template(i) { 
        % //... Do something ... //
    % }
%  }
% /*
% *   After the specialization the code above        
% *    is transformed to an unrolled list of 
% *   akin .cu instructions
% */
    % ...
%  bool _93951;
%  _93951 = _93949 != 97; // 97 is ascii for 'a'
%  if (_93951) goto l93952;
%  else goto l93955;
    % ...
% \end{lstlisting}
% \end{figure}

% here are some silly things =)
% As long as the limitation on the number of registers per thread is met, all static data eventually \todo{get} into registers memory.
% \question{Moreover, if excessive usage of register memory happens, data would automatically leak to cached local memory resulting in automatic memory management, i.e. avoiding explicit allocations.}


%\begin{figure}[b]
%\begin{lstlisting}[language=C,caption=Partial evaluation example,label=lst1]
%/*
%*   assume  template is ['\x49','\x44',...]  
%*/
%
% for i in unroll(0,template_size) {
%    //global/constant memory access
%    if ibuffer(t_id + i as i64) != template(i) { 
%        //... Do something ... //
%    }
% }
%\end{lstlisting}
%\end{figure}

%\begin{figure}[b]
%\begin{lstlisting}[language=C,caption=Code after partial evaluation,label=lst2]
%    ...
% LDG.E.U8 R0, [R4] ; //load from global memory, i.e. ibuffer(...)
% BFE R6, R0, 0x1000 ;
% ISETP.NE.AND P0, PT, R6, 0x49, PT ; //0x49 got extracted, so we have avoided global memory access!
% @P0 SYNC ;
% LDG.E.U8 R0, [R4+0x1] ;
% BFE R0, R0, 0x1000 ;
% ISETP.NE.AND P0, PT, R0, 0x44, PT ; //extracted again!
%    //and so on
%    ...
%\end{lstlisting}
%\end{figure}

%\begin{figure}[b]
%\begin{lstlisting}[language=C,caption=Code without partial evaluation,label=lst3]
% LDG.E.U8 R8, [R8] //load global
% LDG.E.U8 R6, [R6] ; // load global
% BFE R13, R8, 0x1000 ;
% BFE R12, R6, 0x1000 ;
% ISETP.NE.AND P0, PT, R13, R12, PT ; //compare pre-loaded registers

%\end{lstlisting}
%\end{figure}


%\todo{Given that caches tend to undergo misses and random access could hurt coalescing or broadcasting}, we exploit partial evaluation techniques to specialize a GPU program on static data in such a way to move the data straight into the code rather than to any memory space.

\section{Evaluation}

As an instance of a problem, consider the \textit{file carving}~\cite{DataCarving}, in a field of \textit{cyber forensics} it stands for extracting files from raw data, i.e. from lost clusters, unallocated clusters and slack space of the disk or digital media.
To extract a file, we should detect a specific file header: for a predefined set of file types, the headers to search for are known beforehand and commonly are relatively short.

The partial evaluator being used is one developed as part of \textit{AnyDSL} framework~\cite{LeiBa}.
So, we compare AnyDSL framework implementation leveraging partial evaluation with respect to the file headers against two base-line implementations in CUDA C with global and constant memory for header access respectively. 
All implementations invoke the algorithm in a separate thread for each position in the subject string.
The headers are stored as a single char-array and accessed via offsets.
The algorithm simply iterates over all headers searching for a match, if it encounters a mismatch, it jumps to the next header forward through the array.


The approach has been evaluated on Ubuntu 18.04 system with \textit{Intel Core i7-6700} processor, 8GB of RAM and \textit{Pascal}-based \textit{GeForce GTX 1070} GPU with 8GB device memory.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.46\textwidth]{results.pdf}
  \caption{Multiple string pattern matching evaluation}
  \label{fig:eval}
\end{figure}

To estimate the performance gain brought by partial evaluation the problem of file carving has been evaluated.
For the evaluation, the piece of data of 4 GB size has been taken from a hard drive and patterns to be searched have been taken from a taxonomy of file headers specifications~\cite{Headers}.
The headers have been divided into groups of size 16 and run over multiple times.
The results are presented in Figure~\ref{fig:eval}.
The points are the average kernel running time and the grey regions are the area of standard deviation.   

Since the headers could be lengthy and mismatches happen quite often, such an access pattern hurts coalescing, increasing the overall number of memory transactions. 
Given that, the performance speedup of a partially evaluated algorithm achieves on a raw data piece of 4 GB size is up to $8$ compared to CUDA C version with global memory and up to $3$ with constant one as illustrated in Figure~\ref{fig:eval}. Namely, the partially evaluated version spends about 300 ms for searching while global and constant memory CUDA C versions making it in 800 ms and 2500 ms respectively.  
Note that specialization time is 2 sec, so in case of analysing 1 Tb storage, performance improvement would be significant. 


\section{Conclusion}
In this work, we apply partial evaluation to optimize GPU programs.
We show that this optimization technique in the context of file carving problem can improve the performance up to $8$ times if compare na\"ive implementation executed with optimization and without it.
Note that optimizations do not require manual manipulation with source code and implementation of additional stuff for memory management.
  
The partial evaluator being used assumes the programs to be written with special \textit{DSL}.
Nevertheless, partial evaluation could be applied to general-purpose languages, and CUDA C and the upcoming research is dedicated to the generalization of the technique so as the partial evaluation could be applied at runtime, during GPU-based application execution.
Moreover, the performance of partial evaluator should be improved in order to decrease specialization overhead.

Specialization can produce code with a huge number of variables, and it can make register management harder. 
Could our partial evaluation be combined with advanced register spilling techniques (for example with~\cite{Sakdhnagool2019RegDemIG}) to improve performance?

%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}


%% Bibliography
\bibliography{bib}


%% Appendix
% \appendix
% \section{Appendix}

% Text of appendix \ldots

\end{document}
