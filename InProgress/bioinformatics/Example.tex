% 6 - 7  pages including all
\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{hyperref}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.


\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\begin{document}

\title{The Composition of Dense Neural Networks and Formal Grammars for Secondary Structure Analysis}

\author{\authorname{Semyon Grigorev\sup{1,2}, Polina Lunina\sup{1,2}}
\affiliation{\sup{1}St. Petersburg State University, 7/9 Universitetskaya nab., St.Petersburg, Russia}
\affiliation{\sup{2}JetBrains Research, Universitetskaya emb., 7-9-11/5A, St.Petersburg, Russia}
\email{s.v.grigoriev@spbu.ru, Semen.Grigorev@jetbrains.com, lunina\_polina@mail.ru}
}

\keywords{Dense Neural Network, DNN, Machine Learning, Secondary Structure, Genomics Sequences, Proteomics Sequences, Formal Grammar, Parsing}

\abstract
{
We propose a way to combine formal grammar and artificial neural networks for biological sequences processing.
Formal grammar is used for utilization information about sequence secondary structure and neural networks allow us to deal with mutations and noise.
In contrast to the classical way when \textbf{probabilistic} grammars are used for secondary structure modeling, we propose to use \textbf{arbitrary (not probabilistic)} grammars which simplifies grammar creation.
Instead of modeling the structure of the full sequence, we create a grammar which describes features of the secondary structure.
Then we use undirected matrix-based parsing for features extraction: the fact that some substring is derivable from some nonterminal is a feature. 
And after that, we use a dense neural network for features processing.
In this paper, we provide a detailed description for all the parts of our receipt: grammar, parsing algorithm, and network architecture.
Also, we discuss possible ways for improvements and future work.
Finally, we provide the results of tRNA and 16s rRNA processing which shows that our idea can be applied to solve real problems.
}

\onecolumn \maketitle \normalsize \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

\noindent Accurate, fast, and precise sequences classification and subsequences detection are the open problems in different areas of bioinformatics, such as genomics and proteomics. 
Challenge here is a high variability of sequences belonging to the same class (or one want to mark as the same class).
Probabilistic models, such as Hidden Markov's Models (HMMs) or probabilistic (stochastic) grammars (PCFGs, SCFGs), can help to deal with variability.
Formal grammars are more successful in long-distance connections handling.
Moreover, grammars can model the secondary structure of sequences more explicitly.

For example, algorithms that can efficiently and accurately identify and classify bacterial taxonomic hierarchy have become a focus in computational genomics.
The idea that the secondary structure of genomic sequences is sufficient for solving the detection and classification problems lies at the heart of many tools~\cite{GrammarsRNA,PCFG,meta,LWPCFG}.
One of the ways to specify the secondary structure is to use formal grammars. 
The problem here is that the sequences obtained from the real bacteria usually contain a huge number of mutations and noise which renders precise methods impractical. 
Probabilistic grammars and covariance models (CMs) are a way to take the noise into account~\cite{EddyDurbin}, but it is difficult to create (train or learn) high-quality grammar or model.
However, CMs are successfully used in some tools, for example, the Infernal tool~\cite{Infernal}.

Neural networks are another way to deal with noisy data. 
The works~\cite{Humidor,ANN} utilize neural networks for 16s rRNA processing and demonstrate promising results.
But these works do not use information about the secondary structure of sequences.

In this work, we propose a way to combine formal grammars, which involves secondary structure features, and neural networks for sequences processing.
The key idea is not to try to model the full (sub)sequence of interest by grammar, but to create a grammar which describes features of secondary structure and to use a neural network for these features processing.
We provide an evaluation of the proposed approach for tRNA classification and 16s rRNA detection.
Results show that the proposed approach may be applicable.

\section{\uppercase{Proposed solution}}
\label{sec:proposedSolution}

\noindent We propose to combine neural networks and ordinary context-free grammars (not probabilistic which are usually used in this area) in order to handle information of sequences' secondary structure. 
Namely, we propose to extract secondary structure features by using the ordinary context-free grammar and use the dense neural network for features processing.
Features can be extracted by any parsing algorithm and then presented as a boolean matrix, but we choose the parsing algorithm based on matrix multiplication.

In this section, we describe all the components of our recipe and provide some examples end explanations.
 
\subsection{Context-Free Grammars}

\noindent The first component is a context-free grammar. 
It is a well-known fact that the secondary structure of the sequence may be approximated by using formal grammars.
There is a number of works that utilize this fact for different purposes~\cite{GrammarsRNA,LWPCFG,zier2013rna,knudsen2003pfold}.

Probabilistic context-free grammars are usually used for secondary structure modeling because it allows to deal with variations (mutations or some kinds of noise).
In the opposite of it, we use ordinary (not probabilistic) grammars.
Our goal is not to model the secondary structure of the whole sequence (which requires probabilistic grammars), but to describe features of the secondary structure, such as stems, loops, pseudoknots and their composition.
Of course, the set of feature types is limited by the class of grammar which we use.
For example, pseudoknots can not be expressed by context-free grammars, but can be expressed using conjunctive grammars~\cite{KanchanDevi2017,zier2013rna,Okhotin:2001:CG:543313.543323} or multiple context-free ones~\cite{SEKI1991191,Riechert:2016:ADP:2972703.2972851}.

The context-free grammar $G_0$ which we use in our experiments is presented in figure~\ref{fig:cfg-rna}.
It is a context-free grammar over the four-letters alphabet $\Sigma=\{A,C,G,T\}$ with start nonterminal \verb|s1|.
This grammar describes composition of stems with bounded minimal height.

\begin{figure}
\begin{verbatim}
s1: stem<s0> any

any_str : any*[2..10]

s0: any_str | any_str stem<s0> s0

any: A | T | C | G

stem1<s>: A s T | G s C | T s A | C s G 

stem2<s>: stem1< stem1<s> >

stem<s>:  
      A stem<s> T 
    | T stem<s> A 
    | C stem<s> G 
    | G stem<s> C 
    | stem1< stem2<s> >  
 } 
\end{verbatim}
\caption{Context-free grammar $G_0$ for RNA secondary structure features extraction}
\label{fig:cfg-rna}
\end{figure}

First of all, we provide a brief description of grammar specification language.
Left-hand side and right-hand side of the rule are separated by the \verb|:| sign.
In the right-hand side, one can use extended regular expressions over union alphabet of terminals and nonterminals.
Such constructions as bounded repetition and alternative are available.
For example, \verb|any*[2..10]| is a bounded repetition and it stands that the nonterminal \verb|any| may be repeated any number of times from 2 up to 10.
Example of the rule which uses alternatives is \texttt{any: A | T | C | G} which stands that \verb|any| is one of the four terminals.

Another important feature of the language is the existance of parametric rules or meta-rules which allow one to create reusable grammar templates.
More details on meta-rules one can find in~\cite{Thiemann:2008:MCG:1389449.1389465}.
The example of meta-rule in our grammar is \texttt{stem1<s>: A s T | G s C | T s A | C s G}.
This rule has one parameter \verb|s| which stands for something that should be embedded into a stem.
Application of this rule to \verb|any_str| allows one to define stem with a loop of length from 2 up to 10.
In our grammar we use meta-rules in order to describe stems with bounded minimal height: \verb|stem1<s>| is a stem with height exactly 1,  \verb|stem2<s>| is a stem with height exactly 2, and \verb|stem<s>| is a stem with height not lower than 3.

Now we explain what this grammar meens.
This grammar describes a recursive composition of stems.
To see it one can look at the rule for \verb|s0| which is recursive and shows that composition of stems may be embedded into the stem (\verb|stem<s0>| in the right side of this rule).
Every stem should have height not lower than 3 and can be built only from classical base pairs.
Stems may be connected by an arbitrary sequence of length from 2 up to 10, and loops have the same length.
Graphical explanation of this description one can find in figure~\ref{fig:cfg-rna-graphical}.

\begin{figure}
\centering
\includegraphics[width=.45\textwidth]{figures/16sgrammar.pdf}
\caption{Graphical explanation of pattern which is described by grammar in the figure~\ref{fig:cfg-rna}}
\label{fig:cfg-rna-graphical}
\end{figure}

Note that grammar is a variable parameter and may be tuned for specific cases.
The grammar presented above is a result of a set of experiments, so there are no reasons to stand that it is the best grammar for secondary structure features extraction.
For example, one can vary length of unfoldable sequence by changing rule for \verb|any_str|: \verb|any_str : any*[0..10]|, \verb|any_str : any*[1..8]|, or something else.
Also, one can increase (or decrease for some reason) the minimal height of stem or add some new features, such as pseudoknots, to the grammar (in case of usage of conjunctive grammars instead of the context-free one).


\subsection{Parsing Algorithm}

\noindent In the classical scenario, parsing is used for answering the question of whether or not the given sequence is derivable in the given grammar.
Additionally, in case if a sequence is derivable, derivation tree may be provided as a result of parsing. 
It is a classical way: there is a huge number of works on modeling secondary structure of the full sequence of interest by using probabilistic grammars and respective parsing techniques~\cite{knudsen2003pfold,Browny1993StochasticCG,Knudsen2005StochasticCG}.
We propose to use parsing as a feature extraction: we want not to check the derivability of the given string or find the most probable derivation but to find all the derivable substrings of given string for all nonterminals.
So, we use undirected parsing.

CYK~\cite{Younger1967RecognitionAP} is a classical well-known algorithm for undirected parsing. 
This algorithm and its modifications are traditionally used for PCFG/SCFG processing and, as a result, are used in a number of tools, but they demonstrate a poor performance on long sequences and big grammars~\cite{Liu2005}.

An alternative approach is the usage of algorithms based on matrix multiplication, such as Valiant's algorithm~\cite{Valiant:1975:GCR:1739932.1740048} which provides subcubic parsing.

In our work, we use another version of matrix-based algorithm~\cite{Azimov:2018:CPQ:3210259.3210264}.
The theoretical time complexity of this algorithm is worse than the complexity of the Valiant's algorithm, but in practice, this algorithm avoids machinery on submatrices manipulation and demonstrates better performance along with a simple implementation.

From the practical point of view, matrix-based algorithms allow to easily utilize advanced techniques, such as algorithms for sparse and boolean matrices, GPGPU-based libraries, etc.

Moreover, the matrix-based approach can be generalized to conjunctive and even boolean grammars~\cite{OKHOTIN2014101}, as far as to multiple context-free grammars~\cite{mcfgMatrices}, which can provide a base for more expressive features descriptions handling without significant changes in other parts of our solution.

\subsection{Matrices}

\noindent The result of parsing is a set of square boolean matrices. 
Each matrix $M_N$ contains information of all substrings which can be derived from the nonterminal $N$.
In other words, $M_N[i,j]=1$ iff $N \Rightarrow^*_G w[i,j-1]$ where $w$ is the input sequence and $G$ is a context-free grammar, and $N$ is a nonterminal.
Thus, the result of parsing is a set of matrices: one matrix for each nonterminal from grammar.
For further processing, we can select nonterminals of interest.
In our case, for grammar $G_0$ we select matrix for the nonterminal \verb|s1|.

\begin{figure}
\centering
\includegraphics[width=.45\textwidth]{figures/4.pdf}
\caption{Parsing result for sequence which should folds to stem}
\label{fig:matrix-simple-stem}
\end{figure}

The example of such matrix is provided in figure~\ref{fig:matrix-simple-stem}.
This matrix is a result of parsing of the sequence { \center{$w_1=$\texttt{CCCCATTGCCAAGGACCCCACCTTGGCAATCCC}} \\} w.r.t the grammar $G_0$.
In the figure, one can see an upper right triangle of parsing matrix (bottom left is always empty, so omitted) with input string on the diagonal.
Note that string is added only for example readability and real matrix does not contain input string, only results of its parsing.
Each filled cell $[i,j]$ which contains 1 is denote that subsequence $w_1[i,j-1]$ is derivable from \verb|s1| in $G_0$ (so, this subsequence folds to stem with heigh 3 or more).
In order to find stems with height more than 3 one should detect diagonal chains of 1-s: in our example stem has height equal to 10 and one can find chain of 1-s of the length $8=10-2$ (first 1 is a root of the stem of height 3 and each next 1 is a new base pair upon this stem --- root of the stem with height increased by one).
Red boxes and contact map are added for navigation simplification.

Our goal is to extract all the features of the secondary structure, so, our parser finds all the substrings which can be derived from \verb|s1|.
As a result, there are some 1-s out of the chain.
These are correct results: corresponded subsequences can be derived from \verb|s1|. 
In the current example these 1-s may be treated as noise in some sense, but, as we show later, such behavior may be useful in some cases.
Moreover, for long sequences with the complex structure, it may be not evident, which features of the secondary structure are principal.

We use these matrices as an input for the artificial neural network which should detect sufficient features (long chain in our example) and utilize them for applied problem solution (sequence detection or classification, for example).
We drop out the bottom left triangle and vectorize matrices row-by-row in order to get a bit vector, then convert it to a byte vector and use as an input.
The transition from bit vector to byte vector is done in order to decrease the input size which is critical for long sequences. 
On the other hand, such operation may significantly complicate network architecture and training, and it is a reason to try to use bitwise networks~\cite{DBLP:journals:corr:KimS16} in the future.  

\subsection{Artificial Neural Networks}

\noindent Artificial neural networks are one of the possible choices for different classification problems in the case when data has a hard-to-formalize principal for problem features and contains some kinds of noise. Different types of networks are successfully utilized for images, speech, and natural languages processing.

Classical scenario for classification problems is to provide features vectors and try to classify them, which means that network can select important features for each required class.
In our case, the fact that $w[i,j-1]$ is derivable from nonterminal $N$ which is encoded in the matrix is exactly a feature.
So, the vectorized matrix is a vector of features which is a typical input for a neural network.

We use dense neural network because data locality is broken during vectorization and any convolutions are inapplicable.
Moreover, convolutions are used mostly for features extraction, but in our case features are already extracted by parsing.
Thus we need only to detect principal features and relations between them.
And it is a typical area for dense networks.

One of the problems with arbitrary data processing by using neural networks is the input size normalization.
The input layer of the network has fixed size, but input sequence length and hence the length of vectorized parsing result may be varied even for the fixed task.
For example, the length of tRNA may be approximately from 59 up to 250.
We propose two possible ways to solve this problem.
The first way is subsequences processing: for some tasks, it may be enough to process not a full sequence, but only its subsequence.
In this case we can set the length of subsequence lower than the shortest sequence which we want to handle. 
The second way is to set an upper bound and fill the gap with special symbols.
For example, while handling tRNAs we can set the input length to 250 and when we want to process sequence of length 60, then we should fill the rest 190 places by selected special symbol.

The example of the neural network which we use is presented in figure~\ref{fig:nn}.
We actively use dropout and batch normalization because network should perform a number of nontrivial transformations: decompress data from bytes and prepare normalized input which requires additional power.
Despite the fact that initially, batch normalization is an alternative for dropout~\cite{DBLP:journals:corr:IoffeS15}, we use both of them together because separate use has no effects.

\section{Examples}
\label{sec:examples}

\noindent Here we provide more examples of matrices and point out some observations about it in order to provide better intuition on our idea.

\begin{figure}
\centering
\includegraphics[width=.45\textwidth]{figures/5.pdf}
\caption{Parsing result for sequence which should folds to pseudoknot}
\label{fig:pseudoknot}
\end{figure}

The first part is the observation about pseudoknots. 
Let consider the following sequence which can fold to pseudoknot as an example: {\center{$w_2=$\texttt{CCACTTACCTATGACCTAAGTCCTCATACC.}\\}}
Note, that loops are very short for example minimization.
As mentioned above, pseudoknots cannot be expressed in terms of context-free grammars. 
But one can mean pseudoknot as a two crossing stems in some sense, and the parser can extract both of them, as presented in figure~\ref{fig:pseudoknot}.
So, if a neural network is powerful enough, it can detect that if these two features appear simultaneously, then the sequence contains pseudoknot.
As a result, we can detect features which are not expressible in context-free grammars using the proposed way.

The second is an example of a matrix for real tRNA.
Parsing result of the tRNA\footnote{The sequence Novosphingobium\_aromaticivorans\_DSM\_12444\_chr.trna57-GlyGCC (268150-268084)  Gly (GCC) 67 bp Sc: 22.97. From GtRNAdb: \url{http://gtrnadb2009.ucsc.edu/download.html}. Access date: 02.11.2018.} sequence {\center{$w_3=$\texttt{CAGGGCATAACCTAGCCCAACCTTGCCAAGG \\ 
 TTGGGGTCGAGGGTTCGAATCCCTTCGCCCGCTCCA} \\ }} is presented in figure~\ref{fig:real-trna}. Also, one can see predicted secondary structures\footnote{Predicted secondary structures are given by using the Fold Web Server with default settings: \url{http://rna.urmc.rochester.edu/RNAstructureWeb/Servers/Fold/Fold.html} Access date: 02.11.2018.} (top two) in figures~\ref{fig:real-trna-folding1} and~\ref{fig:real-trna-folding2}.

\begin{figure}
\centering
\includegraphics[width=.45\textwidth]{figures/Fold1.pdf}
\caption{Predicted secondary structure for $w_3$}
\label{fig:real-trna-folding1}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=.45\textwidth]{figures/Fold2.pdf}
\caption{Predicted secondary structure for $w_3$}
\label{fig:real-trna-folding2}
\end{figure}

Colored boxes in figure~\ref{fig:real-trna} marks features which correspond to these two predicted foldings: blue marks for~\ref{fig:real-trna-folding1} and red for~\ref{fig:real-trna-folding2}.
Note, that our grammar $G_0$ handles only classical base pairs, so the pair \verb|G - T| which exists in predicted foldings, is not presented in parsing result.
Anyway, we can see that all expected information on the secondary structure is presented in the matrix, of course, with some additional features.
And it is a field for neural networks --- to select appropriate features.

\begin{figure*}
\centering
\includegraphics[width=.98\textwidth]{figures/0m.pdf}
\caption{Parsing result for real tRNA ($w_3$)}
\label{fig:real-trna}
\end{figure*}

Thus we can conclude, that very nontrivial compositions of secondary structure features may be detected by using powerful enough neural networks.
It is an interesting question for future research: what kinds of applications may be built by using such results?

\section{\uppercase{Evaluation}}
\label{sec:evaluation}

\noindent We evaluate the proposed approach on two cases: 16s rRNA detection and tRNA classification.
Note that the goal of the evaluation is to demonstrate the applicability of approach which described above.
So, we do not provide a comparison with existing tools and we do not try to solve real problems.
All of these are tasks for future work.

\subsection{16s rRNA Sequences}
\noindent The first problem is 16s rRNA detection.
We specify context-free grammars which detect stems with the height of more than two pairs and their arbitrary compositions (namely, $G_0$).
For network training we use a dataset consisting of two parts: random subsequences of 16s rRNA sequences from the Green Genes database~\cite{pmid16820507} form positive examples, while the negative examples are random subsequences of full genes from the NCBI database~\cite{pmid19854944}.
All sequences have the length of 512 symbols, totally up to 310000 sequences.
After training, current accuracy is 90\% for validation set (up to 81000 sequences), thus we conclude that our approach is applicable.

\subsection{tRNA Sequences}

\noindent The second problem is tRNA classification: we train a neural network to separate tRNAs into two classes: prokaryotes and eukaryotes.
We prepare 50000 sequences from GtRNADB~\cite{Chan2009} for training: 35000 for training and 15000 for testing.
In this case, we use the next trick for data size normalization.
We set the upper bound of sequence length to 220 and after that we align real tRNA sequence $w$ in the following way: first $k$ symbols of the input is $w$ ($|w|=k$) and the rest $220-k$ symbols are filled by \verb|$| --- a special symbol which is not in input alphabet.

Also, we prepare validation set which contains 217984 sequences for prokaryotes and 62656 sequences for eukaryotes.
All data for validation was taken from tRNADB-CE\footnote{tRNADB-CE: tRNA gene database curated manually by experts. URL: \url{http://trna.ie.niigata-u.ac.jp/cgi-bin/trnadb/index.cgi}. Access date: 31.10.2018}~\cite{Abe2010}.

The architecture of the network which we use in this experiment is presented in figure~\ref{fig:nn}.
Note that it is a training configuration: it contains dropout and batch normalization layers which will be removed after training.
This network contains six dense layers and uses \verb|relu| and \verb|sigmoid| activation functions.

\begin{figure}
\centering
\includegraphics[width=.4\textwidth]{figures/model-crop.pdf}
\caption{architecture of the neural network for tRNA classification}
\label{fig:nn}
\end{figure}

After training our network demonstrates accuracy of 97\%. 
For validation set we get the following results: 3276 of eukaryotes (5.23\% of all eukaryotes) are classified as prokaryotes and 4373 of prokaryotes (2.01\% of all prokaryotes) are classified as eukaryotes. 

As a result, we can conclude that input normalization by filling sequence to upper bound of length by special symbol is working.
Also, we can state that secondary structure contains sufficient information for classification.


\section{\uppercase{Discussion and Future Work}}
\label{sec:Discussion}

\noindent The presented is a work in progress. 
The ongoing experiment is finding all instances of 16s rRNA in full genomes.
Also, we plan to use the proposed approach for the filtration of chimeric sequences and classification.
A composition of our approach with other methods and tools as well as grammar tuning and detailed performance evaluation may improve the applicability for the real data processing.

One of the problems of the proposed approach is that parsing is a bottleneck of performance.
A possible solution is to construct a network which can handle sequences instead of parsing data.
It may be done in the following way.
\begin{enumerate}
\item Create a training set of matrices using parsing.
\item Build and train the network $NN_1$ which can handle vectorized matrices.
\item Create new network $NN_2$ by extending $NN_1$ with a head (set of layers) which should convert the sequence to input for $NN_1$
\item Train $NN_2$. Weights of layers from $NN_1$ should be fixed.
\item For concrete problem we can tune weights of $NN_2$ to get an appropriate quality.
\end{enumerate}
This way we can use parsing only for training which is less performance critical step than usage in application.

Another task is to understand the features which network extracts in order to get inspiration in, for example, grammar tuning.
It may be done by trained network visualization.
There is a set of tools for user-friendly convolutional networks visualization, but not for dense networks.
It may be useful to create such a tool and customize it for our domain.

We do some experiments in genomic sequence analysis, but what about proteomics?
Works on grammar-based approaches to proteomics sequences analysis have a long history~\cite{Jimenez-Montaño1984,Dyrka2008ASC,Sciacca2011AnnotatedSC}%,DBLP:Witold:Proteins}
This area provides new challenges, such as more complex grammar, more symbols in the alphabet, more complex rules of interactions, more complex features.
As a result, more powerful languages may be required in this area.
So, it may be interesting to apply the proposed approach to proteomics sequences analysis.
One of the possible crucial problems is to detect functionally equivalent sequences with sufficiently different length.

Also, it may be interesting to use other types of neural networks.
Bitwise networks~\cite{DBLP:journals:corr:KimS16} may be reasonable because the result of parsing is a bitwise matrix, so it looks like a natural way to use these networks to process such result. 
Another direction is convolutional networks utilization.
One can treat parsing matrices as bitmaps: one can set a specific color for each nonterminal and get a multicolor picture as a sum of matrices.
The problem here is a picture size: typical matrix size is $n \times n$ where $n$ is a length of the input sequence.

An important part of work is a training data preparation.
One of the difficult problems is creation of a balanced dataset.
Biological datasets (like GreenGenes) contain a huge number of samples for some well-studied organisms and a very small number of samples for others.
Moreover, datasets often contain unclassified and candidatus sequences.
It is not evident how we should prepare datasets in order to get a high-quality trained network.

To conclude, our work is in the beginning stage and current results are promising. 
There is a huge number of experiments in different directions which may be potentially interesting.
In order to choose the right direction, we hope to discuss future work with the community.


\section*{\uppercase{Acknowledgements}}

\noindent The research was supported by the Russian Science Foundation grant 18-11-00100 and a grant from JetBrains Research.


%\vfill

\bibliographystyle{apalike}
{\small
\bibliography{example}}


\vfill
\end{document}

