\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.

\subfigtopskip=0pt
\subfigcapskip=0pt
\subfigbottomskip=0pt

\begin{document}

\title{Authors' Instructions: Preparation of Camera-Ready Contributions to SCITEPRESS Proceedings}

\author{\authorname{First Author Name\sup{1}, Second Author Name\sup{1} and Third Author Name\sup{2}}
\affiliation{\sup{1}Institute of Problem Solving, XYZ University, My Street, MyTown, MyCountry}
\affiliation{\sup{2}Department of Computing, Main University, MySecondTown, MyCountry}
\email{\{f\_author, s\_author\}@ips.xyz.edu, t\_author@dc.mu.edu}
}

\keywords{The paper must have at least one keyword. The text must be set to 9-point font size and without the use of bold or italic font style. For more than one keyword, please use a comma as a separator. Keywords must be titlecased.}

\abstract{
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
Abstract is very abstract
}

\onecolumn \maketitle \normalsize \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

\noindent Algorithms that can efficiently and accurately identify and classify bacterial taxonomic hierarchy have become a focus in computational genetics.
The idea that secondary structure of genomic sequences is sufficient for solving the detection and classification problems lies at the heart of many tools~\cite{GrammarsRNA,PCFG,meta,LWPCFG}. 
The secondary structure can be specified in terms of formal grammars. 
The sequences obtained from the real bacteria usually contain a huge number of mutations and ``noise'' which renders precise methods impractical. 
Probabilistic grammars and covariance models (CMs) are a way to take the noise into account~\cite{EddyDurbin}.
For example, CMs are successfully used in the Infernal tool.%~\cite{Infernal}.
Neural networks is another way to deal with ``noisy'' data. 
The works~\cite{Humidor,ANN} utilize neural networks for 16s rRNA processing and demonstrate promising results. 
%
%The idea that secondary structure of some genomic sequences contains sufficient information which can be used for its 
%detection and classification and can be specified in terms of formal grammars is widely used in different tools~\cite{GrammarsRNA, PCFG, meta, LWPCFG}.
%Real sequences contain huge number of mutations and ``noise'', so precise methods for secondary structure handling are irrelevant.
%As a result, probabilistic methods such as probabilistic grammars and covariance models (CMs) are used in this area~\cite{EddyDurbin}.
%For example, CMs are successfully used in the Infernal tool.%~\cite{Infernal}.

\section{\uppercase{Proposed solution}}
\label{sec:proposedSolution}

\noindent We combine neural networks and ordinary context-free grammars to detect genomic sequences. 
We extract features by using the ordinary (not probabilistic) context-free grammar and use the dense neural network for features processing.
Features can be extracted by any parsing algorithm and then presented as a boolean matrix $M$ such that $M[i,j]=1$ iff $S \Rightarrow^*_G w[i,j]$ where $w$ is the input sequence and $G$ is context-free grammar with the start nonterminal $S$.

\subsection{Grammar}

\noindent An example of grammar which we use in our experiments is presented in figure~\ref{fig:grammar}.

\begin{figure}
\begin{verbatim}

s1: stem<s0> any

a_0_7 : any*[2..10]

s0: a_0_7 | a_0_7 stem<s0> s0

any: A | U | C | G

stem1<s>: A s U | G s C | U s A | C s G 

stem2<s>: stem1< stem1<s> >

stem<s>:  
      A stem<s> U 
    | U stem<s> A 
    | C stem<s> G 
    | G stem<s> C 
    | stem1< stem2<s> >  
 } 
\end{verbatim}
\caption{Grammar for secondary structure features extraction}
\label{fig:grammar}
\end{figure}

\subsection{Matrices}

\noindent We use line-by-line compressed matrix representation: sequence of 32 cells (bits) is compressed to unsigned integer. 
Top right triangle of matrix is always empty, so can be ignored. 
We hope that compression to 16 bit integer or byte may decrease complexity of neural network and improve result quality, but it requires significantly more memory on GPGPU which can be serious technical problem.


{\centering
  \fbox{\includegraphics[width=2.5cm]{figures/mt1.png}}
  \
  \fbox{\includegraphics[width=2.5cm]{figures/mt2.png}}
  \
  \fbox{\includegraphics[width=2.5cm]{figures/mt3.png}}
  }


\subsection{Newral Network}

\noindent We use dense neural network with 14 dense layers.
Almost all of them are wrapped with dropout (up to 75\%) and batch normalization layers for learning stabilization.

\section{\uppercase{Evaluation}}
\label{sec:evaluation}

\noindent We evaluate the proposed approach for 16s rRNA detection.
We specify context-free grammars which detect stems with the hight of more than two pairs and their arbitrary compositions.
For network training we use dataset consisting of two parts: random subsequences of 16s rRNA sequences from the Green Genes database form positive examples, while the negative examples are random subsequences of full genes from the NCBI database.
All sequences have the length of 512 symbols, totally up to 310000 sequences.
After training, current accuracy is 90\% for validation set (up to 81000 sequences), thus we conclude that our approach is applicable.

\section{\uppercase{Future Work}}
\label{sec:FutureWork}

\noindent The presented is a work in progress. 
The ongoing experiment is finding all instances of 16s rRNA in full genomes.
Also we plan to use the proposed approach for the filtration of chimeric sequences and the classification.
Composition of our approach with other methods and tools as well as grammar tuning and detailed performance evaluation may improve the applicability for the real data processing.


\section{\uppercase{Discussion}}
\label{sec:Discussion}

Protenomics

Different letghts

Embedding: string -> matrix

Other types of NNs



\section*{\uppercase{Acknowledgements}}

\noindent The research was supported by the Russian Science Foundation grant 18-11-00100 and a grant from JetBrains Research.


\vfill
\bibliographystyle{apalike}
{\small
\bibliography{example}}


\vfill
\end{document}

