\section{Evaluation}
To show practical applicability of the proposed algorithm, we implement this algorithm using different optimizations and apply these implementations to the navigation query problem for a dataset taken from a paper~\cite{RDF}. We also compare performance of our implementations with existing analogues from papers~\cite{GLL,RDF}. This analogues use more complex algorithms, while our algorithm uses only simple matrix operations.

Since our algorithm works on graphs, then each RDF file from dataset was converted to edge-labeled directed graph as follows. For each triple $(o,p,s)$ from RDF file, we added edges $(o,p,s)$ and $(s,p^{-1},o)$ to the graph. We also constructed synthetic graphs $g_1$, $g_2$ and $g_3$ by simple repeating the existing graphs.

All tests were run on a PC with the following characteristics:
\begin{itemize}
    \item OS: Microsoft Windows 10 Pro
    \item System Type: x64-based PC
    \item CPU: Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz, 3601 Mhz, 4 Core(s), 4 Logical Processor(s)
    \item RAM: 16 GB
    \item GPU: NVIDIA GeForce GTX 1070
    \begin{itemize}
        \item CUDA Cores:       1920 
        \item Core clock:       1556 MHz 
        \item Memory data rate: 8008 MHz
        \item Memory interface: 256-bit 
        \item Memory bandwidth: 256.26 GB/s
        \item Dedicated video memory:   8192 MB GDDR5
    \end{itemize}
\end{itemize}

We denote the implementation of the algorithm from a paper~\cite{GLL} as $GLL$. The algorithm presented in this paper is implemented in F\# programming language~\cite{fsharp} and is available on GitHub\footnote{GitHub repository of YaccConstructor project: \url{https://github.com/YaccConstructor/YaccConstructor}.}. We denote our implementations of the proposed algorithm as follows:
\begin{itemize}
    \item dGPU (dense GPU) --- an implementation with using row-major order for general matrix representation and using GPU to calculate matrix operations. For calculations of the matrix operations on GPU, we use wrapper for CUBLAS library from managedCuda\footnote{GitHub repository of managedCuda library: \url{https://kunzmi.github.io/managedCuda/}.} library.
    \item sCPU (sparse CPU) --- an implementation with using CSR format for sparse matrix representation and using CPU to calculate matrix operations. For sparse matrix representation in CSR format, we use Math.Net Numerics\footnote{Math.Net Numerics WebSite: \url{https://numerics.mathdotnet.com/}.} package.
    \item sGPU (sparse GPU) --- an implementation with using CSR format for sparse matrix representation and using GPU to calculate matrix operations. For calculations of the matrix operations on GPU, where matrices represented in CSR format, we use wrapper for CUSPARSE library from managedCuda library.
\end{itemize}

We evaluate two classical \textit{same-generation query}~\cite{FndDB} which, for example, is applicable in bioinformatics.

\textbf{Query 1} is based on the grammar $G^1_S$ for retrieving concepts on the same layer, where:
\begin{itemize}
    \item A grammar $G^1 = (N^1, \Sigma^1, P^1)$.
    \item A set of non-terminals $N^1 = \{S\}$.
    \item A set of terminals $\Sigma^1 = \{subClassOf, subClassOf^{-1},$ \\ $type, type^{-1}\}$.
    \item A set of production rules $P^1$ is presented in Figure~\ref{ProductionRulesQuery1}.
\end{itemize}

\begin{figure}[h]
   \[
\begin{array}{rl}
   0: & S \rightarrow \text{\textit{subClassOf}}^{-1} \ S \ \text{\textit{subClassOf}} \\ 
   1: & S \rightarrow \text{\textit{type}}^{-1} \ S \ \text{\textit{type}} \\ 
   2: & S \rightarrow \text{\textit{subClassOf}}^{-1} \ \text{\textit{subClassOf}} \\ 
   3: & S \rightarrow \text{\textit{type}}^{-1} \ \text{\textit{type}} \\ 
\end{array}
\]
\caption{Production rules for the query 1 grammar.}
\label{ProductionRulesQuery1}
\end{figure}

\begin{table*}[ht]
\centering
\caption{Evaluation results for Query 1}
\label{tbl1}

\begin{tabular}{ | c | c | c | c | c | c | c |}
\hline
Ontology & \#triples & \#results & GLL(ms) & dGPU(ms) & sCPU(ms) & sGPU(ms) \\
\hline 
\hline
skos        & 252 & 810 & 10 & 37 & 14 & 64\\
generations & 273 & 2164 & 19 & 43 & 18 & 100\\
travel      & 277 & 2499 & 24 & 46 & 20 & 113\\
univ-bench  & 293 & 2540 & 25 & 53 & 23 & 127\\
atom-primitive & 425 & 15454 & 255 & 127 & 82 & 247\\
biomedical-measure-primitive & 459 & 15156 & 261 & 179 & 96 & 215\\
foaf        & 631 & 4118 & 39 & 99 & 44 & 167\\
people-pets & 640 & 9472 & 89 & 262 & 127 & 182\\
funding     & 1086 & 17634 & 212 & 928 & 414 & 351\\
wine        & 1839 & 66572 & 819 & 1342 & 758 & 621\\
pizza       & 1980 & 56195 & 697 & 719 & 369 & 573\\
$g_{1}$     & 8688 & 141072 & 1926 & --- & 25777 & 3243\\
$g_{2}$     & 14712 & 532576 & 6246 & --- & 53913 & 5501\\
$g_{3}$     & 15840 & 449560 & 7014 & --- & 22001 & 5191\\
\hline
\end{tabular}

\end{table*}

\begin{table*}[ht]
\centering
\caption{Evaluation results for Query 2}
\label{tbl2}

\begin{tabular}{ | c | c | c | c | c | c | c |}
\hline
Ontology & \#triples & \#results & GLL(ms) & dGPU(ms) & sCPU(ms) & sGPU(ms) \\
\hline 
\hline
skos        & 252 & 1 & 1 & 15 & 0 & 15\\
generations & 273 & 0 & 1 & 0 & 0 & 0\\
travel      & 277 & 63 & 1 & 17 & 15 & 31\\
univ-bench  & 293 & 81 & 11 & 31 & 15 & 46\\
atom-primitive & 425 & 122 & 66 & 31 & 15 & 156\\
biomedical-measure-primitive & 459 & 2871 & 45 & 176 & 74 & 171\\
foaf        & 631 & 10 & 2 & 31 & 15 & 15\\
people-pets & 640 & 37 & 3 & 93 & 31 & 46\\
funding     & 1086 & 1158 & 23 & 764 & 312 & 125\\
wine        & 1839 & 133 & 8 & 421 & 276 & 178\\
pizza       & 1980 & 1262 & 29 & 593 & 357 & 218\\
$g_{1}$     & 8688 & 9264 & 167 & --- & 40005 & 1040\\
$g_{2}$     & 14712 & 1064 & 46 & --- & 20863 & 1493\\
$g_{3}$     & 15840 & 10096 & 393 & --- & 30146 & 3091\\
\hline
\end{tabular}

\end{table*}


A grammar $G^1$ is transformed into an equivalent grammar in normal form, which is necessary for our algorithm. This transformation is the same as in Section~\ref{section_example}. Let $R_S$ be context-free relation for a start non-terminal in the transformed grammar.

The result of query 1 evaluation is presented in Table~\ref{tbl1}, where \#triples is a number of triples $(o,p,s)$ in RDF file, and \#results is a number of pairs $(n,m)$ in the context-free relation $R_S$. We can determine whether $(i,j) \in R_S$ by asking whether $S \in b^+_{i,j}$, where $b^+$ is the transitive closure calculated by the proposed algorithm. Since a dense matrix representation significantly degrades performance with increasing of the graph size, then we omit $dGPU$ performance on graphs $g_1$, $g_2$ and $g_3$. All implementations in Table~\ref{tbl1} have the same \#results and demonstrate up to 1000 times better performance as compared to the algorithm presented in~\cite{RDF} for $Q_1$. Our implementations demonstrate better performance than $GLL$ with increasing of the graph size. We also can conclude that acceleration from the $GPU$ increases with the size of the graph.

\textbf{Query 2} is based on the grammar $G^2_S$ for retrieving concepts on the adjacent layers, where:
\begin{itemize}
    \item A grammar $G^2 = (N^2, \Sigma^2, P^2)$.
    \item A set of non-terminals $N^2 = \{S, B\}$.
    \item A set of terminals $\Sigma^2 = \{subClassOf, subClassOf^{-1}\}$.
    \item A set of production rules $P^2$ is presented in Figure~\ref{ProductionRulesQuery2}.
\end{itemize}

\begin{figure}[h]
   \[
\begin{array}{rl}
   0: & S \rightarrow B \ \text{\textit{subClassOf}} \\ 
   1: & S \rightarrow \text{\textit{subClassOf}} \\ 
   2: & B \rightarrow \text{\textit{subClassOf}}^{-1} \ B \ \text{\textit{subClassOf}} \\ 
   3: & B \rightarrow \text{\textit{subClassOf}}^{-1} \ \text{\textit{subClassOf}} \\ 
\end{array}
\]
\caption{Production rules for the query 2 grammar.}
\label{ProductionRulesQuery2}
\end{figure}

A grammar $G^2$ is transformed into an equivalent grammar in normal form. Let $R_S$ be context-free relation for a start non-terminal in the transformed grammar.

The result of the query 2 evaluation is presented in Table~\ref{tbl2}. Since a dense matrix representation significantly degrades performance with increasing of the graph size, then we omit $dGPU$ performance on graphs $g_1$, $g_2$ and $g_3$. All implementations in Table~\ref{tbl2} have the same \#results. On this query $GLL$ demonstrates better performance than our implementations but we also can conclude that acceleration from the $GPU$ increases with the size of the graph.

As a result, we conclude that our algorithm can be applied to some real-world problems and it allows to speed up computations by means of GPGPU.

