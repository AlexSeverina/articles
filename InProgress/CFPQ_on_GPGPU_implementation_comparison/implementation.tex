\section{Implementation}

We implement the matrix-based algorithm for CFPQ by using a number of different programming languages and tools.
Our goal is to investigate the effects of the next features of implementation.
\begin{itemize}
\item \textbf{GPGPU utilization.}
It is well-known that GPGPUs are suitable for matrices operations, but the performance of the whole solution depends on task details: overhead on data transferring may negate the effect of parallel computations.
Can GPGPUs utilization for CFPQ improve performance in comparison with CPU version?

\item \textbf{Existing libraries utilization} is a good practice in software engineering.
Is it possible to achieve higher performance by using existing libraries for matrices operations or we need to create own solution to get more control?

\item \textbf{Low-level programming}.
GPGPU programming is traditionally low-level programming by using C-based languages (CUDA C, OpenCL C).
On the other hand, there is a number of approaches to creating GPGPU-based solution by using such high-level languages as a Python.
Can we get a high-performance solution by using such approaches?

\item \textbf{Sparce matrices.} Real graphs often are sparse, but not always.
Is it suitable to use sparse matrix representation for CFPQ?

\end{itemize}

We provide the next implementations for investigation.

\begin{itemize}
  \item CPU-based solutions
%  \begin{itemize}

    \textbf{[Scipy]} Sparse matrices multiplication by using Scipy~\cite{scipy} in Python programming language.

    \textbf{[M4RI]} Dense matrices multiplication by using m4ri\footnote{Actually we use pull request which is not merged yet: \url{https://bitbucket.org/malb/m4ri/pull-requests/9/extended-m4ri-to-multiplication-over-the/diff}. The original library implements operations over $GF(2)$, and this pull request contains operations over boolean semiring}~\cite{M4RI} library which implements 4 Russian method~\cite{arlazarov1970economical} in C language.
    This library choosen because it is one of performnat implementation of 4 russian method~\cite{albrechtefficient}.
  %\end{itemize}
  \item GPGPU-based solutions

    \textbf{[GPU4R]} Manual implementation of 4 Russian method in CUDA C.

    \textbf{[GPU\_N]} Manual implementation of na\"ive boolean matrix multiplication in CUDA C.

    \textbf{[GPU\_Py]} Manual implementation of na\"ive boolean matrix multiplication in Python by using number compiler\footnote{Numba is a JIT compiler which supports GPGPU for a subset of Python programming. Official page: \url{http://numba.pydata.org/}. Access date: 03.05.2019}.

\end{itemize}

As far as a number of matrices and its size can be statically defined at the start, all GPGPU based implementations allocate all required memory on the GPGPU only once, at the start of computations.
By this way, it is possible to significantly reduce overhead on data transferring: all input data loads to GPGPU at the start, and result loads from GPGPU to the host at the finish.
No active data transferring and memory allocating during query computation.
