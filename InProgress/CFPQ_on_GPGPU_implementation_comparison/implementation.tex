\section{Implementation}

We implement the matrix-based algorithm for CFPQ by using a number of different programming languages and tools.
Our goal is to investigate the effects of the following features of implementation.
\begin{itemize}
\item \textbf{GPGPU utilization.}
It is well-known that GPGPUs are suitable for matrices operations, but the performance of the whole solution depends on numder of details. For example, overhead on data transferring may negate the effect of parallel computations.
Can GPGPUs utilization for CFPQ improve performance in comparison with CPU version?

\item \textbf{Existing libraries utilization} is a good practice in software engineering.
Is it possible to achieve higher performance by using existing libraries for matrices operations or we need to create own solution to get more control?

\item \textbf{Low-level programming}.
GPGPU programming traditionally involves low-level programming in C-based languages (CUDA C, OpenCL C).
But can we achieve high-performant solution with level languages such as a Python?

\item \textbf{Sparce matrices.} Real graphs are often sparse.
Can we gain something by using sparse matrix representation for CFPQ?

\end{itemize}

We provide the following implementations for investigation.

\begin{itemize}
  \item CPU-based solutions
%  \begin{itemize}

    \textbf{[Scipy]} Sparse matrices multiplication by using Scipy~\cite{scipy} in Python programming language.

    \textbf{[M4RI]} Dense matrices multiplication by using m4ri\footnote{Actually we use pull request which is not merged yet: \url{https://bitbucket.org/malb/m4ri/pull-requests/9/extended-m4ri-to-multiplication-over-the/diff}. The original library implements operations over $GF(2)$, and this pull request contains operations over boolean semiring}~\cite{M4RI} library which implements 4 Russian method~\cite{arlazarov1970economical} in C language.
    This library is choosen because it is one of performnat implementation of 4 russian method~\cite{albrechtefficient}.
  %\end{itemize}
  \item GPGPU-based solutions

    \textbf{[GPU4R]} Our own implementation of 4 Russian method in CUDA C.

    \textbf{[GPU\_N]} Our own implementation of the na\"ive boolean matrix multiplication in CUDA C with boolean values treated as bits and packed into \texttt{uint\_32}.

    \textbf{[GPU\_Py]} Manual implementation of na\"ive boolean matrix multiplication in Python by using numba compiler\footnote{Numba is a JIT compiler which supports GPGPU for a subset of Python programming. Official page: \url{http://numba.pydata.org/}. Access date: 03.05.2019}.
    Boolean values packed into \texttt{uint\_32}.

\end{itemize}

As far as a set of matrices and its size can be statically at the start of computations, all GPGPU based implementations allocate all required memory on the GPGPU once, at the start of computations.
This way, it is possible to significantly reduce overhead on data transferring: all input data loads to GPGPU at the start, and the result loads from GPGPU to the host at the end.
As a result, there is no active data transferring and memory allocating during query computation.
