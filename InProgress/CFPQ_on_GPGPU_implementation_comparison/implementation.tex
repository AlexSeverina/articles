\section{Implementation}

We implement matrix-based algorithm for CFPQ by using a number of different programming languages and tools.
Our goal is to investigate effects of the next features of implmentation.
\begin{itemize}
\item \textbf{GPGPU utilization.} 
It is mell-known that GPGPUs are sutable for matrices operations, but performance of whole solution depends on task details: overhead on data transferring may negate effect of parallel computations. 
Can GPGPUs utilization for CFPQ improve performance in comparison with CPU version?

\item \textbf{Existing libraries utilization} is a good practice in software engeneering.
Is it possible to achaive highe performance by using existing libraries for matrices operations or we need to create own solution to get more control?

\item \textbf{Low-level programming}. 
GPGPU programming is traditionally low-level programming by using C-based languages (CUDA C, OpenCL C). 
On the other hand, there are number of approaches to create GPGPU-based solution by ysing such high-level languages as a Python. 
Can we get high-performance solution by using such approaches?

\item \textbf{Sparce matrices.} Real graphs often are sparse, but not always.
Is it sutable to use sparse matrix representation for CFPQ? 

\end{itemize}

We provide next implementations for investigation.

\begin{itemize}
  \item CPU-based solutions 
  \begin{itemize}
    \item[\textbf{[Scipy]}] Saprse matrices multiplication by using Scipy~\cite{scipy} in Python programming language.
    \item[\textbf{[M4RI]}] Dense matrices multiplication by using m4ri\footnote{Actually we use pull request which is not merged yet: \url{https://bitbucket.org/malb/m4ri/pull-requests/9/extended-m4ri-to-multiplication-over-the/diff}. The original library implements operations over $GF(2)$, and this pull request conteains operations over boolean semiring}~\cite{M4RI} library which implements 4 russian method~\cite{arlazarov1970economical} in C language.
    This library choosen because it is one of performnat implementation of 4 russian method~\cite{albrechtefficient}.
  \end{itemize}
  \item GPGPU-based solutions
  \begin{itemize}
    \item[\textbf{[GPU4R]}] Manual implemenattion of 4 russian metod in CUDA C.
    \item[\textbf{[GPU\_N]}] Manual implementation of na\"ive boolean matrix multiplication in CUDA C.
    \item[\textbf{[GPU\_Py]}] Manual implementation of na\"ive boolean matrix multiplication in Pyton by using numba compiler\footnote{Numba is a JIT compiler which supports GPGPU for subset of Python programming. Offical page: \url{http://numba.pydata.org/}. Access date: 03.05.2019}. 
  \end{itemize}
\end{itemize}

As far as number of matrices and its size can be statically defined at he start, all GPGPU based implementattions allocate all requred memory on the GPGPU only onece, at the start of computattions.
By this way it is possible to significante reduce overhead on data transferring: all input data loads to GPGPU at the start, and result loads from GPGPU to the host at the finish.
No active data transferring and memory allocating duting query computation. 
