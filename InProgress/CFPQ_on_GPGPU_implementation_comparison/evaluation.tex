\section{Evaluation}

We evaluate all the described implementations on all data sets and queries presented.
We compare our implementations with~\cite{Azimov:2018:CPQ:3210259.3210264}.
We exclude the time required to load data from files.
The time required for data transfer is included.

For evaluation, we use a PC with Ubuntu 18.04 installed.
It has Intel core i7 8700k 3,7HGz CPU, DDR4 32 Gb RAM, and Geforce 1080Ti GPGPU with 11Gb RAM.

Results of evaluation are summarized in the tables below.
Time is measured in seconds.
The result for each algorithm is averaged over 10 runs.
Time is not presented if the time limit is exceeded, or if ther is not enough memory to allocate the data.

{\setlength{\tabcolsep}{0.4em}
\begin{table*}
\caption{RDFs querying results (time in milliseconds)}
\label{tbl:tableRDF}
\rowcolors{1}{}{lightgray}
\begin{tabular}{| p{1.25cm} | c | c |c | c | c | c | c | c | c | c | c | c | c | c |}
    \hline
    \multicolumn{3}{|c|}{RDF}        & \multicolumn{6}{|c|}{Query $G_4$}                               & \multicolumn{6}{|c|}{Query $G_5$} \\
    %\cline{2-13}
    \hline
    Name                                & \#V & \#E  & Scipy & M4RI  & GPU4R & GPU\_N & GPU\_Py & CuSprs & Scipy & M4RI & GPU4R & GPU\_N & GPU\_Py & CuSprs \\
    \hline
    \hline
    \small{atm-prim}                    & 291 & 685  & 3     &  2    & 2     & 1      & 5       & 269    & 1     & \ltz & 1     & \ltz   & 2       & 267  \\
    \small{biomed}                      & 341 & 711  & 3     &  5    & 2     & 1      & 5       & 283    & 4     & \ltz & 1     & \ltz   & 5       & 280  \\
    \small{foaf}                        & 256 & 815  & 2     &  9    & 2     & \ltz   & 5       & 270    & 1     & \ltz & 1     & \ltz   & 2       & 263  \\
    \small{funding}                     & 778 & 1480 & 4     &  7    & 4     & 1      & 5       & 279    & 2     & \ltz & 3     & \ltz   & 4       & 274  \\
    \small{generations}                 & 129 & 351  & 3     &  3    & 2     & \ltz   & 5       & 273    & 1     & \ltz & 1     & \ltz   & 2       & 263  \\
    \small{people\_pets}                & 337 & 834  & 3     &  3    & 3     & 1      & 7       & 284    & 1     & \ltz & 1     & \ltz   & 3       & 277  \\
    \small{pizza}                       & 671 & 2604 & 6     &  8    & 3     & 1      & 6       & 292    & 2     & \ltz & 2     & \ltz   & 5       & 278  \\
    \small{skos}                        & 144 & 323  & 2     &  4    & 2     & \ltz   & 5       & 273    & \ltz  & \ltz & 1     & \ltz   & 2       & 265  \\
    \small{travel}                      & 131 & 397  & 3     &  5    & 2     & \ltz   & 6       & 268    & 1     & \ltz & 1     & \ltz   & 3       & 271  \\
    \small{unv-bnch}                    & 179 & 413  & 2     &  4    & 2     & \ltz   & 5       & 266    & 1     & \ltz & 1     & \ltz   & 3       & 266  \\
    \small{wine}                        & 733 & 2450 & 7     &  6    & 4     & 1      & 7       & 294    & 1     & \ltz & 3     & \ltz   & 3       & 281  \\
    \hline
  \end{tabular}
\end{table*}
}


The results of the first dataset \textbf{[RDF]} are presented in table~\ref{tbl:tableRDF}.
We can see, that in this case running time for all our implementations is smaller than time for the reference implementation, and that \textbf{[GPU\_N]} is faster than other implementations while other implementations demonstrate similar performance.
It is obvious that performance improvement in comparison with the first implementations is huge and it is necessary to extend dataset with new RDFs of the significantly bigger size.


{\setlength{\tabcolsep}{0.4em}
\begin{table}[H]
\caption{Worst case evaluation results}
\label{tbl:tableWorst}
\rowcolors{1}{}{lightgray}
\begin{tabular}{| l | c | c | c | c | c | c | }
    \hline
    \#V  & Scipy    & M4RI    & GPU4R  & GPU\_N  & GPU\_Py & CuSprs   \\
    \hline
    \hline
    16   & 0.032    & \ltz    & 0.008  & 0.002   & 0.027   & 0.309    \\
    32   & 0.118    & 0.001   & 0.034  & 0.008   & 0.136   & 0.441    \\
    64   & 0.476    & 0.041   & 0.133  & 0.032   & 0.524   & 0.988    \\
    128  & 2.194    & 0.226   & 0.562  & 0.129   & 2.751   & 3.470    \\
    256  & 15.299   & 1.994   & 3.088  & 0.544   & 11.883  & 15.317   \\
    512  & 121.287  & 23.204  & 13.685 & 2.499   & 43.563  & 102.269  \\
    1024 & 1593.284 & 528.521 & 88.064 & 19.357  & 217.326 & 1122.055 \\
    2048 & -        & -       & -      & 325.174 & -       & -        \\
    \hline
  \end{tabular}
\end{table}
}

Results of the theoretical worst case (\textbf{[Worst]} datatset) are presented in table~\ref{tbl:tableWorst}.
This case is really hard to process: even for a graph of 1024 vertices, query evaluation time is greater than 10 seconds even for the most performant implementation.
We can see, that time grows too fast with the number of vertices.


{\setlength{\tabcolsep}{0.25em}
\begin{table}[H]
\caption{Sparse graphs querying results}
\label{tbl:tableSparse}
\rowcolors{1}{}{lightgray}
\begin{tabular}{| l | c | c | c | c | c | c | }
    \hline
    Graph              & Scipy   & M4RI     & GPU4R  & GPU\_N & GPU\_Py & CuSprs  \\
    \hline
    \hline
    \small{G5k-0.001}  & 10.352  & 0.647    & 0.113  & 0.041  & 0.216   & 5.729   \\
    \small{G10k-0.001} & 37.286  & 2.395    & 0.435  & 0.215  & 1.331   & 35.937  \\
    \small{G10k-0.01}  & 97.607  & 1.455    & 0.273  & 0.138  & 0.763   & 47.525  \\
    \small{G10k-0.1}   & 601.182 & 1.050    & 0.223  & 0.114  & 0.859   & 395.393 \\
    \small{G20k-0.001} & 150.774 & 11.025   & 1.842  & 1.274  & 6.180   & -       \\
    \small{G40k-0.001} & -       & 97.841   & 11.663 & 8.393  & 37.821  & -       \\
    \small{G80k-0.001} & -       & 1142.959 & 88.366 & 65.886 & -       & -       \\
    \hline
  \end{tabular}
\end{table}
}

The next is a \textbf{[Sparse]} datatset presented in table~\ref{tbl:tableSparse}.
The evaluation shows that sparsity of graphs (value of parameter \texttt{p}) is important both for implementations which use sparse matrices and for implementations which use dense matrices.
Note that the behavior of sparse matrices based implementation is as expected, but for dense matrices we can see, that more sparse graphs are processed faster.
Reasons for such behaviour demand further investigation.
Note that we estimate only query execution time, so it is hard to compare our results with the results presented in~\cite{fan2018scaling}, but running time of our \textbf{[GPU\_N]} implementation is significantly smaller than the one provided in~\cite{fan2018scaling}.

\begin{table*}
\caption{Full querying results}
\label{tbl:tableFull}
\rowcolors{1}{}{lightgray}
\begin{tabular}{| l | c | c | c | c | c | c | c | c | c | c | c | c |}
    \hline
    \multirow{2}{*}{\#V} & \multicolumn{6}{|c|}{Query $G_2$}                               & \multicolumn{6}{|c|}{Query $G_3$} \\
    \cline{2-13}
                         & Scipy   & M4RI    & GPU4R   & GPU\_N  & GPU\_Py & CuSprs    & Scipy    & M4RI     & GPU4R   & GPU\_N  & GPU\_Py & CuSprs \\
    \hline
    \hline
%    10                   & 0.001   & \ltz    & \ltz    & \ltz    & 0.002   & !!!      & 0.002    & !!!     & 0.001   & 0.001   & 0.004   & !!!     \\
%    50                   & 0.002   & \ltz    & 0.001   & \ltz    & 0.003   & !!!      & 0.005    & !!!     & 0.002   & 0.001   & !!!     & !!!     \\
    100                  & 0.007   & 0.002    & 0.002   & \ltz    & 0.003   & 0.278    & 0.023    & 0.076   & 0.005   & 0.001   & 0.007   & 0.290   \\
    200                  & 0.040   & 0.003    & 0.002   & 0.001   & 0.004   & 0.279    & 0.105    & 0.098   & 0.004   & 0.001   & 0.007   & 0.296   \\
    500                  & 0.480   & 0.003    & 0.003   & 0.001   & 0.004   & 0.329    & 1.636    & 0.094   & 0.007   & 0.001   & 0.010   & 0.382   \\
    1000                 & 3.741   & 0.007    & 0.005   & 0.001   & 0.006   & 0.571    & 13.071   & 0.106   & 0.009   & 0.001   & 0.009   & 0.839   \\
    2000                 & 40.309  & 0.063    & 0.019   & 0.003   & 0.017   & 1.949    & 93.676   & 0.108   & 0.030   & 0.005   & 0.026   & 3.740   \\
    5000                 & 651.343 & 0.366    & 0.125   & 0.038   & 0.150   & 99.651   & 1205.421 & 0.851   & 0.195   & 0.075   & 0.239   & 201.151 \\
    10000                & -       & 1.932    & 0.552   & 0.315   & 0.840   & 1029.042 & -        & 4.690   & 1.055   & 0.648   & 1.838   & -       \\
    25000                & -       & 33.236   & 7.252   & 5.314   & 15.521  & -        & -        & 70.823  & 15.240  & 10.961  & 36.495  & -       \\
    50000                & -       & 360.035  & 58.751  & 44.611  & 129.641 & -        & -        & 775.765 & 130.203 & 91.579  & 226.834 & -       \\
    80000                & -       & 1292.817 & 256.579 & 190.343 & 641.260 & -        & -        & -       & 531.694 & 376.691 & -       & -       \\

    \hline
  \end{tabular}
\end{table*}

The last dataset is a \textbf{[Full]}, and results are shown in table~\ref{tbl:tableFull}.
As we expect, this case is very hard for sparse matrices based implementations: running time grows too fast.
This dataset also demonstrates the impact of the grammar size.
Both queries specify the same constraints, but the grammar $G_3$ in CNF contains 2 times more rules then the grammar $G_2$, so, running time for big graphs differs by more than twice.

Finally, we can conclude that GPGPU utilization for CFPQ can significantly improve performance, but more research on advanced optimization techniques should be done.
On the other hand, the high-level implementation (\textbf{[GPU\_Py]}) is comparable with other GPGPU-based implementations.
So, it may be a balance between implementation complexity and performance.
Highly optimized existing libraries can be of some use: the implementation based on m4ri is faster than the reference implementation and the other CPU-based implementation.
Moreover, it is comparable with some GPGPU-based implementations in some cases.
Sparse matrices utilization demands more thorough investigation.
The main question is if we can create an efficient implementation for sparse boolean matrices multiplication.
