\section{Evaluation}

We evaluate all described implementations on all data sets and queries presented.
We compare our impementations with~\cite{Azimov:2018:CPQ:3210259.3210264}.
We exclude time required to load data from files.
The time required for data transfer is included.

For evaluation, we use a PC with Ubuntu 18.04 installed.
It has Intel core i7 8700k 3,7HGz CPU, Ddr4 32 Gb RAM, and Geforce 1080Ti GPGPU with 11Gb RAM.

Results of evaluation are summarized in the tables below.
Time is measured in seconds.
Result for each algorithm is an average time of 10 runs.
Time is not presented if the time limit is exceeded, or if no memory enough to allocate the data necessary.

{\setlength{\tabcolsep}{0.4em}
\begin{table*}
\caption{RDFs querying results}
\label{tbl:tableRDF}
\begin{tabular}{| p{1.25cm} | c | c |c | c | c | c | c | c | c | c | c | c | c | c |}
    \hline
    \multicolumn{3}{|c|}{RDF}        & \multicolumn{6}{|c|}{Query $G_4$}                               & \multicolumn{6}{|c|}{Query $G_5$} \\
    %\cline{2-13}
    \hline
    Name                                & \#V & \#E  & Scipy & M4RI  & GPU4R & GPU\_N & GPU\_Py & CuSprs & Scipy & M4RI & GPU4R & GPU\_N & GPU\_Py & CuSprs \\
    \hline
    \hline
    \small{atom-primitive}              & 291 & 685  & 0.003 & 0.002 & 0.002 & 0.001  & 0.005   & 0.269  & 0.001 & \ltz & 0.001 & \ltz   & 0.002   & 0.267  \\
    \small{biomed.-mesure-primitive}    & 341 & 711  & 0.003 & 0.005 & 0.002 & 0.001  & 0.005   & 0.283  & 0.004 & \ltz & 0.001 & \ltz   & 0.005   & 0.280  \\
    \small{foaf}                        & 256 & 815  & 0.002 & 0.009 & 0.002 & \ltz   & 0.005   & 0.270  & 0.001 & \ltz & 0.001 & \ltz   & 0.002   & 0.263  \\
    \small{funding}                     & 778 & 1480 & 0.004 & 0.007 & 0.004 & 0.001  & 0.005   & 0.279  & 0.002 & \ltz & 0.003 & \ltz   & 0.004   & 0.274  \\
    \small{generations}                 & 129 & 351  & 0.003 & 0.003 & 0.002 & \ltz   & 0.005   & 0.273  & 0.001 & \ltz & 0.001 & \ltz   & 0.002   & 0.263  \\
    \small{people\_pets}                & 337 & 834  & 0.003 & 0.003 & 0.003 & 0.001  & 0.007   & 0.284  & 0.001 & \ltz & 0.001 & \ltz   & 0.003   & 0.277  \\
    \small{pizza}                       & 671 & 2604 & 0.006 & 0.008 & 0.003 & 0.001  & 0.006   & 0.292  & 0.002 & \ltz & 0.002 & \ltz   & 0.005   & 0.278  \\
    \small{skos}                        & 144 & 323  & 0.002 & 0.004 & 0.002 & \ltz   & 0.005   & 0.273  & \ltz  & \ltz & 0.001 & \ltz   & 0.002   & 0.265  \\
    \small{travel}                      & 131 & 397  & 0.003 & 0.005 & 0.002 & \ltz   & 0.006   & 0.268  & 0.001 & \ltz & 0.001 & \ltz   & 0.003   & 0.271  \\
    \small{univ-bench}                  & 179 & 413  & 0.002 & 0.004 & 0.002 & \ltz   & 0.005   & 0.266  & 0.001 & \ltz & 0.001 & \ltz   & 0.003   & 0.266  \\
    \small{wine}                        & 733 & 2450 & 0.007 & 0.006 & 0.004 & 0.001  & 0.007   & 0.294  & 0.001 & \ltz & 0.003 & \ltz   & 0.003   & 0.281  \\
    \hline
  \end{tabular}
\end{table*}
}


The results of the first dataset \textbf{[RDF]} are presented in a table~\ref{tbl:tableRDF}.
We can see, that in this case running time for all our implementations smaller than time for the reference implementation, and that \textbf{[GPU\_N]} is faster than other implementations while other implementations demonstrate similar performance.
Also, it is obvious that performance improvement in comparison with the first implementations is huge and it is necessary to extend dataset with new RDFs of the significantly biggest size.


{\setlength{\tabcolsep}{0.4em}
\begin{table}[H]
\caption{Worst case evaluation results}
\label{tbl:tableWorst}
\begin{tabular}{| l | c | c | c | c | c | c | }
    \hline
    \#V  & Scipy    & M4RI    & GPU4R  & GPU\_N  & GPU\_Py & CuSprs   \\
    \hline
    \hline
    16   & 0.032    & \ltz    & 0.008  & 0.002   & 0.027   & 0.309    \\
    32   & 0.118    & 0.001   & 0.034  & 0.008   & 0.136   & 0.441    \\
    64   & 0.476    & 0.041   & 0.133  & 0.032   & 0.524   & 0.988    \\
    128  & 2.194    & 0.226   & 0.562  & 0.129   & 2.751   & 3.470    \\
    256  & 15.299   & 1.994   & 3.088  & 0.544   & 11.883  & 15.317   \\
    512  & 121.287  & 23.204  & 13.685 & 2.499   & 43.563  & 102.269  \\
    1024 & 1593.284 & 528.521 & 88.064 & 19.357  & 217.326 & 1122.055 \\
    2048 & -        & -       & -      & 325.174 & -       & -        \\
    \hline
  \end{tabular}
\end{table}
}

Results of the theoretical worst case (\textbf{[Worst]} datatset) is presented in table~\ref{tbl:tableWorst}.
This case is really hard to process: even for a graph of 1024 vertices, query evaluation time is greater than 10 seconds even for most performant implementation.
Also, we can see, that time grows too fast with a number of vertices.


{\setlength{\tabcolsep}{0.25em}
\begin{table}[H]
\caption{Sparse graphs querying results}
\label{tbl:tableSparse}
\begin{tabular}{| l | c | c | c | c | c | c | }
    \hline
    Graph              & Scipy   & M4RI     & GPU4R  & GPU\_N & GPU\_Py & CuSprs  \\
    \hline
    \hline
    \small{G5k-0.001}  & 10.352  & 0.647    & 0.113  & 0.041  & 0.216   & 5.729   \\
    \small{G10k-0.001} & 37.286  & 2.395    & 0.435  & 0.215  & 1.331   & 35.937  \\
    \small{G10k-0.01}  & 97.607  & 1.455    & 0.273  & 0.138  & 0.763   & 47.525  \\
    \small{G10k-0.1}   & 601.182 & 1.050    & 0.223  & 0.114  & 0.859   & 395.393 \\
    \small{G20k-0.001} & 150.774 & 11.025   & 1.842  & 1.274  & 6.180   & -       \\
    \small{G40k-0.001} & -       & 97.841   & 11.663 & 8.393  & 37.821  & -       \\
    \small{G80k-0.001} & -       & 1142.959 & 88.366 & 65.886 & -       & -       \\
    \hline
  \end{tabular}
\end{table}
}

The next is a \textbf{[Sparse]} datatset presented in table~\ref{tbl:tableSparse}.
The evaluation shows that sparsity of graphs (value of parameter \texttt{p}) is important both for implementations which use sparse matrices and for implementations which use dense matrices.
Note that the behavior of sparse matrices based implementation is as expected, but for dense matrices we can see, that more sparse graphs processed faster.
Reasons of such behaviour demand further investigation.
Note that we estimate only query execution time, so it is hard to compare our results with the results presented in~\cite{fan2018scaling}.
But it would be interesting to do such a comparison in the future because the running time of our \textbf{[GPU\_N]} implementation is significantly smaller than the provided in~\cite{fan2018scaling}.

The last dataset is a \textbf{[Full]}, and results are shown in table~\ref{tbl:tableFull}


\begin{table*}
\caption{Full querying results}
\label{tbl:tableFull}
\begin{tabular}{| l | c | c | c | c | c | c | c | c | c | c | c | c |}
    \hline
    \multirow{2}{*}{\#V} & \multicolumn{6}{|c|}{Query $G_2$}                               & \multicolumn{6}{|c|}{Query $G_3$} \\
    \cline{2-13}
                         & Scipy   & M4RI    & GPU4R   & GPU\_N  & GPU\_Py & CuSprs    & Scipy    & M4RI     & GPU4R   & GPU\_N  & GPU\_Py & CuSprs \\
    \hline
    \hline
%    10                   & 0.001   & \ltz    & \ltz    & \ltz    & 0.002   & !!!      & 0.002    & !!!     & 0.001   & 0.001   & 0.004   & !!!     \\
%    50                   & 0.002   & \ltz    & 0.001   & \ltz    & 0.003   & !!!      & 0.005    & !!!     & 0.002   & 0.001   & !!!     & !!!     \\
    100                  & 0.007   & 0.002    & 0.002   & \ltz    & 0.003   & 0.278    & 0.023    & 0.076   & 0.005   & 0.001   & 0.007   & 0.290   \\
    200                  & 0.040   & 0.003    & 0.002   & 0.001   & 0.004   & 0.279    & 0.105    & 0.098   & 0.004   & 0.001   & 0.007   & 0.296   \\
    500                  & 0.480   & 0.003    & 0.003   & 0.001   & 0.004   & 0.329    & 1.636    & 0.094   & 0.007   & 0.001   & 0.010   & 0.382   \\
    1000                 & 3.741   & 0.007    & 0.005   & 0.001   & 0.006   & 0.571    & 13.071   & 0.106   & 0.009   & 0.001   & 0.009   & 0.839   \\
    2000                 & 40.309  & 0.063    & 0.019   & 0.003   & 0.017   & 1.949    & 93.676   & 0.108   & 0.030   & 0.005   & 0.026   & 3.740   \\
    5000                 & 651.343 & 0.366    & 0.125   & 0.038   & 0.150   & 99.651   & 1205.421 & 0.851   & 0.195   & 0.075   & 0.239   & 201.151 \\
    10000                & -       & 1.932    & 0.552   & 0.315   & 0.840   & 1029.042 & -        & 4.690   & 1.055   & 0.648   & 1.838   & -       \\
    25000                & -       & 33.236   & 7.252   & 5.314   & 15.521  & -        & -        & 70.823  & 15.240  & 10.961  & 36.495  & -       \\
    50000                & -       & 360.035  & 58.751  & 44.611  & 129.641 & -        & -        & 775.765 & 130.203 & 91.579  & 226.834 & -       \\
    80000                & -       & 1292.817 & 256.579 & 190.343 & 641.260 & -        & -        & -       & 531.694 & 376.691 & -       & -       \\

    \hline
  \end{tabular}
\end{table*}


As we expect, this case is very hard for sparse matrices based implementations: running time grows too fast.
Also, we can see, that the grammar size is important.
Both queries specify the same restriction, but grammar $G_3$ in CNF contains 2 times more rules then grammar $G_2$, and as a result, the running time for big graphs differs more than 2 times.

Finally, we can conclude that GPGPU utilization for CFPQ can significantly improve performance, but more research on advanced optimization techniques should be done.
On the other hand, high-level implementation (\textbf{[GPU\_Py]}) is comparable with other GPGPU-based implementations.
So, it may be balance between implementation complexity and performance.
Highly optimized existing libraries can be useful: implementation based on m4ri is faster than reference implementation and other CPU-based implementation.
Moreover, it is comparable with some GPGPU-based implementations in some cases.
Sparse matrices utilization should be investigated more.
The main question is if we can create an efficient implementation for sparse boolean matrices multiplication.
