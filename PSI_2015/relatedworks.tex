\section{Related Works}

Our approach for syntax analysis of string-embedded languages borrows some common principles
from existing techniques in this area. In addition, we reuse RNGLR syntax analysis algorithm 
and some accompanying constructs. In this section we provide a review and recollect some important
notions which will be referred to later on. 

The analysis of string-embedded languages, as a rule, requires a set of \emph{hotspots} to
be indicated in the host application source code. Hotspot is considered as some ``point 
of interest'', where the analysis of the set of possible string values is desirable. This task can be
performed either in a user-assisted manner or automatically using some pragmatic 
considerations or knowledge of the framework being analyzed. The following logical steps 
include static analysis to construct an approximation for the set of all possible string values,
lexical, syntax, and, perhaps, some kind of semantic analysis. These steps not
necessarily performed separatedly; some of them may be omitted.

A rather natural idea of \emph{regular approximation} is to approximate the set of all possible 
strings by a regular expression. In recognition-centric formulation, this approach boils down to
the problem of inclusion of approximating regular language into context-free reference language, which
is decidable for a number of practically significant cases~\cite{LangInclusion}.
Many approaches follow this route. In~\cite{Stranger}, forward reachability analysis is used to compute regular 
approximation for all string values in the program. Further analysis is based on patterns detection in approximation 
set or generation of some finite subset of strings for analysis by standalone tools. Regular approximation in~\cite{JSA} 
is acquired by widening context-free approximation, initially built as a result of program analysis. 
Our approach is partially inspired by Alvor~\cite{Alvor,ALVOR2}, which utilizes GLR-based technique for syntax 
analysis of regular approximation; this framework implements abstract lexical analysis to convert a
regular language over characters into regular language over tokens, which simplifies syntax analysis.

Kyung-Goo Doh et al. in a series of papers~\cite{AbstrParsing,LRAbstrParsing,LRAbstrParsingSema} introduced an
approach, based on implicit representation of the set of potential strings as a system of data-flow equations. 
Conventional LALR(1) is chosen for the basis of parsing algorithm; original control tables are reused. 
Syntax analysis is performed as the system of dataflow equations is being solved iteratively in the space of abstract stacks.
The problem of infinite stack growth, which appears in general case, is handled using abstract 
interpretation~\cite{AbstractInterpretation}. This approach later evolved to a certain kind of semantic processing
in terms of attribute grammars which made it possible to analyze a wider class of languages, than
LALR(1).

RNGLR (Right-Nulled Genralized LR) is a modification of Generalized LR (GLR) algorithm, which
was developed by Masaru Tomita~\cite{Tomita} in the context of natural language processing. 
GLR was designed to handle ambiguous context-free grammars. Ambiguities in the grammar produce 
shift/reduce and reduce/reduce conflicts, speaking in terms of LR approach. The algorithm 
uses parser tables, similar to those for classical LR, each cell of which can contain multiple 
actions. The general approach is to carry out all possible actions during parsing 
using graph-based data structures to efficiently represent the set of stacks 
and derivation trees. Originally, Tomita's algorithm was unable to recognize all context-free languages.  
Elizabeth Scott and Adrian Johnstone presented RNGLR~\cite{RNGLR},
which extends GLR with a certain way of handling \emph{right nullable} 
rules (i.e. rules of the form $\mathrm{A} \rightarrow \alpha \beta$, where $\beta$ 
reduces to an empty string).

To efficiently represent the set of all stacks, produced during parsing,
RNGLR uses Graph Structured Stack (GSS). GSS is a directed graph,
whose vertices correspond to the elements of individual stacks and edges link succesive
stack elements. Each vertex can have multiple incoming and outgoing edges to merge 
multiple stacks together; thus stack element sharing is implemented. Each vertex is 
a pair $(s,l)$, where $s$ is a parser state and $l$ is a \emph{level} (position in the input string). 
Vertices in GSS are unique and there are no multi-edges. 

According to RNGLR, an input is read left-to-right, one token at a time, and 
the levels of GSS are constructed sequentially for each input position: first, all  
possible reductions are applied, then the next input terminal is shifted and
pushed to the GSS. When a reduction or pushing is performed, 
the algorithm modifies GSS in the following manner. Suppose an 
egde $(v_t,v_h)$ has to be added to the GSS. By construction, the head vertex
$v_h$ is always already in the GSS. If the tail vertex is also in the GSS, then
a new egde $(v_t,v_h)$ is added (provided it is not yet there); otherwise both 
new tail vertex and new edge are created and added to the GSS. Every time a new 
vertex $v=(s,l)$ is created, the algorithm calculates the new parser 
state $s'$ from $s$ and the next terminal of the input. The pair $(v,s')$, called 
\emph{push}, is added to the global collection $\mathcal{Q}$. The set of $\epsilon$-reductions 
is also calculated, when a new vertex is added to the GSS, and reductions from this set are added to the 
global queue $\mathcal{R}$. Reductions with length $l>0$ are calculated and added to $\mathcal{R}$ 
each time a new (non-$\epsilon$) edge is created. 

An input string can have several derivation trees and, as a rule, they can have 
numerous identical subtrees. Shared Packed Parse Forest (SPPF)~\cite{SPPF} is a directed graph
designed for a compact representation of all possible derivation trees.  
SPPF has the following structure: the \emph{root} (i.e. vertex with no incoming edges) corresponds 
to the starting nonterminal of the grammar; vertices with no outgoing edges correspond to terminals 
or derivation of $\epsilon$-string; the rest of the vertices is divided into two classes: \emph{nonterminal} 
and \emph{production}. Each nonterminal vertex keeps a collection of production nodes, each of which represents one  
possible derivation of that nonterminal. Production vertices represent a right-hand side of the 
production and keep an ordered list of terminal or nonterminal nodes. The length of this list lies
in the range $[l-k..l]$, where $l$ is the length of production right-hand side, and $k$ is 
the number of rightmost symbols, which derive $\epsilon$ (nullable symbols are ignored to reduce memory consumption).

SPPF is constructed simultaneously with GSS. Each edge of the GSS is associated with either 
a terminal or nonterminal node. When a GSS edge is added with a push, 
a new terminal node is created and associated with the edge. Nonterminal nodes are associated
with edges, which were added, when reductions were performed: if the edge has already been in GSS, 
a production node is added to the family of nonterminal nodes, associated with the edge. All subgraphs 
from the edges of the reduction path are added as children to the production node. After the input 
is read to the end, all vertices with accepting states are searched and nodes associated with 
outgoing edges of such vertices are merged to form the resulting SPPF. All unreachable vertices 
are deleted from the SPPF graph, which leaves only the actual derivation trees for the input.

The detailed algorithm description in the form of pseudocode can be found in Appendix~\ref{RNGLRCode}.
