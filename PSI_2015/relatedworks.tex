\section{Related Works}

Our approach for syntax analysis of string-embedded languages borrows some common principles
from existing techniques in this area. In addition, we reuse RNGLR syntax analysis algorithm 
and some accompanying constructs. In this section we provide a review and recollect some important
notions which will be referred to later on. 

The analysis of string-embedded languages as a rule requires a set of \emph{hot-spots} to
be indicated in the host application source code. Hot spot is considered as some ``point 
of interest'', where the analysis of the set of possible string values is desirable. This task can be
performed either in a user-assisted manner or automatically using some pragmatic 
considerations or knowledge of the framework being analyzed. The following logical steps 
include static analysis to construct an approximation for the set of all possible string values,
lexical, syntax, and, perhaps, some kind of semantic analysis. These steps not
necessarily performed separatedly; some of them may be omitted.

A rather natural idea of \emph{regular approximation} is to approximate the set of all possible 
strings by a regular expression. In recognition-centric formulation this approach boils down to
the problem of inclusion of approximating regular language into context-free reference language, which
is decidable for a number of practically significant cases~\cite{LangInclusion}.
Many approaches follow this route. In~\cite{Stranger} forward reachability analysis is used to compute regular 
approximation for all string values in the program. Further analysis is based on patterns detection in approximation 
set or generation of some finite subset of strings for analysis by standalone tools. Regular approximation in~\cite{JSA} 
is acquired by widening context-free approximation, initially built as a result of program analysis. 
Our approach is partially inspired by Alvor~\cite{Alvor,ALVOR2}, which utilizes GLR-based technique for syntax 
analysis of regular approximation; this framework implements abstract lexical analysis to convert a
regular language over characters into regular language over tokens, which simplifies syntax analysis.

Kyung-Goo Doh et al. in a series of papers~\cite{AbstrParsing,LRAbstrParsing,LRAbstrParsingSema} introduced an
approach, based on implicit representation of the set of potential strings as a system of data-flow equations. 
Conventional LALR(1) is chosen for the basis of parsing algorithm; original control tables are reused. 
Syntax analysis is performed as the system of dataflow equations is being solved iteratively in the space of abstract stacks.
The problem of infinite stack growth, which appears in general case, is handled using abstract 
interpretation~\cite{AbstractInterpretation}. This approach later evolved to a certain kind of semantic processing
in terms of attribute grammars which made it possible to analyze a wider class of languages, than
LALR(1).

\subsection{RNGLR}
Generalized LR parsing algorithm was presented by Masaru Tomita~\cite{Tomita}
as a solution for natural language processing and was intended to handle ambiguous
context-free grammars. Ambiguities of grammar produce Shift/Reduce and 
Reduce/Reduce conflicts. The algorithm uses parser tables similar to classical LR-tables,
each cell of which can contain multiple actions in case of conflicts. The general approach of 
the algorithm is to carry out all possible actions in these situations. The algorithm uses
graph-based data structures to effectively represent the set of stacks and derivation
trees.

However, Tomita's algorithm failed to process general context-free grammars.  
Elizabeth Scott and Adrian Johnstone presented Right-Nulled Generalized LR algorithm~\cite{RNGLR}
which extends and corrects Tomita's GLR parsing methods by
specific way of handling \emph{right nullable} rules (i.e. rules of the form 
$\mathrm{A} \rightarrow \alpha \beta$, where $\beta$ reduces to the empty string). 
That is, not only reductions for items $\mathrm{A} \rightarrow \alpha \cdot$ are 
applied, but also for the items of the form  $\mathrm{A} \rightarrow \alpha \cdot \beta$, 
where $\beta \Rightarrow \epsilon$. Thus, reduction length -- the number of 
symbols to be reduced to a nonterminal -- may be less than or equal to the length 
of righthand side of the rule. There are also possible reductions of 0-length, 
also called as $\epsilon$-reductions, corresponding to items of the form $\mathrm{A} 
\rightarrow \cdot$. All reductions are stored in the parse table as pairs of 
a nonterminal to reduce to and a reduction length.

To represent the set of stacks produced during conflict processing efficiently,
RNGLR algorithm uses Graph Structured Stack. GSS is an directed graph, 
vertices of which corresponds to elements of classical stack and edges link sequential 
elements together. Each vertex can have multiple outgoing edges to merge several stacks
together and have multiple incoming edges and, by means of it, be shared between stacks. 
Vertex is a pair $(s, l)$, where $s$ is a parser state and $l$ is a level -- position 
in an input string. Vertices in GSS are unique and there is no multiple edges. 

An input string could have several derivation trees and, as a rule, they have 
numerous identical subtrees. Shared Packed Parse Forest (SPPF) is a directed graph
utilized to produce compact representation of possible derivation trees. RNGLR 
algorithm constructs SPPF in the similar manner as Rekers does~\cite{SPPF}. 
SPPF has the following structure: the \emph{root} (i.e. the vertex
with no incoming edges) corresponds to the start nonterminal of the grammar; vertices 
with no outgoing edges correspond to the terminals or $\epsilon$-tree (which means 
$\mathrm{A} \Rightarrow \epsilon$); the rest of the vertices could be 
divided into 2 classes: nonterminal and production. Each nonterminal node has a 
\emph{family} (a collection) of production nodes, each of which present one of the 
possible derivations of the nonterminal. Production nodes representing righthand side 
of the production have an ordered collection of terminal or nonterminal nodes. The 
lenght of a collection could be equal to the lenght of the rule, or less, if final 
symbols derive $\epsilon$: nullable symbols are ignored to reduce memory consumption. 

RNGLR algorithm reads an input from left to right, one token at a time, and 
constructs levels of GSS sequentially for each position in the input: first, all the 
possible reductions are applied, then the next terminal of the input is shifted and
pushed to the GSS. When reduction or push is processed, the algorithm modifies GSS in 
the following manner. Consider adding an $(v_{t}, v_{h})$ edge to GSS. As the algorithm
builds a stack, the head vertex $v_{h}$ of the edge to be added is always in GSS by the 
time of adding. The tail vertex, on the other hand, could be either in GSS or not. In 
the first situation, new vertex does not created, only edge $(v_{t}, v_{h})$ is added 
if there were no such edge (else, nothing is changed in GSS). In the second situation, 
both new tail vertex and new edge is created and added to GSS. Every time new vertex
is created, the algorithm calculates new parser state by created vertex state and the 
next terminal of the input (i.e. calculates the \emph{push}), then adds the push to a
global collection $\mathcal{Q}$. The set of $\epsilon$-reductions is also calculated 
when new vertex is added to the GSS, and they are added to a global queue $\mathcal{R}$.
Edge creation entails the calculation of reductions with nonzero length. The detailed 
algorithm description and GSS construction routine can be found in the Appendix 
section~\ref{RNGLRCode}. 

SPPF and GSS are constructed simultaneously. Each edge of GSS is associated with either 
a terminal node or a nonterminal node. When a GSS edge is added while push processing, 
terminal node is created and associated with the edge. Nonterminal nodes are associated
with edges added during reduction processing: if the edge has already been in GSS, 
production node is added to the family of a nonterminal node associated with the edge.
Subgraphs from the edges of the reduction path are added as children to the production
node. After the input is read to the end, vertices with accepting states are searched,
nodes accociated with outgoing edges of such vertices are merged to form the actual
SPPF. All unreachable vertices are deleted from the SPPF graph which leaves only 
actual derivation trees for the input.