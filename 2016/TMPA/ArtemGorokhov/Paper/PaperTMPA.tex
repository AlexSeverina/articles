\documentclass[runningheads,a4paper]{llncs}

\usepackage{mathtools}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[caption=false]{subfig}

\usepackage{url}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}

\urldef{\mailsa}\path|gorohov.art@gmail.com|    
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

%To economize paper
%\textwidth=190mm
%\textheight=250mm
%\topmargin=-20mm
%\oddsidemargin=-15mm
%\evensidemargin=-15mm


\begin{document}

\algnewcommand\algorithmicswitch{\textbf{switch}}
\algnewcommand\algorithmiccase{\textbf{case}}
\algnewcommand\algorithmicassert{\texttt{assert}}
\algnewcommand\Assert[1]{\State \algorithmicassert(#1)}
% New "environments"
\algdef{SE}[SWITCH]{Switch}{EndSwitch}[1]{\algorithmicswitch\ #1\ \algorithmicdo}{\algorithmicend\ \algorithmicswitch}
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1}{\algorithmicend\ \algorithmiccase}

\algtext*{EndSwitch}
\algtext*{EndCase}
\algtext*{EndWhile}% Remove "end while" text
\algtext*{EndIf}% Remove "end if" text
\algtext*{EndFor}% Remove "end for" text
\algtext*{EndFunction}% Remove "end function" text

\newtheorem{mydef}{Definition}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Extended Context-Free Grammars Parsing with Generalized LL}

% a short form should be given in case it is too long for the running head
\titlerunning{ECFG parsing with GLL}

\author{Artem Gorokhov \and Semyon Grigorev}
\authorrunning{Artem Gorokhov}
% (feature abused for this document to repeat the title also on left hand pages)

\institute{ Saint Petersburg State University\\
            7/9 Universitetskaya nab.\\
            St. Petersburg, 199034 Russia\\
\email{\mailsa}
\\
\email{\path|semen.grigorev@jetbrains.com|}
}


\toctitle{Extended Context-Free Grammars Parsing with Generalized LL}
\tocauthor{Artem Gorokhov}
\maketitle

%Authors are invited to submit full papers (not exceeding 12 pages) or short papers (up to 6 pages)

\begin{abstract}
% Предлагается модификация алгоритма GLL, работающая с грамматиками в форме EBNF. 
% Это приносит ошутимый прирост производительности в работе с некоторыми грамматиками языков программирования.


%At least 70 and at most 150 words.
Parsing is important step of static program analysis. 
It allows to get structural representation of code.
Parser generators a widely used for parser creation. 
EBNF is very popular for languages syntax description.
But transformation to more simple form (BNF, CNF) is required for popular tools.
There are number of works on EBNF processing without transformation.
But problems.
Generalized LL, arbitrary grammars in $O(n^s)$, factorization can increase performance...
Factorization can be improved.
We propose modification of GLL which can handle arbitrary grammar in EBNF without transformations...
performance improvements, .....

\keywords{Parsing, GLL, SPPF, EBNF, ECFG, RRPG, Automata }
\end{abstract}


\section{Introduction}%--------------------------------------------------------------------------------------------------------------------------------------------

Static program analysis usually performed over structural representation of code and parsing is a classical way to get such representation.
Parser generators often used for parser creation automation: these tools allow to create parser from grammar of language which should be specified in appropriate format.
It allows to decrease efforts required for syntax analyzer creation and maintenance.

Extended BNF (EBNF) is a useful format of grammar specification. 
Expressive and compact description of language syntax. 
This formalism often used in documentation --- one of main source of information for parsers developers.

There are a wide range of parsing techniques and algorithms: CYK, LR(k), LALR(k), LL, etc. 
One of he most popular area is generalized parsing: technique which allows to handle ambiguous grammars. 
It is possible to simplify language description required for parser generation in case a parser generator is based on generalized algorithm.
LL family is more intuitive than LR, can provide better diagnostics, but LL(1) is not enough to process some languages: there are LR, but not LL languages.
Moreover, left and hidden left recursion in grammars is a problem. 
In order to solve these problems generalized LL (GLL) was proposed~\cite{scott2010gll}. 
This algorithm handles arbitrary context free grammar, even unambiguous and (hidden)left-recursive.
Worst-case time and space complexity of GLL is cubic in terms of input size. 
For LL grammars it demonstrates linear time and space complexity.

The problem is that classical parsing algorithms requires BNF.
It is possible to convert from EBNF to BNF but with this conversion we loose the structure of main grammar and resulting trees are for the BNF grammars.

ELL, ELR~\cite{AttributedELL,ELRR,ECFGparsing,ELLParser,ELL,ECFG,ELALR,ELRParsing} and other can process EBNF but they do not deal with ambiguities in grammars.

Factorization for GLL was introduced~\cite{scott2016structuring} but it is not full support of EBNF.

In this work we present modified generalized LL parsing algorithm which handles grammars in EBNF without transformations.
Changes are very native for GLL nature.
Proposed modifications allow to get sufficient parsing performance improvement.

This article is structured as follows.
We start from .... Extended BNF
generalized LL algorithm description.
Blah-blah


%Синтаксический анализ программ это широко известная область, ...
%Проблема в том, что грамматики, используемые в реальной жизни пишутся в форме EBNF. А GLL принимает только BNF.
%Можно проводить преобразование грамматики из EBNF к BNF, но так она разрастается, что, в некоторых случаях, замедляет процедуру разбора.
%Предлагается модификация алгоритма GLL, работающая с грамматиками в форме EBNF.



\section{EBNF processing}%--------------------------------------------------------------------------------------------------------------------------------------------

Extended Backus-Naur Form~\cite{iso} is a syntax of expressing context-free grammars. In addition to the Backus-Naur Form syntax it 
uses such constructions:
\begin{itemize}
    \item alternation $\mid$
    \item option [ ... ]
    \item repetition \{ ... \}
    \item grouping ( ... )
\end{itemize}
This form is usually used in technical documentation. While parser generators widely use Extended CFG form:
right parts of productions are regular expressions. It operates under similar constructions so EBNF can be represented in ECFG form.
In this article we will use the notation of ECFG.

An \textit{extended context-free grammar} (ECFG) is a tuple ($N$, $\Sigma$, $P$, $S$), where $N$
and $\Sigma$ are finite sets of nonterminals and terminals, $ S\in N$ is the start symbol,
and $P$ (the productions) is a map from $N$ to regular expressions over alphabet $N \cup \Sigma$.

%Regular expression syntax? Look at ``Towards a Taxonomy for ECFG and RRPG Parsing''



\section{Generalized LL Parsing}%--------------------------------------------------------------------------------------------------------------------------------------------

There are works about parsing ECFG: ELL(k)~\cite{!!!} and ELR(k)~\cite{!!!} parsers;
Early-style parsers~\cite{!!!} but none of them work with arbitrary ECFG.
Generalized algorithms (GLL and GLR) was purposed to perform syntax analysis of linear input by any context-free 
grammar. Unlike the GLR, GLL algorithm~\cite{scott2010gll} is rather intuitive and allows to perform better diagnostics.
As an output of GLL we get Shared Packed Parse Forest(SPPF)~\cite{scott2013gll} that represents all possible derivations of input string.

Work of the GLL algorithm based on descriptors, it allows to handle all posible derivations.
Descriptor is a four-element tuple $(L,i, T, S)$ that can uniquely define state of parsing process. 
$L$ is a grammar slot ---
pointer to position in grammar of the form$(S \to \alpha \cdot \beta)$, $i$ --- position in input,
T --- already built SPPF root, $S$ --- current Graph Structured Stack(GSS)~\cite{GSS} node.
We used efficient GSS described in~\cite{afroozeh2015faster}.
In initial state we have descriptors that describe start positions in grammar and input, dummy tree node and bottom of GSS.
On each step algorithm processes first descriptor in queue and makes actions depending on the grammar and input.
If there are any ambiguity algorithm will queue descriptor for all cases to handle them all. 

There are table based approach~\cite{ragozina} which allows to generate only tables for given grammar instead of full parser code.
The idea is similar to one in original article and main function uses same tree construction and stack processing functions.
Code can be found in appendix. Note: we do not include the check for first/follow sets in this paper.

\subsection{Factorization}%--------------------------------------------------------------------------------------------------------------------------------------------

In order to improve performance Elizabeth Scott and Adrian Johnstone offered support of factorised grammars in GLL~\cite{scott2016structuring}. 
The idea is to automatically factorize grammars and use them for parser generation. 

Main algorithm creates and queues new descriptors depending on current parse state that we get from unqueued descriptor. 
In case descriptor was already created it does not add it to queue. For this purpose we have a set of
\textbf{all} created descriptors. Thus reducing a number of possible descriptors decreases the parse time
and required memory.

Let us spot on \textbf{slots}. Factorization decreases the number of grammar slots. 
Consider example from the paper~\cite{scott2016structuring} on fig.~\ref{fig:ExampleOfFactorization}.

\begin{figure}
    \centering
    \subfloat[Production $P_0$]{
        $
        \begin{array}{crcl}
        S::&=& a& a\ B\ c\ d \\
           &|& a& a\ c\ d \\
           &|& a& a\ c\ e \\
           &|& a& a
        \end{array}
        $
    }
    ~
    \subfloat[Production $P_0\prime$]{
        $S::= a\ a\ ( B\ c\ d\ |\ c\ ( d\ |\ e )\ |\ \varepsilon)$
    }
    \caption{Example of factorization}
    \label{fig:ExampleOfFactorization}
\end{figure}
Production $P_0$ factorises to $P_0'$.
Second is much compact and contains much less possible slots, 
so parser creates less descriptors.
It gives significant performance improvement on some grammars.

But this idea can be extended to full EBNF support.
Let us show how to do it.


\section{Extended CFG GLL Parsing}%--------------------------------------------------------------------------------------------------------------------------------------------

In this section we will show an application of Extended Context-Free Grammars(ECFG) in automatons and corresponding GLL-style parsers.

The idea of factorisation was evolved to use of automatons and their minimization.
Right parts of ECFG are regular expressions under alphabet of terminals and nonterminals.
There are some basic methods of conversion regular expressions to nondeterministic finite state automatons. 
Thus for each right-hand side of grammar productions we can build a finite state automaton, with edges tagged with 
terminals, nonterminals or $\varepsilon$-symbols. Thompson's method~\cite{Thompson:1968:PTR:363347.363387} can be used for this purpose. 
In built automatons each nonterminal edge should be complemented with name of initial state of automaton that stands 
for this nonterminal. An example of constructed automatons for grammar $\Gamma_{0}$(fig.~\ref{fig:grammarG0}) is given on fig.~\ref{fig:initialAutomatonsForG0}. State 0 is start state.
We will call final states of automatons ``pop'' states.

Produced automatons are $\varepsilon$-NFAs under alphabet of nonterminals, terminals and $\varepsilon$ symbols.
Decrease of the quantity of the automaton states decreases number of GLL descriptors, as it was with factorization.
Thus to increase performance of parsing we can minimize the number of states in produced automatons.
%without loosing the structure of initial grammar.

First, every automaton should be converted to determinate FA. An algorithm is described in~\cite{aho1974design}.
Then John Hopcroft's algorithm~\cite{hopcroft1971n} can be applied to all automatons at one time. 
An algorithm is based on dividing all states on equivalent classes. Initial state of algorithm consist 
of 2 classes: first contains final states and second contains all other. For our problem we can set an 
initial state as follow: first class contains all final states of \textbf{all} automatons and second class 
contains all the other. As an algorithm result we get classes which represent new states of automaton. 
%Initial state is class that contains initial state of automaton that represents productions of start nonterminal.
An example for grammar $G_0$ is shown on fig.~\ref{fig:minimizedAutomatonsForG0}.

Now we can define automaton that will be used for parsing.

\begin{mydef}
Automaton $A$ is a tuple $(\Sigma, N, Q, S, A, P, \delta, F)$, where
$\Sigma$ is a set of terminals,
$N$ --- set of nonterminals,
$Q$ --- set of states,
$S \in Q$ --- start state,
$A \in N$ --- start nonterminal,
$P \in Q$ --- set of ``pop'' states,
$\delta : Q \times M \to Q$, 
$M \in \Sigma \cup N$ --- edges of automaton,
$F : N \to Q$ --- start states of nonterminals.
\end{mydef}

Note: this automatons are not an abstract machines, but just a ``convenient'' representations of grammars.

\begin{figure}
    \centering
    \subfloat[Grammar $G_0$]{
        $
        \begin{array}{crcl}
            S& ::= &(a&|B)\ a \\
            B& ::= &a& \\
        \end{array}
        $
        \label{fig:grammarG0}
    }
    ~
    \subfloat[Automatons for productions]{
        \includegraphics[scale=.48]{pictures/automatonForG0.pdf}
        \label{fig:initialAutomatonsForG0}
    }
    ~
    \subfloat[Minimized automaton]{
        \includegraphics[scale=.46]{pictures/minimizedAutomatonForG0.pdf}
        \label{fig:minimizedAutomatonsForG0}
    }
    \caption{Example of automatons}
    \label{fig:fig1}
\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=4cm]{pictures/SPPFforG0.pdf}
%    \caption{SPPF for input "aa"}
%    \label{fig:SPPF}
%\end{figure}


\subsection{Input processing}%--------------------------------------------------------------------------------------------------------------------------------------------

Slots have become automaton states. And just as we can move through grammar slots we can move through states 
of automaton. But in automaton we have nondeterministic choice because there can be many edges to other states.
Such significant cases:
\begin{itemize} 
\item edge that contains current input terminal exists
a) it leads to ``pop'' state
b) next state is not ``pop''
\item nonterminal edge exists 
\end{itemize}
Both cases can be simultaneously, this brings nondeterminism. For second case we just can call create function for each nonterminal. But for the terminal case we need to add descriptor that describes next position to queue without checking it's existence in descriptor elimination set.
%So we need to prevent creation of descriptors for each nonterminal on outgoing edges. We can generate tables that 
%tells us what nonterminals can infer strings that starts with current terminal. And add descriptors only for 
%this edges. Moreover we need to create descriptor for edge that marked with current terminal if such exists.

%In states of parsing we can have a nondeterministic choice because the states of automaton can be ``pop'' states and outgoing edges can contain nonterminals.
%If there exist outgoing edge that contains current terminal we need to create intermediate node.
%But if the next state is ``pop'' state we also need to create nonterminal node and call \textbf{pop} function.
%Moreover we need to call \textbf{create} function for edges that contains nonterminal.
%To handle nondeterminism \textbf{parse} function queues new descriptors for all described cases.

\input{add.tex}
Function \textbf{add} queues descriptor if it was not already created.
\input{create.tex}
Function \textbf{create} is called when we meet nonterminal on edge.
It performs necessary operations with GSS and checks if there are already built SPPF for current
input position and nonterminal.
\input{pop.tex}
\textbf{Pop} function is called when we reach ``pop'' state. It queues descriptors for all outgoing edges from current GSS node.
\input{parse.tex}
The main function \textbf{parse} handles queued descriptor and checks all outgoing edges from current state to be appropriate
for transition depending on current input terminal, and symbol on edge.


\subsection{SPPF construction}

First, we should define derivation trees for automatons: it is an ordered tree whose root labeled with start state,
leaf nodes are labeled with a terminals from automaton's edges or $\varepsilon$ and interior nodes are labeled with 
nonterminals from automaton's edges and have a sequence of children that corresponds to edge labels of path in 
automaton that starts from the start state of this nonterminal. More formal. 

\begin{mydef}

Derivation tree of sentence $\alpha$ for the automaton $A=(\Sigma, N, Q, S, A, P, \delta, F)$:%grammar $G=(\Sigma, N, S, P)$:

\begin{itemize}
\item Ordered rooted tree. Root labeled with $A$
\item Leafs are terminals $\in \Sigma$
\item Nodes are nonterminals $\in N$
\item Node with label $N_i \in N$ has children $l_0 \dots l_n (l_i \in \Sigma \cup N)$ iff exists
path
$F(N_i) \xrightarrow[]{l_0} \dots \xrightarrow{l_n} q_m$, $q_m \in P$. 
%for $\omega = l_0 \cdot l_1 \dots\ l_n\in (\Sigma \cup N)^*$ exists $p \rightarrow M \in P$ such that $\omega \in L(M)$
\end{itemize}

\end{mydef}

Automaton is ambiguous if there exist string that have more than one derivation trees. Thus, we can define SPPF for automatons. 
It is similar to SPPF for grammars described in~\cite{scott2013gll}. SPPF contains symbol nodes, packed nodes
and intermediate nodes. 

Packed nodes are of the form $(S, k)$, where $S$ is a state of automaton. 
Symbol nodes have labels $(X, i, j)$ where $X \in \Sigma \cup N \cup \varepsilon$. 
Intermediate nodes have labels $ (S, i, j) $, where $S$ is a state of automaton. $i$ is position in input before leftmost leaf terminal, $j$ --- position after rightmost leaf.

Packed node necessarily has right child --- symbol node, and optional left child --- symbol or intermediate node.
Nonterminal and intermediate nodes may have several packed children. 
Terminal symbol nodes are leafs.

Use of intermediate and packed nodes leads to binarization of SPPF and thus the space complexity is $O(n^{3})$.
%But in grammars slot defines position,
%previous and next symbol, when DFA state tells the position only. Thus we can can construct SPPF using 
%In general, we can't uniquely correspond an original grammar slot to automaton state.
%We can consider example. For the grammar \ref{fig:grammarG0}, automaton will be represented as showed on fig.\ref{fig:automatonForG0}.
%SPPF for input "aa" is on fig.\ref{fig:SPPF}.




State 1 can be matched with two grammar slots: $S ::= (a \cdot a)|(b$ $a)$ and $S ::= (a$ $a)|(b \cdot a)$. 
But SPPF represents WHAT???




\textbf{function} getNodeT$(x,i)$ did not change

We defined function \textbf{getNodes} which can construct two nodes: intermediate and nonterminal (at least one of them, at most both).
It uses modified function \textbf{getNodeP} that takes additional argument: state or nonterminal name. Symbol in returned SPPF node will be this argument's value.
\input{getNodes.tex}
\input{getNodeP.tex}



\section{Evaluation}

Left factorization vs EBNF

Small demo example (message to Scott)

\begin{figure}[h]
$$
\begin{array}{crcl}
S ::=& A\ A\ A\ A\ A\ A \\
     &|A\ a\ A\ A\ A\ A \\
A ::=& S\ A\ |\ a\ A\ |\ a \\
\end{array}
$$
\caption{Grammar $G_0$.}
\label{testGrammar}
\end{figure}

We have compared our parsers built on factorized grammar and on minimized automatons.
Grammar $G_0$(fig.~\ref{testGrammar}) was used for the tests,
it has long ``common tail'' which is not unified with factorization.
FSA built for this grammar presented on fig.~\ref{dfa}.

\begin{wrapfigure}{R}{0.2\textwidth}
\centering
\includegraphics[width=0.15\textwidth]{pictures/minimizedDFA.pdf}
\caption{\label{dfa}Minimized automaton for grammar $G_0$}
\end{wrapfigure}

Description of input. 
Short info about PC.

Note: SPPF construction was disabled while testing.

\begin{table}[h]
\begin{center}
  \begin{tabular}{ | l | l | l | l | l | l | l | l | l | }
\hline
    Length & \multicolumn{2}{ c| }{Time, seconds} & \multicolumn{2}{ c| }{Descriptors} & \multicolumn{2}{ c| }{GSS Nodes} & \multicolumn{2}{ c| }{GSS Edges} \\ \hline
     & factorized & minimized & factorized & minimized & factorized & minimized & factorized & minimized \\ \hline
    100 & 0.206 & 0.127 & 52790 & 38530 & 200 & 200 & 42794 & 28534 \\ \hline
    200 & 1.909 & 1.54 & 215540 & 157030 & 400 & 400 & 175544 & 117034 \\ \hline
    300 & 8.844 & 7.125 & 488290 & 355530 & 600 & 600 & 398294 & 265534 \\ \hline
    400 & 25.876 & 21.707 & 871040 & 634030 & 800 & 800 & 711044 & 474034 \\ \hline
    500 & 60.617 & 51.245 & 1363790 & 992530 & 1000 & 1000 & 1113794 & 742534 \\ \hline
    1000 & 842.779 & 768.853 & 5477540 & 3985030 & 2000 & 2000 & 4477544 & 2985034 \\ \hline
     & \multicolumn{2}{ c| }{Average gain: 19$\%$} & \multicolumn{2}{ c| }{Average gain: 27$\%$} & \multicolumn{2}{ c| }{Average gain: 0$\%$} & \multicolumn{2}{ c| }{Average gain: 33$\%$} \\ \hline
\end{tabular}
\end{center}
\caption{Experiments results.}
\label{expTable}
\end{table}

Table~\ref{expTable} shows that in general minimized version works $19\%$ faster, uses $27\%$ less descriptors and $33\%$ less GSS edges.
Also we use this automaton approach in metagenomic assemblies parsing and it gives visible performance increase.
A bit more discussion on evaluation.


Examples of SPPF.
May be some nontrivial cases: s -> a* a* and so on


\section{Conclusion and Future Work}

Described algorithm implemented in F\# as part of the YaccConstructor project.
Source code available here:~\cite{YaccConstructor}.

Proposed modification can not only increase performance, but also decrease memory usage. 
It is critical for big input processing.
For example, Anastasia Ragozina in her master's thesis~\cite{ragozina} shows that GLL can be used for graph parsing.  
In some areas graphs can be really huge: assemblies in bioinfomatics ($10^8$...).
Proposed modification can improve performance not only in case of classical parsing, but in graph parsing too. 
We perform some tests that shows performance increasing in metagenomic analysis, but full integration with graph parsing and formal description is required.

One of way to specify any useful manipulations on derivation tree (or semantic of language) is an attributed grammars~\cite{!!!}.
YARD supports it but our algorithm is not.
So, attributed grammar and semantic calculation is a future work.

Yet another question is possibility of unification our results with tree languages: our definition of derivation tree for ECFG is quite similar to unranced tree and SPPF is similar to automata for unranced trees~\cite{TATA}.
Theory of tree languages seems more mature than theory of general SPPF manipulations.


\bibliographystyle{abbrv}
\bibliography{bibliography}
\input{appendix}
\end{document}
