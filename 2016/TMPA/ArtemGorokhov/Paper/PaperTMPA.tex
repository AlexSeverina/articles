\documentclass[runningheads,a4paper]{llncs}

\usepackage{mathtools}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[caption=false]{subfig}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.9}

\usepackage{url}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}
  
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

%To economize paper
%\textwidth=190mm
%\textheight=250mm
%\topmargin=-20mm
%\oddsidemargin=-15mm
%\evensidemargin=-15mm


\begin{document}

\algnewcommand\algorithmicswitch{\textbf{switch}}
\algnewcommand\algorithmiccase{\textbf{case}}
\algnewcommand\algorithmicassert{\texttt{assert}}
\algnewcommand\Assert[1]{\State \algorithmicassert(#1)}
% New "environments"
\algdef{SE}[SWITCH]{Switch}{EndSwitch}[1]{\algorithmicswitch\ #1\ \algorithmicdo}{\algorithmicend\ \algorithmicswitch}
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1}{\algorithmicend\ \algorithmiccase}

\algtext*{EndSwitch}
\algtext*{EndCase}
\algtext*{EndWhile}% Remove "end while" text
\algtext*{EndIf}% Remove "end if" text
\algtext*{EndFor}% Remove "end for" text
\algtext*{EndFunction}% Remove "end function" text

\newtheorem{mydef}{Definition}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Extended Context-Free Grammars Parsing with Generalized LL}

% a short form should be given in case it is too long for the running head
\titlerunning{ECFG parsing with GLL}

\author{Artem Gorokhov \and Semyon Grigorev}
\authorrunning{Artem Gorokhov, Semyon Grigorev}
% (feature abused for this document to repeat the title also on left hand pages)

\institute{ Saint Petersburg State University\\
            7/9 Universitetskaya nab.\\
            St. Petersburg, 199034 Russia\\
\email{\path|gorohov.art@gmail.com| }
\\
\email{\path|semen.grigorev@jetbrains.com|}
}


\toctitle{Extended Context-Free Grammars Parsing with Generalized LL}
\tocauthor{Artem Gorokhov}
\maketitle

%\tableofcontents

%Authors are invited to submit full papers (not exceeding 12 pages) or short papers (up to 6 pages)

\begin{abstract}
Parsing plays an important role in static program analysis: during this step a structural representation of code is created upon which further analysis is performed. 
Parser generator tools, being provided with syntax specification, automate parser development. 
Language documentation often acts as such specification. 
Documentation usually takes form of ambiguous grammar in Extended Backus-Naur Form which most parser generators fail to process. 
Automatic grammar transformation generally leads to parsing performance decrease. 
Some approaches support EBNF grammars natively, but they all fail to handle ambiguous grammars. 
On the other hand, Generalized LL parsing algorithm admits arbitrary context-free grammars and achieves good performance, but cannot handle EBNF grammars. 
The main contribution of this paper is a modification of GLL algorithm which can process grammars in a form which is closely related to EBNF (Extended Context-Free Grammar). 
We also show that the modification improves parsing performance as compared to grammar transformation based approach. 

\keywords{Parsing, Generalized Parsing, Extended Context-Free Grammar, GLL, SPPF, EBNF, ECFG, RRPG, Recursive Automata}
\end{abstract}


\section{Introduction}%--------------------------------------------------------------------------------------------------------------------------------------------

Static program analysis is usually performed over a structural representation of code and parsing is a classical way to get such representation.
Parser generators are often used to automate parser creation: these tools derives parser from grammar.
It decreases amount of effort required for syntax analyzer development and maintenance.

Extended Backus-Naur Form~\cite{EBNFISO} is a metasyntax for expressing context-free grammars. 
In addition to the Backus-Naur Form syntax it uses the following constructions: alternation $\mid$, optional symbols [ ... ], repetition \{ ... \}, and grouping ( ... ).

This form is widely used for grammar specification in technical documentation because expressive power of EBNF makes syntax specification much more compact and human-readable. 
Because documentation is one of the main sources of information for parsers developers, it would be helpful to have a parser generator which supports grammar in EBNF.
Note, that EBNF is a standardized notation for \textit{extended context-free grammars}~\cite{ECFG} which can be defined as follows.

\begin{mydef}
An \textbf{extended context-free grammar} (ECFG)~\cite{ECFG} is a tuple ($N$, $\Sigma$, $P$, $S$), where $N$
and $\Sigma$ are finite sets of nonterminals and terminals respectively, $ S\in N$ is the start symbol,
and $P$ (the productions) is a map from $N$ to regular expressions over alphabet $N \cup \Sigma$.
\end{mydef}

ECFG is widely used as an input format for parser generators, but classical parsing algorithms often require CFG, and, as a result, parser generators usually require conversion to CFG.
It is possible to transform ECFG to CFG~\cite{ELL}, but this transformation leads to grammar size 
increase and change in grammar structure: new nonterminals are added during transformation.
As a result, parser constructs derivation tree with respect to the transformed grammar, making it harder for a language developer to debug grammar and use parsing result later.

There is a wide range of parsing techniques and algorithms~\cite{AttributedELL,ELRR,ECFGparsing,ELLParser,ELL,ECFG,ELALR,ELRParsing} which are able to process grammar in ECFG.
Detailed review of results and problems in ECFG processing area is provided in the paper ``Towards a Taxonomy for ECFG and RRPG Parsing''~\cite{ECFG}. 
We only note that most of algorithms are based on classical LL~\cite{ELLParser,AttributedELL,PredictiveECFG} and LR~\cite{ELRParsing,ELALR,ELRR} techniques, and they can handle only appropriate subclasses of ECFG.
As a result, parsing techniques for ECFG is an actual problem, and there is no solution for handling arbitrary (including ambiguous) ECFGs.

The LL-based parsing algorithms are more intuitive than LR-based and can provide better error diagnostic.
Currently LL(1) seems is be the most practical algorithm.
Unfortunately, some languages are not LL(k) for any $k$ , and left recursive grammars are a problem for LL-based tools.
Another restriction for LL parsers is ambiguities in grammar which, being combined with previous flaws, complicates industrial parsers creation.
Generalized LL, proposed in~\cite{scott2010gll},solves all these problems: it handles arbitrary CFGs, including ambiguous and left recursive.  
Worst-case time and space complexity of GLL is cubic in terms of input size and, for LL(1) grammars, it demonstrates linear time and space complexity.

In order to improve performance of GLL algorithm, modification for left factorized grammars processing was introduced in~\cite{scott2016structuring}.
Factorization transforms grammar so that there are no two productions with same prefixes (see fig~\ref{fig:ExampleOfFactorization} for example).
It is shown, that factorization can reduce memory usage and increase performance by reusing common parts of rules for one nonterminal.
Similar idea can be applied to ECFGs processing (module some details).

To summarize, if it were possible to handle ECFG specification with tools based on generalized parsing algorithm, it would greatly simplify language development.
In this work we present a modification of generalized LL parsing algorithm which handles arbitrary ECFGs without any transformations.
Also we demonstrate that proposed modifications improve parsing performance and memory usage.

\section{ECFG Handling with Generalized LL Algorithm}%--------------------------------------------------------------------------------------------------------------------------------------------

The purpose of generalized parsing algorithms is to provide arbitrary context-free grammars handling,.
Generalized LL algorithm~(GLL)~\cite{scott2010gll} inherits properties of classical LL algorithms: it is more intuitive and provides better syntax error diagnostic than generalized LR algorithms.
Also, our experience shows that GLR-based solutions are more complex rather then GLL-based, which agrees with the observation in~\cite{ECFG} that LR-based ECFG parsers are very complex. 
Thus, we choose GLL as a base for our solution.
In this section we present GLL-style parser for arbitrary ECFG processing.

\subsection{Generalized LL Parsing Algorithm}%--------------------------------------------------------------------------------------------------------------------------------------------

An idea of the GLL algorithm is based on descriptors which can uniquely define state of parsing process.
Descriptor is a four-element tuple $(L, i, T, S)$ where elements are defined as fallows: 
\begin{itemize}
\item $L$ is a grammar slot --- pointer to position in grammar of the form~$(S \to \alpha \cdot \beta)$;
\item $i$ --- position in the input;
\item $T$ --- already built node of parse forest;
\item $S$ --- current Graph Structured Stack~(GSS)~\cite{afroozeh2015faster} node.
\end{itemize}

In the initial state there is only one descriptor which consists of start positions in grammar ($L = (S \to \cdot \beta)$) and input ($i=0$), dummy tree node and the bottom of GSS.
Queue is used for descriptors processing control.
The algorithm dequeues the first descriptor and acts depending on the grammar and input.
If there is an ambiguity, then algorithm queues descriptors for all cases to handle them later. 

There is a table based approach~\cite{ragozina} which generates only tables for given grammar instead of full parser code.
The idea is similar to the one in the original paper and uses the same tree construction and stack processing routines.
Pseudocode illustrating this approach can be found in appendix~\ref{GLLCode}. Note, that we do not include check for first/follow sets in this paper.


\subsection{Factorization}%--------------------------------------------------------------------------------------------------------------------------------------------

In order to improve performance of GLL Elizabeth Scott and Adrian Johnstone offered a support for left-factorized grammars in this parsing algorithm~\cite{scott2016structuring}. 

The basic algorithm creates and queues new descriptors depending on the current parse state that obtained from dequeued descriptor. 
A descriptor is enqueued only if it has not been created before.
Also, all processed descriptors stored to avoid its duplication.
Thus, by reducing descriptors to process, parse time and the amount of required memory can be decreased.

One of the way to decrease the number of descriptors is to reduce the number of grammar slots. 
This can be achieved by grammar factorization.
Consider example of grammar and its factorized version from the paper~\cite{scott2016structuring} in fig.~\ref{fig:ExampleOfFactorization}.

\begin{figure}
    \centering
    \subfloat[Production $P_0$]{
        $
        \begin{array}{rl}
        S::=& a a\ B\ c\ d \\
        |& a a\ c\ d \\
        |& a a\ c\ e \\
        |& a a
        \end{array}
        $
    }
    ~
    \subfloat[Production $P_0\prime$]{
        $
        \begin{array}{rl}
        S::= a\ a\ (& B\ c\ d\ \\
                   |&\ c\ ( d\ |\ e )\ \\
                   |&\ \varepsilon \ )
        \end{array}
        $
    }
    \caption{Example of factorization}
    \label{fig:ExampleOfFactorization}
\end{figure}

Production $P_0$ transforms to $P_0'$ during factorization.
Second one is much more compact and contains less possible slots, so parser creates less descriptors.
It gives significant performance improvement on some grammars.

We can evolve this idea to support ECFG.
In the next section, we will show how to do it.

\subsection{Recursive automata}

In order to ease adoption of ideas of grammar factorization for handling ECFGs with GLL we use recursive automata (RA)~\cite{tellier2006learning} for ECFG representation.
We use the following definition of RA.

\begin{mydef}
    Recursive automaton~(RA) $R$ is a tuple $(\Sigma, Q, S, F, \delta)$, where
    $\Sigma$ is a set of terminals,
    $Q$ --- set of states,
    $S \in Q$ --- start state,
    $F \subseteq Q$ --- set of final states,
    $\delta : Q \times (\Sigma \cup Q) \to Q$ --- transition function.
\end{mydef}

The only difference between Recursive Automaton and Finite State Automaton (FSA) is that transitions in RA are labeled either by terminal ($\Sigma$) or by state ($Q$).
Further in this paper, we call transitions by elements from $Q$ as \textit{nonterminal transitions} and by terminal as \textit{terminal transitions}.

Right parts of ECFG productions are regular expressions over the alphabet of terminals and nonterminals.
Thus for each right-hand side of grammar productions we can build a finite state automaton 
using Thompson's method~\cite{Thompson:1968:PTR:363347.363387}. 
To transform the set of produced automata we need to eliminate $\varepsilon$-transitions and replace
transitions by nonterminals with transitions labeled by start states of corresponding to nonterminal FSA.
An example of constructed recursive automata for grammar $\Gamma_{0}$(fig.~\ref{fig:grammarG0})
is given on fig.~\ref{fig:initialAutomatonsForG0}, state 0 is start state.

Decrease of the number of the automaton states reduces the number of GLL descriptors, as it was with factorization.
Thus to increase performance of parsing we can minimize the number of states in produced automaton.

First, RA should be converted to deterministic RA using the algorithm for FSA described in ~\cite{aho1974design}.
Then John Hopcroft's algorithm~\cite{hopcroft1971n} can be applied to RA to minimize the number of states.
An example for grammar $G_0$ is shown on fig.~\ref{fig:minimizedAutomatonsForG0}.

\begin{figure}
    \centering
    \subfloat[Grammar $G_0$.]{
        $
        \begin{array}{rll}
            S ::= &(a&|B)\ a \\
            B ::= &a& \\
        \end{array}
        $
        \label{fig:grammarG0}
    }
    ~
    \subfloat[Initial RA.]{
        \includegraphics[scale=.48]{pictures/G0initialAutomatons.pdf}
        \label{fig:initialAutomatonsForG0}
    }
    ~
    \subfloat[Minimized RA.]{
        \includegraphics[scale=.46]{pictures/G0minimizedAutomaton.pdf}
        \label{fig:minimizedAutomatonsForG0}
    }
    \caption{Example of automata}
    \label{fig:fig1}
\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=4cm]{pictures/SPPFforG0.pdf}
%    \caption{SPPF for input "aa"}
%    \label{fig:SPPF}
%\end{figure}


\subsection{Input processing}%--------------------------------------------------------------------------------------------------------------------------------------------

A GLL idea is to traverse through grammar and input simultaneously, creating multiple descriptors for the case of ambiguity.

Just as we can move through grammar slots we can move through states of automaton for grammar.
We replace grammar slot in descriptor by in constructed RA.
The problem is that in automaton we have nondeterministic choice because there can be many transitions to other states.
Consider such significant cases:

\begin{itemize} 
\item there are transition by current input terminal to final state
\item there are transition by current input terminal to state that is not final
\item there are nonterminal transition
\end{itemize}

All of them should be handled and this leads to nondeterminism. 
For the last case we just can call create function for each state.
But for the terminal cases we need to add descriptor that describes next position to queue without checking its existence in descriptor elimination set.
Thus we use descriptors queue to handle nondeterminism in states, while original algorithm uses it to handle ambiguity in grammars.

\input{add.tex}

Function \textbf{add} queues descriptor if it has not already been created.

\input{create.tex}

Function \textbf{create} is called when we meet nonterminal transition.
It performs necessary operations with GSS and checks if there are already built SPPF for current
input position and nonterminal.

\input{pop.tex}

\textbf{Pop} function is called when we reach final state. It queues descriptors for all outgoing edges from current GSS node.

\input{parse.tex}

The main function \textbf{parse} handles queued descriptor and checks all transitions from current state to be appropriate
for current input terminal, or calls create function when meets nonterminal transitions.


\subsection{Parse forest construction}
!!!!!!
Later, in order to build SPPF, we will need not only start state number, but nonterminal names.
For this purpose we define function $\Delta : Q \to N$ where $N$ is nonterminal name.


Result of the parsing process is structural representation of input --- a derivation tree, or parse forest for the case of many derivation variants.

First, we should define derivation tree for recursive automaton: it is an ordered tree whose root is labeled with start state,
leaf nodes are labeled with terminals or $\varepsilon$ and interior nodes are labeled with 
nonterminals $A$ and have a sequence of children that corresponds to transition labels of path in 
automaton that starts from the state $\Delta(A)$.

\begin{mydef}

Derivation tree of sentence $\alpha$ for the recursive automaton $R=(\Sigma, Q, S, F, \delta)$:%grammar $G=(\Sigma, N, S, P)$:

\begin{itemize}
\item Ordered rooted tree. Root is labeled with $\Delta(S)$
\item Leaves are terminals $a\in \Sigma$
\item Nodes are nonterminals $A\in \Delta(Q)$
\item Node with label $N_i \in \Delta(q_i)$ has children $l_0 \dots l_n (l_i \in \Sigma \cup \Delta(Q))$ iff there exists a
path
$q_i \xrightarrow[]{l_0} q_{i+1} \xrightarrow[]{l_1} \dots \xrightarrow{l_n} q_m$, $q_m \in F$. 
%for $\omega = l_0 \cdot l_1 \dots\ l_n\in (\Sigma \cup N)^*$ exists $p \rightarrow M \in P$ such that $\omega \in L(M)$
\end{itemize}

\end{mydef}

RA is ambiguous if there exists string that has multiple derivation trees. 
We work with arbitrary grammars, thus our RA can be ambiguous and we can reuse Shared Packed Parse Forest (SPPF)~\cite{SPPF} that can represent all possible derivation trees.
It is similar to SPPF for grammars described in~\cite{scott2013gll}.
SPPF contains symbol nodes, packed nodes and intermediate nodes. 

Packed nodes are of the form $(S, k)$, where $S$ is a state of automaton. 
Symbol nodes have labels $(X, i, j)$ where $X \in \Sigma \cup \Delta(Q) \cup \{\varepsilon\}$. 
Intermediate nodes have labels $ (S, i, j) $, where $S$ is a state of automaton.
Position in the input previous to leftmost leaf terminal is denoted by $i$, and position succeeding rightmost leaf --- $j$.

Packed node necessarily has right child --- symbol node, and optional left child --- symbol or intermediate node.
Nonterminal and intermediate nodes may have several packed children. 
Terminal symbol nodes are leaves.

Use of intermediate and packed nodes leads to binarization of 
SPPF and thus provides better sharing. So in general this representation of 

\subsection{SPPF construction functions}
The function \textbf{getNodeT$(x,i)$} which create terminal nodes is reused without modification from basic algorithm.
To handle nondeterminism in states we defined function \textbf{getNodes} which checks if the next state of RA
is final and for that case constructs nonterminal nodes in addition to intermediate.
It uses modified function \textbf{getNodeP} that takes additional argument: state or nonterminal name.
Symbol in constructed SPPF node becomes the value of the arguments.
\input{getNodes.tex}
\input{getNodeP.tex}

\section{Evaluation}

We have implemented parser generator for proposed algorithm, and
in this section we show SPPF example and performance comparison 
with the parser built on factorized grammar.

\subsection{SPPF example}

For the SPPF example we have taken ECFG grammar $G_1$(fig.~\ref{fig:grammarG1}).
It contains constructions(option and repetition) that should be converted with
use of extra nonterminals to build regular GLL parser. Our generator constructs
recursive automaton $R_1$(fig.~\ref{fig:automatonForG1}) and parser for it.

For input $ aabk $ this parser builds SPPF showed on fig.~\ref{fig:SPPFForG1}.

\begin{figure}[H]
    $$
    \begin{array}{rl}
    S ::= &(a^{*}\ b?\ k)\ |\ M \\
    M ::= &a\ a\ b\ k \\
    \end{array}
    $$
    \caption{Grammar $G_1$.}
    \label{fig:grammarG1}
\end{figure}
\begin{figure}[H]
    \centering
    \subfloat[RA $R_1$ for $G_1$.]{
        \includegraphics[scale=.5]{pictures/G1automaton.pdf}
        \label{fig:automatonForG1}
    }
    ~
    \subfloat[SPPF for $R_1$ and input $ aabk $.]{
        \includegraphics[scale=.5]{pictures/G1SPPF.pdf}
        \label{fig:SPPFForG1}
    }
    \caption{Example of SPPF.}
    %\label{fig:fig2}
\end{figure}


\subsection{Performance measure}

We have compared our parsers built on factorized grammar and on minimized recursive automaton.
Grammar $G_2$(fig.~\ref{fig:grammarG2}) was used for the tests,
it has long tails in alternatives which are not unified with factorization.
FSA built for this grammar presented on fig.~\ref{fig:automatonForG2}.

\begin{figure}[H]
    \centering
    \subfloat[Grammar $G_2$.]{
        $
        \begin{array}{rc}
        S ::=& K\ (K\ K\ K\ K\ K \\
             &     |a\ K\ K\ K\ K) \\
        K ::=& S\ K\ |\ a\ K\ |\ a \\
        \end{array}
        $
        \label{fig:grammarG2}
    }

    \subfloat[RA for grammar $G_2$.]{
        \includegraphics[scale=.5]{pictures/G2automaton.pdf}
        \label{fig:automatonForG2}
    }
    \caption{Grammar $G_2$ and RA for it.}
\end{figure}

For this grammar parser for RA should create less GSS edges because the tails of alternatives in producions
are represented by the only path in RA. This fact leads to decrease of SPPF nodes and descriptors.

Experiments were performed on inputs of different length and are presented in fig.~\ref{expPlots1} and fig.~\ref{expPlots2}.
Exact values for the input $a^{40}$ shown in table~\ref{expTable}.

All tests were run on a PC with the following characteristics:
\begin{itemize}
    \item OS: Microsoft Windows 10 Pro x64
    \item CPU: Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz, 3601 Mhz, 4 Cores, 4 Logical Processors
    \item RAM: 32 GB
\end{itemize}

\begin{table}[h]   
\begin{center}
    \begin{tabular}{ | c | c | c | c | c | c |}
        \hline
                           & Time, s & Descriptors & GSS Edges & GSS Nodes & SPPF Nodes   \\ \hline
        Factorized grammar & 81.814  & 7940        & 6974      & 80        & 111127244  \\ \hline
        Minimized RA       & 54.637  & 5830        & 4234      & 80        & 74292078  \\ \hline
    \end{tabular}
\end{center}
\caption{Experiments results for input $a^{40}$.}
\label{expTable}
\end{table}

\input{performancePlots1.tex}

\input{performancePlots2.tex}

Results of performed experiments approve the fact that on some grammars or approach 
show better results then parsers built on factorized grammars.
With grammar $G_2$ in general minimized RA version works $33\%$ faster, uses $27\%$ less descriptors, $29\%$ less GSS edges
and $33\%$ less SPPF nodes.

%We also use this automaton approach in metagenomic assemblies parsing and it gives visible performance increase.

%A bit more discussion on evaluation.

%Examples of SPPF.

%May be some nontrivial cases: s -> a* a* and so on



\section{Conclusion and Future Work}

Described algorithm and parser generator based on it implemented in F\# as part of the YaccConstructor project.
Source code available here:~\url{https://github.com/YaccConstructor/YaccConstructor}.

As we show in evaluation, proposed modification not only increase performance, but also decrease memory usage. 
It is critical for big input processing.
For example, Anastasia Ragozina in her master's thesis~\cite{ragozina} shows that GLL can be used for graph parsing.  
In some areas graphs can be really huge: metagenomic assemblies in bioinfomatics, social graphs.
We hope that proposed modification can improve performance not only in case of classical parsing, but in graph parsing too. 
We perform some tests that shows performance increasing in metagenomic analysis, but full integration with graph parsing and formal description is required.

One of way to specify any useful manipulations on derivation tree (or semantic of language) is an attributed grammars, but it is not supported in the algorithm which presented in this article.
There is number of works on subclasses of attributed ECFGs (for example~\cite{AttributedELL}), however still no solution for arbitrary ECFGs.
Thus, arbitrary attributed ECFGs and semantic calculation support is a future work.

Yet another question is possibility of unification our results with tree languages: our definition of derivation tree for ECFG is quite similar to unranked tree and SPPF is similar to automata for unranked trees~\cite{TATA}.
Theory of tree languages seems more mature than theory of general SPPF manipulations and relations between tree languages and SPPF investigation may get interesting results. For example, relations between tree languages and ECFG discussed here~\cite{TreeLangAndECFG} but mainly in context of XML manipulation.


\bibliographystyle{abbrv}
\bibliography{bibliography}
\input{appendix}
\end{document}
