\documentclass[runningheads,a4paper]{llncs}

\usepackage{mathtools}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[caption=false]{subfig}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.9}

\usepackage{url}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}


%\captionsetup[figure]{width=.85\textwidth}
%\captionsetup[subfigure]{width=.45\textwidth}
  
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

%To economize paper
%\textwidth=190mm
%\textheight=250mm
%\topmargin=-20mm
%\oddsidemargin=-15mm
%\evensidemargin=-15mm


\begin{document}

\algnewcommand\algorithmicswitch{\textbf{switch}}
\algnewcommand\algorithmiccase{\textbf{case}}
\algnewcommand\algorithmicassert{\texttt{assert}}
\algnewcommand\Assert[1]{\State \algorithmicassert(#1)}
% New "environments"
\algdef{SE}[SWITCH]{Switch}{EndSwitch}[1]{\algorithmicswitch\ #1\ \algorithmicdo}{\algorithmicend\ \algorithmicswitch}
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1}{\algorithmicend\ \algorithmiccase}

\algtext*{EndSwitch}
\algtext*{EndCase}
\algtext*{EndWhile}% Remove "end while" text
\algtext*{EndIf}% Remove "end if" text
\algtext*{EndFor}% Remove "end for" text
\algtext*{EndFunction}% Remove "end function" text

\newtheorem{mydef}{Definition}

\mainmatter  % start of an individual contribution


\title{Extended Context-Free Grammars Parsing with Generalized LL}

\titlerunning{ECFG parsing with GLL}

\author{Artem Gorokhov \and Semyon Grigorev}
\authorrunning{Artem Gorokhov, Semyon Grigorev}


\institute{ Saint Petersburg State University\\
            7/9 Universitetskaya nab.\\
            St. Petersburg, 199034 Russia\\
\email{\path|gorohov.art@gmail.com| }
\\
\email{\path|semen.grigorev@jetbrains.com|}
}


\toctitle{Extended Context-Free Grammars Parsing with Generalized LL}
\tocauthor{Artem Gorokhov}
\maketitle

%\tableofcontents

%Authors are invited to submit full papers (not exceeding 12 pages) or short papers (up to 6 pages)

\begin{abstract}
Parsing plays an important role in static program analysis: during this step a structural representation of code is created upon which further analysis is performed. 
Parser generator tools, being provided with syntax specification, automate parser development. 
Language documentation often acts as such specification. 
Documentation usually takes form of ambiguous grammar in Extended Backus-Naur Form which most parser generators fail to process. 
Automatic grammar transformation generally leads to parsing performance decrease. 
Some approaches support EBNF grammars natively, but they all fail to handle ambiguous grammars. 
On the other hand, Generalized LL parsing algorithm admits arbitrary context-free grammars and achieves good performance, but cannot handle EBNF grammars. 
The main contribution of this paper is a modification of GLL algorithm which can process grammars in a form which is closely related to EBNF (Extended Context-Free Grammar). 
We also show that the modification improves parsing performance as compared to grammar transformation-based approach. 

\keywords{Parsing, Generalized Parsing, Extended Context-Free Grammar, GLL, SPPF, EBNF, ECFG, RRPG, Recursive Automata}
\end{abstract}


\section{Introduction}%--------------------------------------------------------------------------------------------------------------------------------------------

Static program analysis is usually performed over a structural representation of code and parsing is a classical way to get such representation.
Parser generators are often used to automate parser creation: these tools derive a parser from a grammar.

Extended Backus-Naur Form~\cite{EBNFISO} is a metasyntax for expressing context-free grammars. 
In addition to the Backus-Naur Form syntax it uses the following constructions: alternation $\mid$, optional symbols [\dots], repetition \{\dots\}, and grouping (\dots).

This form is widely used for grammar specification in technical documentation because expressive power of EBNF makes syntax specification more compact and human-readable. 
Because documentation is one of the main sources of data for parsers developers, it would be helpful to have a parser generator which supports grammar specification in EBNF.
Note that EBNF is a standardized notation for \textit{extended context-free grammars}~\cite{ECFG} which can be defined as follows.

\begin{mydef}
An \textbf{extended context-free grammar} (ECFG)~\cite{ECFG} is a tuple ($N$, $\Sigma$, $P$, $S$), where $N$
and $\Sigma$ are finite sets of nonterminals and terminals respectively, $ S\in N$ is the start symbol,
and $P$ (productions) is a map from $N$ to regular expressions over alphabet $N \cup \Sigma$.
\end{mydef}

ECFG is widely used as an input format for parser generators, but classical parsing algorithms often require CFG, and, as a result, parser generators usually require conversion to CFG.
It is possible to transform ECFG to CFG~\cite{ELL}, but this transformation leads to grammar size 
increase and change in grammar structure: new nonterminals are added during transformation.
As a result, parser constructs derivation tree with respect to the transformed grammar, making it harder for a language developer to debug grammar and use parsing result later.

There is a wide range of parsing techniques and algorithms~\cite{AttributedELL,ELRR,ECFGparsing,ELLParser,ELL,ECFG,ELALR,ELRParsing} which are able to process grammar in ECFG.
Detailed review of results and problems in ECFG processing area is provided in the paper ``Towards a Taxonomy for ECFG and RRPG Parsing''~\cite{ECFG}. 
We only note that most of algorithms are based on classical LL~\cite{ELLParser,AttributedELL,PredictiveECFG} and
LR~\cite{ELRParsing,ELALR,ELRR} techniques, but they admit only restricted subclasses of ECFG.
Thus, there is no solution for handling arbitrary (including ambiguous) ECFGs.

The LL-based parsing algorithms are more intuitive than LR-based and can provide better error diagnostic.
Currently LL(1) seems to be the most practical algorithm.
Unfortunately, some languages are not LL(k) for any $k$, and left recursive grammars are a problem for LL-based tools.
Another restriction for LL parsers is ambiguities in grammar which, being combined with previous flaws, complicates industrial parsers creation.
Generalized LL, proposed in~\cite{scott2010gll}, solves all these problems: it handles arbitrary CFGs, including ambiguous and left recursive.  
Worst-case time and space complexity of GLL is cubic in terms of input size and, for LL(1) grammars, it demonstrates linear time and space complexity.

In order to improve performance of GLL algorithm, modification for left factorized grammars processing was introduced in~\cite{scott2016structuring}.
Factorization transforms grammar so that there are no two productions with same prefixes (see fig.~\ref{fig:ExampleOfFactorization} for example).
It is shown, that factorization can reduce memory usage and increase performance by reusing common parts of rules for one nonterminal.
Similar idea can be applied to ECFGs processing.

To summarize, possibility of handling ECFG specification with tools based on generalized parsing algorithm would greatly simplify language development.
In this work we present a modification of generalized LL parsing algorithm which handles arbitrary ECFGs without transformation to CFG.
We also demonstrate that on some grammars proposed modifications improve parsing performance and memory usage comparing to GLL for factorized grammar.

\section{ECFG Handling with Generalized LL Algorithm}%--------------------------------------------------------------------------------------------------------------------------------------------

The purpose of generalized parsing algorithms is to provide arbitrary context-free grammars handling.
Generalized LL algorithm~(GLL)~\cite{scott2010gll} inherits properties of classical LL algorithms: it is more intuitive and provides better syntax error diagnostic than generalized LR algorithms.
Also, our experience shows that GLR-based solutions are more complex than GLL-based, which agrees with the observation in~\cite{ECFG} that LR-based ECFG parsers are very complex. 
Thus, we choose GLL as a base for our solution.
In this section we present GLL-style parser for arbitrary ECFG processing.

\subsection{Generalized LL Parsing Algorithm}%--------------------------------------------------------------------------------------------------------------------------------------------

An idea of the GLL algorithm is based on handling of descriptors which can uniquely define state of parsing process.
Descriptor is a four-element tuple $(L, i, T, S)$ where: 
\begin{itemize}
\item $L$ is a grammar slot --- pointer to a position in the grammar of the form~$(S \to \alpha \cdot \beta)$;
\item $i$ --- position in the input;
\item $T$ --- already built root of parse forest;
\item $S$ --- current Graph Structured Stack~(GSS)~\cite{afroozeh2015faster} node.
\end{itemize}

GLL moves through the grammar and the input simultaneously, creating multiple descriptors in case of ambiguity, and using queue to control descriptors processing.
In the initial state there is only one descriptor which consists of start position in grammar
($L = (S \to \cdot \beta)$), input ($i=0$), dummy tree node $ (\$) $, and the bottom of GSS.
At each step, the algorithm dequeues a descriptor and acts depending on the grammar and the input.
If there is an ambiguity, then algorithm enqueues descriptors for all possible cases to process them later. 
To achieve cubic time complexity, it is important to enqueue only descriptors which have not been created before.
Global storage of all created descriptors is used to decide whether or not a descriptor should be enqueued.

There is a table based approach~\cite{ragozina} for GLL implementation which generates only tables for given grammar instead of full parser code.
The idea is similar to the one in the original paper and uses the same tree construction and stack processing routines.
Pseudocode illustrating this approach can be found in the appendix~\ref{GLLCode}.
Note that we do not include check for first/follow sets in this paper.


\subsection{Grammar Factorization}%--------------------------------------------------------------------------------------------------------------------------------------------

In order to improve performance of GLL, Elizabeth Scott and Adrian Johnstone proposed a support for left-factorized grammars in this parsing algorithm~\cite{scott2016structuring}. 

It is obvious from GLL description that to decrease parse time and the amount of required memory, it is sufficient to reduce the number descriptors to process.
One of the ways to do it is to reduce the number of grammar slots, and it can be done by grammar factorization.
An example of factorization is provided in fig.~\ref{fig:ExampleOfFactorization}: grammar $G_0$ transforms to $G_0'$ during factorization.
This example is discussed in the paper~\cite{scott2016structuring}, and it is shown, that, by producing less slots, such transformation can improve performance significantly for some grammars.

\begin{figure}
    \centering
    \subfloat[Original grammar $G_0$]{
        $
        \begin{array}{rl}
        S::= a\ a\ B\ c\ d \ | \ a\ a\ c\ d \ | \ a\ a\ c\ e |\ a\ a
        \end{array}
        $
    }
    ~
    \subfloat[Factorized grammar $G_0'$]{
        $
        \begin{array}{rl}
        S::= a\ a\ ( B\ c\ d\ |\ c\ ( d\ |\ e )\ |\ \varepsilon \ )
        \end{array}
        $
    }
    \caption{Example of grammar factorization}
    \label{fig:ExampleOfFactorization}
\end{figure}

We can evolve this idea to support ECFG, and we will show how to do it in the next section.

\subsection{Recursive Automata and ECFGs}

In order to ease adoption of ideas of grammar factorization for handling ECFGs with GLL we use recursive automaton (RA)~\cite{tellier2006learning} for ECFG representation.
We use the following definition of RA.

\begin{mydef}
    Recursive automaton~(RA) $R$ is a tuple $(\Sigma, Q, S, F, \delta)$, where $\Sigma$ is a finite set of terminals, $Q$ --- finite set of states, $S \in Q$ --- start state, $F \subseteq Q$ --- set of final states, $\delta : Q \times (\Sigma \cup Q) \to Q$ --- transition function.
\end{mydef}

The only difference between Recursive Automaton and Finite State Automaton (FSA) is that transitions in RA are labeled either by terminal ($\Sigma$) or by state ($Q$).
Further in this paper, we call transitions by elements from $Q$ \textit{nonterminal transitions} and by terminal --- \textit{terminal transitions}.

Note that grammar factorization leads to partial minimization of automata in the right-hand sides of productions.
Also note that grammar slots are equivalent to states of automata which are built from right-hand sides of productions.
Right-hand sides of ECFG productions are regular expressions over the union alphabet of terminals and nonterminals.
So, our goal is to build RA with minimal number of states for given ECFG, which can be done by the following steps.
\begin{enumerate} 
\item Build an FSA using Thompson's method for each right-hand side of productions~\cite{Thompson:1968:PTR:363347.363387}.
\item Create a map $M$ from each nonterminal to a corresponded start state.
This map should be kept consistent during all of the following steps. 
\item Convert FSAs from previous step to a deterministic FSAs without $\varepsilon$-transitions using the algorithm described in~\cite{aho1974design}.
\item Minimize DFSAs, for example, by using John Hopcroft's algorithm~\cite{hopcroft1971n}.
\item Replace transitions by nonterminals with transitions labeled by start states using map $M$.
Result of this step is a required RA. We also use map $M$ to define function $\Delta : Q \to N$ where $N$ is nonterminal name.
\end{enumerate}

An example of ECFG to RA transformation is presented in fig.~\ref{fig:fig1}, where state 0 is the start state of resulting RA.

\begin{figure}
    \centering
    \subfloat[Grammar $G_1$]{
        $
        \begin{array}[b]{rl}
            S ::= a^{+} S\ b? \ | \ c \ \ \ 
        \end{array}
        $
        \label{fig:grammarG0}
    }
    ~
    \subfloat[FSA for $G_1$]{
        \includegraphics[scale=.5]{pictures/G0initialAutomaton.pdf}
        \label{fig:initialAutomatonsForG0}
    }
    ~
    \subfloat[RA $R_1$ for $G_1$]{
        \includegraphics[scale=.5]{pictures/G0minimizedAutomaton.pdf}
        \label{fig:RAForG0}
    }
    \caption{Grammar to RA transformation}
    \label{fig:fig1}
\end{figure}


\subsection{Parse Forest Construction}

Result of the parsing process is a structural representation of the input --- a derivation tree or parse forest in case of multiple derivations.

First, we should define derivation tree for recursive automaton: it is an ordered tree whose root is labeled with the start state,
leaf nodes are labeled with terminals or $\varepsilon$, and interior nodes are labeled with  nonterminals $N$ and their children form a sequence of transition labels of a path in the
automaton which starts from the state $q_i$, where $ \Delta(q_i) = N $.

\begin{mydef}

Derivation tree of sentence $\alpha$ for the recursive automaton $R=(\Sigma, Q, S, F, \delta)$ is an ordered rooted tree with the following properties:

\begin{itemize}
    \item Root is labeled with $\Delta(S)$;
    \item Leaves are terminals $a\in (\Sigma \cup \varepsilon)$;
    \item Nodes are nonterminals $A\in \Delta(Q)$;
    \item Node with label $N_i = \Delta(q_i)$ has:
    \begin{itemize}
        \item 
            children nodes $l_0 \dots l_n (l_i \in \Sigma \cup \Delta(Q))$ iff there exists a
            path $p$ in $R$, $p = q_i \xrightarrow[]{l_0} q_{i+1} \xrightarrow[]{l_1} \dots \xrightarrow{l_n} q_m$, where
            $q_m \in F$, $l_i = 
            \left\{
            \begin{matrix}
            k_i, \text{ if }  k_i \in \Sigma,\\
            \Delta(k_i), \text{ if } k_i \in Q,
            \end{matrix}
            \right.
            $
        \item only one child node labeled with $\varepsilon$ iff $ q_i \in F $
    \end{itemize}
\end{itemize}
\end{mydef}

For arbitrary grammars, RA can be ambiguous in terms of accepted paths, and, as a result, it is possible to get multiple derivation trees for one input string.
Shared Packed Parse Forest (SPPF)~\cite{SPPF} can be used as a compact representation of all possible derivation trees.
We use the binarized version of SPPF, which is proposed in~\cite{brnglr}, in order to decrease memory consumption and achieve cubic worst-case time and space complexity.
Binarized SPPF can be used in GLL~\cite{scott2013gll} and contains the following types of nodes (here $i$ and $j$ are the start and the end of derived substring in terms of positions in the input string):

\begin{itemize}
\item Packed nodes are of the form $(S, k)$, where $S$ is a state of automaton, k --- start of derived substring of right child.
Packed node necessarily has a right child node --- symbol node, and optional left child node --- symbol or intermediate node.
\item Symbol nodes have labels $(X, i, j)$ where $X \in \Sigma \cup \Delta(Q) \cup \{\varepsilon\}$.
Terminal symbol nodes ($X \in \Sigma \cup \{\varepsilon\}$) are leaves. 
Nonterminal nodes ($X \in \Delta(Q)$) may have several packed children nodes. 
\item Intermediate nodes have labels $ (S, i, j) $, where $S$ is a state of automaton, and may have several packed children nodes. 
\end{itemize}

Let us describe modifications of original SPPF construction functions.
The function \textbf{getNodeT$(x,i)$} which creates terminal nodes is reused without any modifications from basic algorithm.
To handle nondeterminism in states, we define function \textbf{getNodes} which checks if the next state of RA
is final and in that case constructs nonterminal node in addition to the intermediate one.
It uses modified function \textbf{getNodeP}: instead of grammar slot, it takes as inputs separately a 
state of RA and symbol for new SPPF node: current nonterminal or the next RA state.

\input{getNodes.tex}
\input{getNodeP.tex}

Let us demonstrate an SPPF example for ECFG grammar $G_1$~(fig.~\ref{fig:grammarG0}).
This grammar contains constructions (option symbol and repetition) that should be converted
with use of extra nonterminals to build basic GLL parser.
Our generator constructs recursive automaton $R_1$~(fig.~\ref{fig:RAForG0}) and parser for it.
Possible trees for input $aacb$ \ are shown in fig.~\ref{fig:treesForG0}.
SPPF built by parser~(fig.~\ref{fig:SPPFForG0}) combines all of them.

\begin{figure}[ht]   
    \centering
    \subfloat[Possible derivation trees for $R_1$ and input $ aacb $]{
        \includegraphics[scale=.5]{pictures/G0trees.pdf}
        \label{fig:treesForG0}
    }
    ~
    \subfloat[SPPF for $R_1$ and input $ aacb $]{
        \includegraphics[scale=.5]{pictures/G0SPPFwithPackedNodes.pdf}
        \label{fig:SPPFForG0}
    }
    \caption{Example for input $ aacb $}
    \label{fig:fig2}
\end{figure}

\subsection{Input Processing}%--------------------------------------------------------------------------------------------------------------------------------------------

In this section we describe changes in control functions of basic GLL algorithm required to handle ECFG.
Main loop is similar to basic GLL one: at each step the main function \textbf{parse} dequeues from $R$ descriptor to be processed.
Suppose that current descriptor is a tuple ($C_S, C_U, i, C_N$), where $C_S$ --- state of RA,
$C_U$ --- GSS node, $i$ --- position in the input string $\omega$, and $C_N$ --- SPPF node. 
The following nonexclusive cases may arise during this descriptor processing.

\begin{itemize} 
    \item \textbf{$C_S$ is a final state.} The only case here is that $C_S$ --- start state of current nonterminal. 
    We should build nonterminal node with child $(\varepsilon, i, i)$ and perform pop action
    (call \textbf{pop} function), because this case indicates that processing of nonterminal is finished.
    
    \item \textbf{There is a terminal transition $C_S \xrightarrow[]{\omega.[i]} q$.} 
    First, build terminal node $ t = (\omega.[i], i, i+1) $ and then call \textbf{getNodes} function
    to build parents for $ C_N $ and $ t $, result is a tuple $ (y, N) $, where $N$ is optional nonterminal node.
    Create descriptor $ (q, C_U, i+1, x) $ and enqueue it regardless of whether it has been created before.
    If $ N $ is not dummy, perform pop action for this node, state $ q $ and position $i + 1 $.
    
    \item\textbf{ There are nonterminal transitions from $C_S$.}
    It means that we should start processing of new nonterminal, thus new GSS nodes should be created.
    Call \textbf{create} function for each such transition to do it.
    It performs necessary operations with GSS and checks if there exists SPPF node for current input position and nonterminal.
\end{itemize}

All required are functions presented below.
Function \textbf{add} queues descriptor, if it has not already been created; this function has not been changed.
\input{create.tex}

\input{pop.tex}

%\textbf{Pop} function is called when we reach final state. It queues descriptors for all outgoing edges from current GSS node.

\input{parse.tex}

\section{Evaluation}

We have compared our parsers built on factorized grammar and on minimized recursive automaton.
Grammar $G_2$~(fig.~\ref{fig:grammarG1}) was used for the tests, it has long tails in alternatives which cannot be unified with factorization.
FSA built for this grammar is presented in fig.~\ref{fig:automatonForG1}.

\begin{figure}[ht]   
    \centering
    \subfloat[Grammar $G_2$]{
        $
        \begin{array}{rl}
        S ::=& K\ (K\ K\ K\ K\ K \ |\ a\ K\ K\ K\ K) \\
        K ::=& S\ K\ |\ a\ K\ |\ a \\
        \end{array}
        $
        \label{fig:grammarG1}
    }

    \subfloat[RA for grammar $G_2$]{
        \includegraphics[scale=.5]{pictures/G1automaton.pdf}
        \label{fig:automatonForG1}
    }
    \caption{Grammar $G_2$ and RA for it}
\end{figure}

For this grammar parser for RA creates less GSS edges because tails of alternatives in productions
are represented by only one path in RA. This fact leads to decrease of SPPF nodes and descriptors.

Experiments were performed on inputs of different length and are presented in fig.~\ref{expPlots}.
Exact values for the input $a^{40}$ are shown in the table~\ref{expTable}.

All tests were run on a PC with the following characteristics:
\begin{itemize}
    \item OS: Microsoft Windows 10 Pro x64
    \item CPU: Intel(R) Core(TM) i7-4790 CPU @ 3.60GHz, 3601 Mhz, 4 Cores, 4 Logical Processors
    \item RAM: 32 GB
\end{itemize}

\begin{table}[ht]   
\begin{center}
    \begin{tabular}{ | c | c | c | c | c | c | }
        \hline
                           & Time, s & Descriptors & GSS Edges & GSS Nodes & SPPF Nodes   \\ \hline
        Factorized grammar & 81.814  & 7940        & 6974      & 80        & 111127244  \\ \hline
        Minimized RA       & 54.637  & 5830        & 4234      & 80        & 74292078  \\ \hline
    \end{tabular}
\end{center}
\caption{Experiment results for input $a^{40}$}
\label{expTable}
\end{table}

\input{performancePlots.tex}

Results of performed experiments support the fact that on some grammars our approach shows better results than parsers built on factorized grammars.
With grammar $G_2$ in general minimized RA version works $33\%$ faster, uses $27\%$ less descriptors, $29\%$ less GSS edges and creates $33\%$ less SPPF nodes.

\section{Conclusion and Future Work}

Described algorithm and parser generator based on it are implemented in F\# programming language as a part of the YaccConstructor project.
Source code is available here:~\url{https://github.com/YaccConstructor/YaccConstructor}.

As we showed in evaluation, proposed modification not only increases performance, but also decreases memory usage. 
It is crucial for big input processing.
For example, Anastasia Ragozina in her master's thesis~\cite{ragozina} shows that GLL can be used for graph parsing.  
Some areas deal with big graphs, for example, metagenomic assemblies in bioinfomatics and social graphs.
We hope that using the proposed modification we can improve performance of graph parsing algorithm too. 
We have performed several tests that demonstrate performance increase in metagenomic analysis, but further integration with graph parsing is required.

One of the ways to specify semantic of language is attributed grammars, but it is not yet supported in the algorithm presented in this article.
There is a number of papers on subclasses of attributed ECFGs (for example~\cite{AttributedELL}), however there is still no general solution for arbitrary ECFGs.
Thus, arbitrary attributed ECFGs and semantic calculation support is a work for future.

Another question is a possibility of unification of our results with tree languages theory: our definition of derivation tree for ECFG is quite similar to unranked tree and SPPF is similar to automata for unranked trees~\cite{TATA}.
Theory of tree languages seems to be more mature than theory of SPPF manipulations in general.
Moreover some relations between tree languages and ECFG are discussed in the paper~\cite{TreeLangAndECFG}.
We hope that investigation of relation between tree languages and SPPF may produce interesting results.


\bibliographystyle{abbrv}
\bibliography{bibliography}
\input{appendix}
\end{document}
