\documentclass[runningheads,a4paper]{llncs}

\usepackage{mathtools}
\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage[caption=false]{subfig}

\usepackage{url}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{algorithmicx}
  
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

%To economize paper
%\textwidth=190mm
%\textheight=250mm
%\topmargin=-20mm
%\oddsidemargin=-15mm
%\evensidemargin=-15mm


\begin{document}

\algnewcommand\algorithmicswitch{\textbf{switch}}
\algnewcommand\algorithmiccase{\textbf{case}}
\algnewcommand\algorithmicassert{\texttt{assert}}
\algnewcommand\Assert[1]{\State \algorithmicassert(#1)}
% New "environments"
\algdef{SE}[SWITCH]{Switch}{EndSwitch}[1]{\algorithmicswitch\ #1\ \algorithmicdo}{\algorithmicend\ \algorithmicswitch}
\algdef{SE}[CASE]{Case}{EndCase}[1]{\algorithmiccase\ #1}{\algorithmicend\ \algorithmiccase}

\algtext*{EndSwitch}
\algtext*{EndCase}
\algtext*{EndWhile}% Remove "end while" text
\algtext*{EndIf}% Remove "end if" text
\algtext*{EndFor}% Remove "end for" text
\algtext*{EndFunction}% Remove "end function" text

\newtheorem{mydef}{Definition}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Extended Context-Free Grammars Parsing with Generalized LL}

% a short form should be given in case it is too long for the running head
\titlerunning{ECFG parsing with GLL}

\author{Artem Gorokhov \and Semyon Grigorev}
\authorrunning{Artem Gorokhov, Semyon Grigorev}
% (feature abused for this document to repeat the title also on left hand pages)

\institute{ Saint Petersburg State University\\
            7/9 Universitetskaya nab.\\
            St. Petersburg, 199034 Russia\\
\email{\path|gorohov.art@gmail.com| }
\\
\email{\path|semen.grigorev@jetbrains.com|}
}


\toctitle{Extended Context-Free Grammars Parsing with Generalized LL}
\tocauthor{Artem Gorokhov}
\maketitle

%\tableofcontents

%Authors are invited to submit full papers (not exceeding 12 pages) or short papers (up to 6 pages)

\begin{abstract}
Parsing plays an important role in static program analysis: during this step a structural representation of code is created upon which further analysis is performed. 
Parser generator tools, being provided with syntax specification, automate parser development. 
Language documentation often acts as such specification. 
Documentation usually takes form of ambiguous grammar in Extended Backus-Naur Form which most parser generators fail to process. 
Automatic grammar transformation generally leads to parsing performance decrease. 
Some approaches support EBNF grammars natively, but they all fail to handle ambiguous grammars. 
On the other hand, Generalized LL parsing algorithm admits arbitrary context-free grammars and achieves good performance, but cannot handle EBNF grammars. 
The main contribution of this paper is a modification of GLL algorithm which can process grammars in a form which is closely related to EBNF (Extended Context-Free Grammar). 
We also show that the modification improves parsing performance as compared to grammar transformation based approach. 

\keywords{Parsing, Generalized Parsing, Extended Context-Free Grammar, GLL, SPPF, EBNF, ECFG, RRPG, Recursive Automata}
\end{abstract}


\section{Introduction}%--------------------------------------------------------------------------------------------------------------------------------------------

Static program analysis is usually performed over a structural representation of code and parsing is a classical way to get such representation.
Parser generators are often used to automata parser creation: these tools allow to derive parser from grammar.
It decreases amount of efforts required for syntax analyzer creation and maintenance.

Extended Backus-Naur Form~\cite{EBNFISO} is a syntax of expressing context-free grammars. 
In addition to the Backus-Naur Form syntax it uses the following constructions: alternation $\mid$, option [ ... ], repetition \{ ... \}, and grouping ( ... ).

This form is widely used for grammar specification in technical documentation because it allows to make description of language syntax more expressive and compact. 
Thus, it is necessary to have a parser generator which supports grammar in EBNF because documentation is one of main source of information for parsers developers.
Note, that EBNF is a standardized notation for \textit{extended context-free grammars} which can be defined as follows.

\begin{mydef}
An \textbf{extended context-free grammar} (ECFG)~\cite{ECFG} is a tuple ($N$, $\Sigma$, $P$, $S$), where $N$
and $\Sigma$ are finite sets of nonterminals and terminals, $ S\in N$ is the start symbol,
and $P$ (the productions) is a map from $N$ to regular expressions over alphabet $N \cup \Sigma$.
\end{mydef}

ECFG is widely used as input formaf for parser generators, but classical parsing algorithms requires CFG, and as a result, parser generators requires conversion to CFG.
It is possible to transform ECFG to CFG~\cite{ELL}, but this transformation leads to grammar size 
increase and change in grammar structure: new nonterminals addition is required during transformation.
As a result, parsing performs not in terms of user defined grammar.
This fact leads to the following problem: parser build structural representation not with respect to the 
original grammar but with respect to transformed.
As a result, derivation tree may differ from expected.

There are algorithms for parsing ECFG without transformations, based on different classical algorithms: 
ELL(k) and ELR(k)~\cite{ELL} parsers, Early-style parsers~\cite{!!!}.
Some of them point out a problem with parsing conflicts~\cite{}, and none of them work with arbitrary ECFG.
Detailed overview is provided 
In order to provide ability to process grammar in ELL, ELR~\cite{AttributedELL,ELRR,ECFGparsing,ELLParser,ELL,ECFG,ELALR,ELRParsing} and other can process EBNF but they do not deal with ambiguities in grammars.



There is a wide range of parsing techniques and algorithms (CYK, LR(k), LALR(k), LL, etc) and parser generation tools, which based on it. 
The LL family is more intuitive than LR and can provide better error diagnostic.
Thus, LL(1) is most practical algorithm, but it is not powerful enough: LL(k) for any $k$ is not enough to process some languages because there are LR, but not LL languages.
Also left and hidden left recursion in grammars is a problem for LL-based parsers.
Moreover handling of arbitrary ambiguous grammars is a also problem for LL-based tools.
All these facts restrict class of grammars which can be handled, which make parser creation difficult. 
In order to solve these problems generalized LL (GLL)~\cite{scott2010gll} was proposed~\cite{scott2010gll}. 
This algorithm handles arbitrary context free grammar, even unambiguous and (hidden)left-recursive.
Worst-case time and space complexity of GLL is cubic in terms of input size and for LL(1) grammars it demonstrates linear time and space complexity.

In order to improve performance of GLL algorithm, modification for left factorized grammars processing was introduced in~\cite{scott2016structuring}.
Factorization means that there are no two productions for one nonterminal with equal prefixes (look at fig~\ref{fig:ExampleOfFactorization} for example).
Shown, that factorization can reduce memory usage and increase performance which achieved by reusing common parts of rules for one nonterminal.
Purposed idea can be used for processing grammars in EBNF with exception of same effects.

To summarise, it is possible to simplify language description required for parser generation in case a parser generator is based on generalized algorithm which can handle grammars in ECFG.
Generalized parsing algorithms can handle arbitrary grammars and these demonstarte good performance.
In this work we present modified generalized LL parsing algorithm which handles arbitrary ECFGs without transformations.
We show that changes of basic algorithm are very native for GLL nature. 
Also we demonstrate that proposed modifications allow to get parsing performance and memory usage improvement.


\section{Generalized LL Parsing Algorithm}%--------------------------------------------------------------------------------------------------------------------------------------------

Generalized parsing algorithms~(GLL and GLR) was purposed to perform syntax analysis by arbitrary context-free 
grammar. Unlike the GLR, GLL algorithm~\cite{scott2010gll} is rather intuitive and allows to perform better syntax error diagnostic.
As an output of GLL we get Shared Packed Parse Forest~(SPPF)~\cite{scott2013gll} that represents all possible derivations of input string.

Work of the GLL algorithm based on descriptors, it allows to handle all posible derivations.
Descriptor is a four-element tuple $(L, i, T, S)$ that can uniquely define state of parsing process. 
$L$ is a grammar slot --- pointer to position in grammar of the form~$(S \to \alpha \cdot \beta)$, $i$ --- position in input,
$T$ --- already built SPPF root, $S$ --- current Graph Structured Stack~(GSS)~\cite{GSS} node.
%We used efficient GSS described in~\cite{afroozeh2015faster}.

In initial state we have descriptors that describe start positions in grammar and input, dummy tree node and bottom of GSS.
On each step algorithm processes first descriptor in queue and makes actions depending on the grammar and input.
If there are any ambiguity algorithm will queue descriptor for all cases to handle them all. 

There are table based approach~\cite{ragozina} which allows to generate only tables for given grammar instead of full parser code.
The idea is similar to one in original article and main function uses same tree construction and stack processing functions.
Pseudo code can be found in appendix~\ref{GLLCode}. Note that we do not include the check for first/follow sets in this paper.




\section{Extended CFG GLL Parsing}%--------------------------------------------------------------------------------------------------------------------------------------------

In this section we will show an application of ECFG in automatons and corresponding GLL-style parsers.

\subsection{Factorization}%--------------------------------------------------------------------------------------------------------------------------------------------

In order to improve performance Elizabeth Scott and Adrian Johnstone offered support of factorised grammars in GLL~\cite{scott2016structuring}. 
The idea is to automatically factorize grammars and use them for parser generation. 

The algorithm creates and queues new descriptors depending on current parse state that we get from unqueued descriptor. 
In case descriptor has been already created it does not add it to queue. For this purpose we have a set of
\textbf{all} created descriptors. Thus reducing a number of possible descriptors decreases the parse time
and required memory.

Factorization decreases the number of grammar slots. 
Consider example from the paper~\cite{scott2016structuring} on fig.~\ref{fig:ExampleOfFactorization}.

\begin{figure}
    \centering
    \subfloat[Production $P_0$]{
        $
        \begin{array}{crcl}
        S::&=& a& a\ B\ c\ d \\
        &|& a& a\ c\ d \\
        &|& a& a\ c\ e \\
        &|& a& a
        \end{array}
        $
    }
    ~
    \subfloat[Production $P_0\prime$]{
        $S::= a\ a\ ( B\ c\ d\ |\ c\ ( d\ |\ e )\ |\ \varepsilon)$
    }
    \caption{Example of factorization}
    \label{fig:ExampleOfFactorization}
\end{figure}

Production $P_0$ factorises to $P_0'$.
Second is much compact and contains much less possible slots, 
so parser creates less descriptors.
It gives significant performance improvement on some grammars.

This idea can also be extended to full ECFG support.
Let us show how to do it.

\subsection{Recursive automata}
The idea of factorisation was evolved to use of automatons and their minimization.

ECFG can be converted to recursive automata~\cite{tellier2006learning}.

\begin{mydef}
    Recursive automaton(RA) $R$ is a tuple $(\Sigma, Q, S, F, \delta)$, where
    $\Sigma$ is a set of terminals,
    $Q$ --- set of states of $R$,
    $S \in Q$ --- start state,
    $F \in Q$ --- set of final states,
    $\delta : Q \times (\Sigma \cup Q) \to Q$ --- transition function.
\end{mydef}

The only difference between RA and FSA is that in RA transition can be labeled either 
by terminal ($\in \Sigma$) or by state ($\in Q$). Further in this peper we will call
transitions by elements from $Q$ as nonterminal transitions and by terminal as terminal transitions.

Right parts of ECFG are regular expressions over alphabet of terminals and nonterminals.
Thus for each right-hand side of grammar productions we can build a finite state automaton 
using Thompson's method~\cite{Thompson:1968:PTR:363347.363387}. 
To transform the set of produced automata we need to eliminate $\varepsilon$-transitions and replace
transitions by nonterminals with transitions labeled by start states of corresponding to nonterminal FSA.
An example of constructed recursive automaton for grammar $\Gamma_{0}$(fig.~\ref{fig:grammarG0})
is given on fig.~\ref{fig:initialAutomatonsForG0}, state 0 is start state.

Decrease of the quantity of the automaton states decreases number of GLL descriptors, as it was with factorization.
Thus to increase performance of parsing we can minimize the number of states in produced automatons.
%without loosing the structure of initial grammar.

First, RA should be converted to deterministic RA using the algorithm for FSA described in ~\cite{aho1974design}.
Then John Hopcroft's algorithm~\cite{hopcroft1971n} can be applied to RA to minimize the number of states.
An example for grammar $G_0$ is shown on fig.~\ref{fig:minimizedAutomatonsForG0}.

%Note: this automatons are not an abstract machines, but just a ``convenient'' representations of grammars.
Note: later we will need a nonterminal names to build a SPPF, for this purpose we define function $\Delta : Q \to N$ where $N$
is nonterminal name.

\begin{figure}
    \centering
    \subfloat[Grammar $G_0$.]{
        $
        \begin{array}{crcl}
            S& ::= &(a&|B)\ a \\
            B& ::= &a& \\
        \end{array}
        $
        \label{fig:grammarG0}
    }
    ~
    \subfloat[Initial RA.]{
        \includegraphics[scale=.48]{pictures/automatonForG0.pdf}
        \label{fig:initialAutomatonsForG0}
    }
    ~
    \subfloat[Minimized RA.]{
        \includegraphics[scale=.46]{pictures/minimizedAutomatonForG0.pdf}
        \label{fig:minimizedAutomatonsForG0}
    }
    \caption{Example of automatons.}
    \label{fig:fig1}
\end{figure}

%\begin{figure}
%    \centering
%    \includegraphics[width=4cm]{pictures/SPPFforG0.pdf}
%    \caption{SPPF for input "aa"}
%    \label{fig:SPPF}
%\end{figure}


\subsection{Input processing}%--------------------------------------------------------------------------------------------------------------------------------------------
An GLL idea is to move through grammar and input simultaneously,
creating multiple descriptors for the case of ambiguity.

Just as we can move through grammar slots we can move through states 
of automaton. Grammar slot in descriptor changes to state in RA.
The problem is that in automaton we have nondeterministic choice because there can be 
many transitions to other states. Consider such significant cases:
\begin{itemize} 
\item there are transition by current input terminal to final state
\item there are transition by current input terminal to state that is not final
\item there are nonterminal transition
\end{itemize}

All of them should be handled and this leads to nondeterminism. 
For the last case we just can call create function for each state.
But for the terminal cases we need to add descriptor that describes
next position to queue without checking it's existence in descriptor elimination set.
Thus we use descriptors queue to handle nondeterminism in states, while original algorithm uses it to 
handle ambiguity in grammars.
%So we need to prevent creation of descriptors for each nonterminal on outgoing edges. We can generate tables that 
%tells us what nonterminals can infer strings that starts with current terminal. And add descriptors only for 
%this edges. Moreover we need to create descriptor for edge that marked with current terminal if such exists.

%In states of parsing we can have a nondeterministic choice because the states of automaton can be final states and outgoing edges can contain nonterminals.
%If there exist outgoing edge that contains current terminal we need to create intermediate node.
%But if the next state is final state we also need to create nonterminal node and call \textbf{pop} function.
%Moreover we need to call \textbf{create} function for edges that contains nonterminal.
%To handle nondeterminism \textbf{parse} function queues new descriptors for all described cases.

\input{add.tex}
Function \textbf{add} queues descriptor if it was not already created.
\input{create.tex}

Function \textbf{create} is called when we meet nonterminal transition.
It performs necessary operations with GSS and checks if there are already built SPPF for current
input position and nonterminal.
\input{pop.tex}
\textbf{Pop} function is called when we reach final state. It queues descriptors for all outgoing edges from current GSS node.

\input{parse.tex}

The main function \textbf{parse} handles queued descriptor and checks all transitions from current state to be appropriate
for current input terminal, or calls create function when meets nonterminal transitions.


\subsection{Parse forest construction}

Result of the parsing process is structural representation of input --- tree, or parse forest for the case of many derivation variants.

First, we should define derivation tree for recursive automatons: it is an ordered tree whose root labeled with start state,
leaf nodes are labeled with a terminals or $\varepsilon$ and interior nodes are labeled with 
nonterminals $A$ and have a sequence of children that corresponds to transition labels of path in 
automaton that starts from the state $\Delta(A)$. More formal definition provided below. 

\begin{mydef}

Derivation tree of sentence $\alpha$ for the recursive automaton $R=(\Sigma, Q, S, F, \delta)$:%grammar $G=(\Sigma, N, S, P)$:

\begin{itemize}
\item Ordered rooted tree. Root labeled with $\Delta(S)$
\item Leaves are terminals $\in \Sigma$
\item Nodes are nonterminals $\in \Delta(Q)$
\item Node with label $N_i \in \Delta(q_i)$ has children $l_0 \dots l_n (l_i \in \Sigma \cup \Delta(Q))$ iff exists
path
$q_i \xrightarrow[]{l_0} \dots \xrightarrow{l_n} q_m$, $q_m \in F$. 
%for $\omega = l_0 \cdot l_1 \dots\ l_n\in (\Sigma \cup N)^*$ exists $p \rightarrow M \in P$ such that $\omega \in L(M)$
\end{itemize}

\end{mydef}

RA is ambiguous if there exist string that have more than one derivation trees. 
We work with arbitrary grammars, thus our RA can be ambiguous and we can define Shared Packed parse Forest (SPPF)~\cite{SPPF} that can represent all possible derivation trees.
It is similar to SPPF for grammars described in~\cite{scott2013gll}. SPPF contains symbol nodes, packed nodes
and intermediate nodes. 

Packed nodes are of the form $(S, k)$, where $S$ is a state of automaton. 
Symbol nodes have labels $(X, i, j)$ where $X \in \Sigma \cup \Delta(Q) \cup \varepsilon$. 
Intermediate nodes have labels $ (S, i, j) $, where $S$ is a state of automaton. $i$ is position in input before leftmost leaf terminal, $j$ --- position after rightmost leaf.

Packed node necessarily has right child --- symbol node, and optional left child --- symbol or intermediate node.
Nonterminal and intermediate nodes may have several packed children. 
Terminal symbol nodes are leaves.

Use of intermediate and packed nodes leads to binarization of SPPF and thus the space complexity is $O(n^{3})$.
%But in grammars slot defines position,
%previous and next symbol, when DFA state tells the position only. Thus we can can construct SPPF using 
%In general, we can't uniquely correspond an original grammar slot to automaton state.
%We can consider example. For the grammar \ref{fig:grammarG0}, automaton will be represented as showed on fig.\ref{fig:automatonForG0}.
%SPPF for input "aa" is on fig.\ref{fig:SPPF}.

%State $1$ can be matched with two grammar slots: $S ::= (a \cdot a)|(b$ $a)$ and $S ::= (a$ $a)|(b \cdot a)$. 
%But SPPF represents WHAT???


\textbf{function} getNodeT$(x,i)$ did not change

We defined function \textbf{getNodes} which can construct two nodes: intermediate and nonterminal (at least one of them, at most both).
It uses modified function \textbf{getNodeP} that takes additional argument: state or nonterminal name. Symbol in returned SPPF node will be this argument's value.
\input{getNodes.tex}
\input{getNodeP.tex}

\section{Evaluation(under construction)}

We have implemented parser generator for suggested algorithm 
and this section provided with SPPF example and performance comparison 
with the parsers built on factorized grammars.

\subsection{SPPF example}

For the ECFG grammar $G_1$(fig.~\ref{fig:grammarG1}) generator constructs
recursive automaton $R_1$(fig.~\ref{fig:automatonForG1}) and parser for it.
For input $ aabk $ this parser builds SPPF showed on fig.~\ref{fig:SPPFForG1}.

\begin{figure}[H]
    \centering
    \subfloat[RA $R_1$ for $G_1$.]{
        \includegraphics[scale=.65]{pictures/automatonForSPPFG1.pdf}
        \label{fig:automatonForG1}
    }
    ~
    \subfloat[SPPF for $R_1$ and input $ aabk $.]{
        \includegraphics[scale=.65]{pictures/SPPFforG1.pdf}
        \label{fig:SPPFForG1}
    }
    \caption{Example of SPPF.}
    %\label{fig:fig2}
\end{figure}

\begin{figure}[H]
    $$
    \begin{array}{crcl}
    S& ::= &(a^{*}\ b?\ k)\ |\ M \\
    M& ::= &a\ a\ b\ k \\
    \end{array}
    $$
    \caption{Grammar $G_1$.}
    \label{fig:grammarG1}
\end{figure}

\subsection{Performance measure}

We have compared our parsers built on factorized grammars and on minimized recursive automatons.
Grammar $G_2$(fig.~\ref{fig:grammarG2}) was used for the tests,
it has long tails in alternatives which is not unified with factorization.
FSA built for this grammar presented on fig.~\ref{fig:automatonForG2}.

\begin{figure}[H]
    \centering
    \subfloat[Grammar $G_2$.]{
        $
        \begin{array}{crcl}
        S ::=& K\ K\ K\ K\ K\ K \\
        &|K\ a\ K\ K\ K\ K \\
        A ::=& S\ K\ |\ a\ K\ |\ a \\
        \end{array}
        $
        \label{fig:grammarG2}
    }
    ~
    \subfloat[RA for grammar $G_2$.]{
        \includegraphics[height=0.45\textwidth]{pictures/automatonForG2.pdf}
        \label{fig:automatonForG2}
    }
    \caption{Grammar $G_2$ and RA for it.}
\end{figure}

% не очень понятно что тут объяснять...
Explanation of slots difference: for BNF, for factorized, for ECFG

Description of input. 

Short info about PC.

%\begin{tabular}{ | l | l | l | l | l | l | l | }
%    \hline
%    Length & \multicolumn{2}{ c| }{Time, seconds} & \multicolumn{2}{ c| }{Descriptors} & \multicolumn{2}{ c| }{GSS Edges} \\ \hline
%    & factorized & minimized & factorized & minimized & factorized & minimized \\ \hline
%    100 & 0.206 & 0.127 & 52790 & 38530 &  42794 & 28534 \\ \hline
%    200 & 1.909 & 1.54 & 215540 & 157030 & 175544 & 117034 \\ \hline
%    300 & 8.844 & 7.125 & 488290 & 355530 & 398294 & 265534 \\ \hline
%    400 & 25.876 & 21.707 & 871040 & 634030 & 711044 & 474034 \\ \hline
%    500 & 60.617 & 51.245 & 1363790 & 992530 & 1113794 & 742534 \\ \hline
%    1000 & 842.779 & 768.853 & 5477540 & 3985030 & 4477544 & 2985034 \\ \hline
%    & \multicolumn{2}{ c| }{Average gain: 19$\%$} & \multicolumn{2}{ c| }{Average gain: 27$\%$} & \multicolumn{2}{ c| }{Average gain: 33$\%$} \\ \hline
%\end{tabular}

\begin{table}[h]
    
\begin{center}
    \begin{tabular}{ | c | c | c | c | c |}
        \hline
                           & Time, s & Descriptors & GSS Edges & SPPF Nodes   \\ \hline
        Factorized grammar & 81.814  & 7940        & 6974      & 111127244  \\ \hline
        Minimized RA       & 54.637  & 5830        & 4234      & 74292078  \\ \hline
        %Gain               & 33$\%$  & 27$\%$      & 29$\%$    & 33$\%$  \\ \hline
    \end{tabular}
\end{center}
\caption{Experiments results.}
\label{expTable}
\end{table}

Table~\ref{expTable} shows experiments results for input $a^{40}$. Minimized RA version works $33\%$ faster, uses $27\%$ less descriptors, $29\%$ less GSS edges
and builds $33\%$ less SPPF nodes.


We also use this automaton approach in metagenomic assemblies parsing and it gives visible performance increase.

A bit more discussion on evaluation.

Examples of SPPF.

May be some nontrivial cases: s -> a* a* and so on


\section{Conclusion and Future Work}

Described algorithm and parser generator based on it implemented in F\# as part of the YaccConstructor project.
Source code available here:~\url{https://github.com/YaccConstructor/YaccConstructor}.

As we show in evaluation, proposed modification not only increase performance, but also decrease memory usage. 
It is critical for big input processing.
For example, Anastasia Ragozina in her master's thesis~\cite{ragozina} shows that GLL can be used for graph parsing.  
In some areas graphs can be really huge: metagenomic assemblies in bioinfomatics, social graphs.
We hope that proposed modification can improve performance not only in case of classical parsing, but in graph parsing too. 
We perform some tests that shows performance increasing in metagenomic analysis, but full integration with graph parsing and formal description is required.

One of way to specify any useful manipulations on derivation tree (or semantic of language) is an attributed grammars, but it is not supported in the algorithm which presented in this article.
There is number of works on subclasses of attributed ECFGs (for example~\cite{AttributedELL}), however stil no solution for arbitrary ECFGs.
Thus, arbitrary attributed ECFGs and semantic calculation support is a future work.

Yet another question is possibility of unification our results with tree languages: our definition of derivation tree for ECFG is quite similar to unranked tree and SPPF is similar to automata for unranked trees~\cite{TATA}.
Theory of tree languages seems more mature than theory of general SPPF manipulations and relations between tree languages and SPPF investigation may get interesting results.


\bibliographystyle{abbrv}
\bibliography{bibliography}
\input{appendix}
\end{document}
