%% For double-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For double-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review,anonymous]{acmart}\settopmatter{printfolios=true}
%% For single-blind review submission, w/o CCS and ACM Reference (max submission space)
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true,printccs=false,printacmref=false}
%% For single-blind review submission, w/ CCS and ACM Reference
%\documentclass[sigplan,review]{acmart}\settopmatter{printfolios=true}
%% For final camera-ready submission, w/ required CCS and ACM Reference
\documentclass[sigplan]{acmart}\settopmatter{printacmref=false, printfolios=false}


%% Conference information
%% Supplied to authors by publisher for camera-ready submission;
%% use defaults for review submission.
\copyrightyear{2020}
\acmYear{2020}
\setcopyright{rightsretained}
\acmConference[PPoPP '20]{25th ACM SIGPLAN Symposium on Principles and
Practice of Parallel Programming}{February 22--26, 2020}{San Diego, CA, USA}
\acmBooktitle{25th ACM SIGPLAN Symposium on Principles and Practice of
Parallel Programming (PPoPP '20), February 22--26, 2020, San Diego, CA, USA}
\acmDOI{10.1145/3332466.3374507}
\acmISBN{978-1-4503-6818-6/20/02}


%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
%\citestyle{acmauthoryear}  %% For author/year citations
%\citestyle{acmnumeric}     %% For numeric citations
%\setcitestyle{nosort}      %% With 'acmnumeric', to disable automatic
                            %% sorting of references within a single citation;
                            %% e.g., \cite{Smith99,Carpenter05,Baker12}
                            %% rendered as [14,5,2] rather than [2,5,14].
%\setcitesyle{nocompress}   %% With 'acmnumeric', to disable automatic
                            %% compression of sequential references within a
                            %% single citation;
                            %% e.g., \cite{Baker12,Baker14,Baker16}
                            %% rendered as [2,3,4] rather than [2-4].


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Note: Authors migrating a paper from traditional SIGPLAN
%% proceedings format to PACMPL format must update the
%% '\documentclass' and topmatter commands above; see
%% 'acmart-pacmpl-template.tex'.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
                        %% http://ctan.org/pkg/subcaption

%%
\newcommand\question[1]{{\color{violet}#1}}
\newcommand\todo[1]{{\color{red}#1}}
%%

%%code things
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\begin{document}

%% Title information
\title[POSTER: Optimizing GPU Programs by Partial Evaluation]{POSTER: Optimizing GPU Programs by Partial Evaluation}         %% [Short Title] is optional;
                                        %% when present, will be used in
                                        %% header instead of Full Title.
%\titlenote{with title note}             %% \titlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'
%\subtitle{Subtitle}                     %% \subtitle is optional
%\subtitlenote{with subtitle note}       %% \subtitlenote is optional;
                                        %% can be repeated if necessary;
                                        %% contents suppressed with 'anonymous'


%% Author information
%% Contents and number of authors suppressed with 'anonymous'.
%% Each author should be introduced by \author, followed by
%% \authornote (optional), \orcid (optional), \affiliation, and
%% \email.
%% An author may have multiple affiliations and/or emails; repeat the
%% appropriate command.
%% Many elements are not rendered, but should be provided for metadata
%% extraction tools.

%% Author with single affiliation.
% \author{Aleksey Tyurin}                                        %% can be repeated if necessary
% \orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
% \affiliation{
%   \position{Position1}
%   \department{Department1}              %% \department is recommended
%   \institution{Institution1}            %% \institution is required
%   \streetaddress{Street1 Address1}
%   \city{City1}
%   \state{State1}
%   \postcode{Post-Code1}
%   \country{Country1}                    %% \country is recommended
% }
% \email{first1.last1@inst1.edu}          %% \email is recommended

% %% Author with two affiliations and emails.
% \author{Daniil Berezun}
% %\authornote{with author2 note}          %% \authornote is optional;
%                                         %% can be repeated if necessary
% \orcid{nnnn-nnnn-nnnn-nnnn}             %% \orcid is optional
% \affiliation{
%   \position{Position2a}
%   \department{Department2a}             %% \department is recommended
%   \institution{Institution2a}           %% \institution is required
%   \streetaddress{Street2a Address2a}
%   \city{City2a}
%   \state{State2a}
%   \postcode{Post-Code2a}
%   \country{Country2a}                   %% \country is recommended
% }

% \email{first2.last2@inst2a.com}         %% \email is recommended

% \author{Semyon Grigorev}
% %\authornote{with author2 note}          %% \authornote is optional;
%                                         %% can be repeated if necessary
% \orcid{0000-0002-7966-0698}             %% \orcid is optional
% \affiliation{
%   \position{Associate Professor}
%   %\department{Department2a}             %% \department is recommended
%   \institution{Saint Petersburg State University}           %% \institution is required
%   \streetaddress{Universitetskaya nab. 7/9}
%   \city{St. Petersburg}
%   \postcode{199034}
%   \country{Russia}                   %% \country is recommended
% }
% \email{s.v.grigoriev@spbu.ru}         %% \email is recommended
% \affiliation{
%   \position{Researcher}
%   %\department{Department2b}             %% \department is recommended
%   \institution{JetBrains Research}           %% \institution is required
%   \streetaddress{Primorskiy prospekt 68-70, Building 1}
%   \city{St. Petersburg}
%   \postcode{197374}
%   \country{Russia}                   %% \country is recommended
% }
% \email{semyon.grigorev@jetbrains.com}         %% \email is recommended

\author{Aleksey Tyurin}
\affiliation{
  \institution{Saint Petersburg State University}
}
\email{alekseytyurinspb@gmail.com}

\author{Daniil Berezun}
\affiliation{%
  \institution{JetBrains Research}
}
\email{daniil.berezun@jetbrains.com}

\author{Semyon Grigorev}
\additionalaffiliation{%
  \institution{JetBrains Research}
}
\affiliation{
  \institution{Saint Petersburg State University}
}
\email{s.v.grigoriev@spbu.ru}

%% Abstract
%% Note: \begin{abstract}...\end{abstract} environment must come
%% before \maketitle command
\begin{abstract}
While GPU utilization allows one to speed up computations to the orders of magnitude, memory management remains the bottleneck making it often a challenge to achieve the desired performance. Thus, different memory optimizations are leveraged to reduce the number of memory transactions and make memory being used more effectively.
%Notably, memory optimizations are being the most significant problem: GPUs memory hierarchy implies certain limitations, thus making data memory allocation management nontrivial and memory to be utilized carefully.
%In the paper we propose an approach automating memory management utilizing partial evaluation, a program transformation technique that enables the data accesses to be precomputed, optimized, and embedded into the code, mitigating memory transactions.
In the paper we propose an approach automating memory management utilizing partial evaluation, a program transformation technique that enables the data accesses to be pre-computed, optimized, and embedded into the code, saving memory transactions.
As an empirical evaluation of our approach we applied the technique to a straightforward CUDA C na\"ive string pattern matching algorithm implementation.
Our experiments show that the transformed program is up to 8 times as efficient as the original one.
\end{abstract}

%%Default
%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
%\begin{CCSXML}
%<ccs2012>
%<concept>
%<concept_id>10011007.10011006.10011008</concept_id>
%<concept_desc>Software and its engineering~General programming languages</concept_desc>
%<concept_significance>500</concept_significance>
%</concept>
%<concept>
%<concept_id>10003456.10003457.10003521.10003525</concept_id>
%<concept_desc>Social and professional topics~History of programming languages</concept_desc>
%<concept_significance>300</concept_significance>
%</concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Software and its engineering~General programming languages}
%\ccsdesc[300]{Social and professional topics~History of programming languages}

%%Not default

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008</concept_id>
       <concept_desc>Software and its engineering~General programming languages</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011041.10011047</concept_id>
       <concept_desc>Software and its engineering~Source code generation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages}
\ccsdesc[500]{Software and its engineering~Source code generation}

%% End of generated code


%% Keywords
%% comma separated list
\keywords{GPU, CUDA, Partial Evaluation}  %% \keywords are mandatory in final camera-ready submission


%% \maketitle
%% Note: \maketitle command must come after title commands, author
%% commands, abstract environment, Computing Classification System
%% environment and commands, and keywords command.
\maketitle


\section{Introduction}

%Performance of GPU-based solutions critically depends on data allocation and memory management, since most applications tend to be bandwidth bound problem requiring the data to be accessed with minimal latency.
%Thus memory optimizations appear to be in a prevailing significance and addressed in a huge amount of research~\cite{10.1007/978-3-319-74313-4_27, Xie2018ICCADU, zhang2019efficient}.
%GPUs memory access latency varies between different memory types, from hundreds of cycles for global memory to just a few for shared and register memory.
%The latency could be aggravated by wrong access patterns, while each type of memory needs a specific pattern.
%Such access patterns could be not satisfied due to thread divergence or the domain of the problem being solved affecting the performance.
%Furthermore, memory management is fully manual in a sense that a programmer should meet the access requirements, while some memory types being not so flexible, e.g. requiring the size of the data to be known beforehand.
%There is some automatic help from the compiler, however it is also limited, e.g. small arrays could be stored in registers, but only if the compiler is able to figure out that arrays indexing is static and if it does not, the array would end up in local memory.   
%%or misaligned accesses and the possibility of proper access patterns could depend on the domain of the problem being solved.
%%For example, the global memory access pattern could be not clear, thus preventing GPU from efficient coalescing.
%%It imposes a burden of memory management to a programmer or makes one rely on caching mechanisms.
%In order to achieve the fastest memory access constant, shared or registers memory should be utilized.
%However, constant memory lacks flexibility in the sense that the size of data should be known beforehand and access pattern also should be kept in mind.
%Shared memory should be used carefully due to considerations of synchronization and bank conflicts, while register allocation is managed by the compiler and explicit storing of data to them is difficult.
%E.g. small arrays could be stored in registers, but only if the compiler is able to figure out that arrays indexing is static and if it does not, the array would end up in local memory.
%Moreover, all these approaches require to create a special nontrivial code for manual data allocation management which makes algorithm implementation harder.
%Finally, such optimizations require specific knowledge from developers.
%One way to solve these problems is to introduce memory management runtime optimizations.

Performance of GPU-based solutions critically depends on memory management, since most applications tend to be bandwidth bound, requiring the data to be allocated and accessed most effectively.
Thereby, memory optimizations appear to be in a prevailing significance and addressed in a huge number of research.

To address issues related to the lack of available GPU memory, sophisticated memory pooling techniques are used leveraging memory swapping and sharing~\cite{zhang2019efficient}.
Further, since GPU memory access latency varies between various memory types, techniques like shared memory register spilling or automatic shared memory allocation are utilized to save global memory transactions and reduce cache contention~\cite{Sakdhnagool2019RegDemIG,Xie2018ICCADU}.
However, memory access latency could be aggravated by improper access patterns that entail the increase of the total number of memory transactions 
making the memory type being used much less effectively.
The proposed approach is a runtime memory optimization addressing the mentioned issues that reduces the overall memory transactions number.
The approach is based on the following typical feature for GPU-based solutions common workflow.

%At the same time, a common workflow of a GPU-based solution has a feature that allows one to introduce runtime optimizations which originally are static.
Suppose % the next scenario.
we have created an interactive solution for huge data analysis allowing the end-user to sequentially write GPU kernel proccessed queries to a dataset.
Let a query be relatively small and data be large, resulting in execution time being signigicant.
Simple examples of such scenarios are multiple patterns matching, database querying, convolutional filtering.
The GPU kernel should be generic and have at least two parameters: a query and data.
But at the moment the user issues a query and the host code is ready to run the kernel the query could be used as a static data for the kernel runtime optimization, since the query remains the same during the execution. 
Specifically, the kernel could be \emph{partially evaluated} in runtime with respect to the query being issued.

\textit{Partial evaluation} or \textit{specialization} is a well-known  optimization technique that given a program and part of its input data, called \emph{static}, specializes the program with respect to the data, producing another, optimized, program which if given only the remaining part of input data, called \emph{dynamic}, yields the same results as the original program would have produced being executed given both parts of the input data~\cite{Jones:1993:PEA:153676,PartialEvalPaper}.

% There is a known program optimization technique that optimizes a given program with respect to statically known inputs, producing another, optimized, program which if given only the remaining dynamic inputs yields the same results as the initial one being executed given both inputs.
% The technique is \textit{partial evaluation} (aka program specialization)~\cite{Jones:1993:PEA:153676,PartialEvalPaper}.
% Basically, given a function $f$ of $n$ arguments with some of them being static, denoted with $k$, partial evaluator evaluates or \textit{specializes} those parts of the function depending only on static arguments, producing a residual function $f'$ of $(n-k)$ arguments.
% Thus it produces a more optimal function in a sense that the specialized function being invoked makes fewer computations than the original one.

%Application of specialization for one of the described scenarios, namely database querying, have been known to significantly improve query execution performance~\cite{10.1007/978-3-319-74313-4_27}.

Application of specialization for one of the described scenarios, namely database querying, has been recently known to significantly improve CPU-based queries execution performance~\cite{10.1007/978-3-319-74313-4_27}.
It appears that partial evaluation is able to achieve similar results for GPU-based applications, optimizing memory accesses.
Particularly, considering the problem of multiple patterns matching, where the patterns are \emph{static} during the execution, the result of the memory access for a particular pattern could be evaluated in specialization time and embedded as a value into the code during compilation, rather than being compiled to a load instruction for some memory type.
More precisely, partial evaluation lets the values to be accessed thought instructions cache, avoiding memory load transactions.
Thus, in this work we propose to apply partial evaluation for GPU code optimization.

%Consider the snippet of code from \hyperref[lst1]{\textbf{Listing 1}}.
%Suppose the array named \textit{template} and its size \textit{template\_size} are statically known. The array \textit{ibuffer} is dynamic.
%Given that memory access indexing and the content of the array are static, the snippet could be transformed to \textit{.ptx} instructions during the compilation as presented in \hyperref[lst2]{\textbf{Listing 2}}.
%The thing is the content of \textit{template} array has been embedded into the comparison instructions rather than to be globally loaded multiple times as in \hyperref[lst2]{\textbf{Listing 3}}.
%\lstset{style=mystyle}
% \begin{figure}[b]
% \begin{lstlisting}[language=C,caption=Partial evaluation example,label=lst1]
% /*
% *   Suppose we have static template_size and
% *    template array itself, assume ['a',...]
% */

%  for i in unroll(0,template_size) {
    % //global/constant memory access
    % if ibuffer(t_id + i as i64) != template(i) {
        % //... Do something ... //
    % }
%  }
% /*
% *   After the specialization the code above
% *    is transformed to an unrolled list of
% *   akin .cu instructions
% */
    % ...
%  bool _93951;
%  _93951 = _93949 != 97; // 97 is ascii for 'a'
%  if (_93951) goto l93952;
%  else goto l93955;
    % ...
% \end{lstlisting}
% \end{figure}

% here are some silly things =)
% As long as the limitation on the number of registers per thread is met, all static data eventually \todo{get} into registers memory.
% \question{Moreover, if excessive usage of register memory happens, data would automatically leak to cached local memory resulting in automatic memory management, i.e. avoiding explicit allocations.}


%\begin{figure}[b]
%\begin{lstlisting}[language=C,caption=Partial evaluation example,label=lst1]
%/*
%*   assume  template is ['\x49','\x44',...]
%*/
%
% for i in unroll(0,template_size) {
%    //global/constant memory access
%    if ibuffer(t_id + i as i64) != template(i) {
%        //... Do something ... //
%    }
% }
%\end{lstlisting}
%\end{figure}

%\begin{figure}[b]
%\begin{lstlisting}[language=C,caption=Code after partial evaluation,label=lst2]
%    ...
% LDG.E.U8 R0, [R4] ; //load from global memory, i.e. ibuffer(...)
% BFE R6, R0, 0x1000 ;
% ISETP.NE.AND P0, PT, R6, 0x49, PT ; //0x49 got extracted, so we have avoided global memory access!
% @P0 SYNC ;
% LDG.E.U8 R0, [R4+0x1] ;
% BFE R0, R0, 0x1000 ;
% ISETP.NE.AND P0, PT, R0, 0x44, PT ; //extracted again!
%    //and so on
%    ...
%\end{lstlisting}
%\end{figure}

%\begin{figure}[b]
%\begin{lstlisting}[language=C,caption=Code without partial evaluation,label=lst3]
% LDG.E.U8 R8, [R8] //load global
% LDG.E.U8 R6, [R6] ; // load global
% BFE R13, R8, 0x1000 ;
% BFE R12, R6, 0x1000 ;
% ISETP.NE.AND P0, PT, R13, R12, PT ; //compare pre-loaded registers

%\end{lstlisting}
%\end{figure}


%\todo{Given that caches tend to undergo misses and random access could hurt coalescing or broadcasting}, we exploit partial evaluation techniques to specialize a GPU program on static data in such a way to move the data straight into the code rather than to any memory space.

\section{Evaluation}

The approach has been evaluated considering the problem of \textit{file carving}~\cite{DataCarving} that stands for extracting files from raw data in a fied of \emph{cyber forensics}. To extract a file, a specific file header should be detected, and the set of relatively short file headers becomes static at the search query executing time. Hence, the query could be specialized with respect to the headers. 
%the headers to search for are known beforehand and commonly are relatively short. 
%As an instance of a problem, consider the \textit{file carving}~\cite{DataCarving}, in a field of \textit{cyber forensics} it stands for extracting files from raw data, i.e. from lost clusters, unallocated clusters and slack space of the disk or digital media.
%To extract a file, we should detect a specific file header: for a predefined set of file types, the headers to search for are known beforehand and commonly are relatively short.

The partial evaluator being used is one developed as part of \textit{AnyDSL} framework~\cite{LeiBa}.
Thus, we compare\footnote{The approach has been evaluated on Ubuntu 18.04 system with \emph{Intel Core i7-6700} processor, 8GB of RAM and \emph{Pascal}-based \emph{GeForce GTX 1070} GPU with 8GB device memory.} na\"ive multiple string matching algorithm implementations: one utilizing AnyDSL framework, leveraging partial evaluation with respect to the file headers, and two base-line ones in CUDA C with global and constant memory for header access respectively.
All implementations invoke the algorithm in a separate thread for each position in the subject string.
The headers are stored as a single char-array and accessed via offsets.
The algorithm simply iterates over all headers searching for a match, if it encounters a mismatch, it jumps to the next header forward through the array. Basically, during specialization the partial evaluator performs loop unrolling and evaluates unrolled memory access instructions for the headers.


%The approach has been evaluated on Ubuntu 18.04 system with \textit{Intel Core i7-6700} processor, 8GB of RAM and \textit{Pascal}-based \textit{GeForce GTX 1070} GPU with 8GB device memory.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.46\textwidth]{results.pdf}
  \caption{Multiple string pattern matching evaluation}
  \label{fig:eval}
\end{figure}

%To estimate the performance gain brought by partial evaluation the problem of file carving has been evaluated.
For the evaluation, the piece of data of 4 GB size has been taken from a hard drive and patterns to be searched have been taken from a taxonomy of file headers specifications~\cite{Headers}.
The headers have been divided into groups of size 16 and run over multiple times.
The results are presented in Figure~\ref{fig:eval}.
The points are the average kernel running time and the grey regions are the area of standard deviation.

Since mismatches happen quite often inducing thread divergence, such access pattern hurts coalescing, increasing the overall number of memory transactions.
Given that, the performance speedup of a partially evaluated algorithm achieves on a raw data piece of 4 GB size is up to $8$ compared to CUDA C version with global memory and up to $3$ with constant one as illustrated in Figure~\ref{fig:eval}. Namely, the partially evaluated version spends about 300 ms for searching while global and constant memory CUDA C versions making it in 800 ms and 2500 ms respectively.
Note that specialization time is 2 sec, so in case of analysing 1 Tb storage, performance improvement would be significant.


\section{Conclusion}
In this work, we apply partial evaluation to optimize GPU programs.
We show that this optimization technique in the context of file carving problem can improve the performance up to $8$ times if compare na\"ive implementation executed with optimization and without it.
Note that optimizations do not require manual manipulation with source code and implementation of additional memory management routines.

The partial evaluator being used assumes the programs to be written with special \textit{DSL}.
Nevertheless, partial evaluation could be applied to general-purpose languages, and CUDA C, and the upcoming research is dedicated to the generalization of the technique so as the partial evaluation could be applied at runtime, during GPU-based application execution.
Moreover, the performance of partial evaluator should be improved in order to decrease specialization overhead.

Finally, specialization could produce code with a huge number of variables, making the compiler to spill excessive registers. A workaround would be a pipelining of specialization with other optimization techniques, e.g. with shared memory register spilling ~\cite{Sakdhnagool2019RegDemIG}.
%Could our partial evaluation be combined with advanced register spilling techniques (for example with~\cite{Sakdhnagool2019RegDemIG}) to improve performance?

%% Acknowledgments
\begin{acks}                            %% acks environment is optional
                                        %% contents suppressed with 'anonymous'
  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
  %% acknowledge financial support and will be used by metadata
  %% extraction tools.
  This material is based upon work supported by the
  \grantsponsor{GS100000001}{National Science
    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
  conclusions or recommendations expressed in this material are those
  of the author and do not necessarily reflect the views of the
  National Science Foundation.
\end{acks}


%% Bibliography
\bibliography{bib}


%% Appendix
% \appendix
% \section{Appendix}

% Text of appendix \ldots

\end{document}
