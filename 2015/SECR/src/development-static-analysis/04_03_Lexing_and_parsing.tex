\subsection{Lexical and syntax analysis}

The purpose of the lexical analysis is to extract tokens defined by the language specification from the input stream of symbols. Also the correspondence between lexical units and the source code must be preserved. The result of lexical analysis of the dynamically generated expression is a finite-state automaton over the alphabet of tokens. In the classic lexical analysis the token can be represented by some identifier and the subsequence of input symbols. In the case of dynamically generated expressions analysis the input symbols subsequence is replaced by the FSA representing the subset of all possible values of dynamically generated expression. Moreover, in the latter case for each symbol we must preserve the information about its position in the source code.

Lexical analysis consists of four steps. Firstly, the input FSA that was build by the approximator is transformed into the deterministic one. After that the FST is constructed based on the FSA. The next step is to build the composition of the obtained FST and the FST that was built based on the language specification. This second FST produces tokens as an output. The composition is a result of the consecutive application of these two transducers, i.e. an output of first FST is used as an input for the second. After that we obtain either a set of lexical errors or a FST with a pair on each edge. The first item in the pair is the symbol with the binding to the source code, the second one is a function that returns either token or nothing. If the previous step end up with no errors, the last step is an interpretation of the FST resulting in a finite-state automaton over the tokens alphabet.

This automaton is processed by the syntax analyzer. The analyzer is based on RNGLR~\cite{RNGLR:ref} algorithm which is a modification of Tomita's GLR-algorithm\cite{Tomita:ref}. Tomita's GLR algorithm allows to handle arbitrary context-free grammars. GLR handles Shift/Reduce and Reduce/Reduce conflicts -- situations when an available data are not enough to choose the correct way of parsing. Therefore, GLR processes all possible ways in case of conflict. So several derivation trees for a single input sequence can be obtained. The parser used in our platform extends this approach. The algorithm processes a token graph as an input. Herewith we meet a new situation when one node has several outgoing edges. We call this a "Shift/Shift" conflict despite the fact it is not an actual conflict because each way should be processed. More detail about the parser algorithm can be found in~\cite{AGLR:ref}. 