1. Hi, my name is Semyon, I'm from the saint Petersburg state university. I want to talk about
2.
3. Classical way is to use probabilistic grammar for secondary structure modelling. We propose to use parser only for features extraction. Namely, we use ordinary, not probabilistic, grammar for features description. Then we use parser for features extraction. After that we use artificial neural network for extracted features processing.
4. Let discuss structure of our solution in more details. The input consists of two parts. The first is a grammar which should not be context-free. We can use more expressive classes of grammars. We only need to have ability to represent parsing result as a set of matrices. The second part is a sequence to be processed.

 These two parts is input for parser. Our implementation is based on Okhotin's algorithm which reduce parsing to matrices multiplication. As a result we can utilize GPGPUs for parsing.

 Result of parsing is a set of boolean matrices: one matrix for each nonterminal. Each matrix represents information about all substrings which is derivable from the given nonterminal. In practice we use only matrix for start nonterminal. If cell i,j contains 1 then substring from i to j derivable from the start nonterminal.

 The input for neural network is a vector of features, so we should vectorize out matrices. We use simple line-by-line vectorization. As far as bottom left triangle is always filled by zeros, we skip it. For long sequences we also compress vectors to decrease network size. We just treat sequences of bits as byte. It is technical trick because most libraries for neural networks can not handle bit vectors, but can handle byte vectors.

 Well. Now we are ready to run dense neural network. At the current state it is classical dense network with agressive dropout and batch normalization. It is necessary for learning stabilization.

Finally we get some results which are depend on problem which we try to solve.

5. Let discuss some parts in more carefully. Firs part is grammar. In our case it is context-free grammar. S-one is a start nonterminal --- it is our point of interest.
This grammar contains two rules which describe arbitrary string of length from 2 up to 10, and arbitrary symbol, respectively.

The next rule is a key of this grammar. This rule describes recursive composition of stems which we want to detect. As you can see, S-zero is sequence of S-zeros, embedded into stem.

Rest rules are necessary for description of stem with bounded minimal height.

First of all, we specify stem with height exactly one,

The next step is definition of stem of height exactly Network

And finally we define stem of height more then three.

In this grammar we use only classical base pairs. But one can add G-T pair. It may increase quality of result.

Well. Let look at the examples of parsing results.
6. First example is a result of parsing of sequence which should contains stem. Note that it is a synthetic sequence. Parsing result is a matrix. Bottom left angle is empty. Input sequence is laced on main diagonal only for example explanation. Real matrix contains only ones and zeros.

Additionally, you can see contact map, which helps to navigate in sequence.

Let look at the matrix content. Each one is a root of stem. Chain of ones is a stem with height more than three. The leftmost one is a stem of height 3 and each next is a stem of height increased by 1. So, here we can see stem of height ten.

Let highlight it. I use boxes for mapping ones to input sequence. One box for first part of folded subsequence, and one -- for the second part. Great: parsing result maps to expected contact map.

You can see some possibly strange ones which are out of chain. As I mention above, we detect all possible substrings. Detected foldings are possible with relation to our grammar. Sometimes these foldings should be treated as noise, but sometimes they may be useful. Let look at example, when it is useful to detect all possible foldings.

7.Yet another synthatic sequence which should fold to pseudoknot. Pseudoknots are not describable by using context-free grammars. Pdeudoknot presented here can be described as interleaved stems. And our matrix contains all these two stems.

The first one

and the second one

No, if neural network is powerful enough, if can infer that given sequence contains pseudoknot, because parsing result contains all necessary information.

Thus, our approach can handle features of secondary structure which can not be expressed in class of grammar in use.

8. And the last example is a real tRNA processing. This sequence is given from GTRNA batabase. In this example we compare parsing result with top two predicted secondary structures which are given by external tool.

9. Well. It is a parsing matrix.

And here you can see first predicted secondary structure.

And this structure is contained in our matrix. As far as we do not handle G-T pairs in our grammar, parsing result is slightly different from predicted structure. For example HERE.

Well. And now the second predicted structure.

Predicted by tool.

And corresponding parts detected by our parser.

And put all together. All two predicted structures are contained in matrix.

10. Short Summary. Parser is just features extractor. It extracts all possible foldings for all possible substrings with relation to given grammar which can be modified. And as a result it is possible to detect features which is not expressable in used class of grammars.

11. The next part of our solution is a 
12.
13.
14.
15.
16.
17.
